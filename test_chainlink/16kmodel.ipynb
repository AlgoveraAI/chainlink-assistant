{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/marshath/play/chainlink/algovate/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from typing import List, Any, Dict\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.retrievers import TFIDFRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain.schema import BaseRetriever, Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(df, retriever):\n",
    "    # Add retrieved_docs column if it doesn't exist\n",
    "    df['retrieved_docs'] = df.get('retrieved_docs', '')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        retrieved_docs = retriever.get_relevant_documents(row['question'])\n",
    "\n",
    "        docs = \"\"\n",
    "        for d in retrieved_docs:\n",
    "            if docs == \"\":\n",
    "                docs = d.page_content\n",
    "            else:\n",
    "                docs = docs + \"\\n\\n:::NEXT DOC:::\\n\\n\" + d.page_content\n",
    "\n",
    "        # Store retrieved docs in the DataFrame\n",
    "        df.at[index, 'retrieved_docs'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/marshath/play/chainlink/algovate/algovate/data/combined_documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6409\n",
      "7912\n",
      "19252\n",
      "40097\n",
      "14632\n"
     ]
    }
   ],
   "source": [
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "for doc in documents:\n",
    "    num_tokens = len(enc.encode(doc.page_content))\n",
    "    if num_tokens > 6000:\n",
    "        print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomeSplitter:\n",
    "    def __init__(self, chunk_threshold=6000, chunk_size=6000, chunk_overlap=50):\n",
    "        self.chunk_threshold = chunk_threshold\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.splitter = TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "    def token_counter(self, document):\n",
    "        tokens = self.enc.encode(document.page_content)\n",
    "        return len(tokens)\n",
    "\n",
    "    def split(self, documents):\n",
    "        chunked_documents = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            try:\n",
    "                if self.token_counter(doc) > self.chunk_threshold:\n",
    "                    chunks = self.splitter.split_documents([doc])\n",
    "                    chunks= [Document(page_content=chunk.page_content, metadata={\"source\": f\"{chunk.metadata['source']} chunk {i}\"}) for i, chunk in enumerate(chunks)]\n",
    "                    chunked_documents.extend(chunks)\n",
    "                else:\n",
    "                    chunked_documents.append(doc)\n",
    "            except:\n",
    "                chunked_documents.append(doc)\n",
    "                print(f\"Error on document {i}\")\n",
    "                print(doc)\n",
    "\n",
    "        return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CustomeSplitter()\n",
    "chunked_documents = splitter.split(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    full_docs: List[Document]\n",
    "    base_retriever:BaseRetriever = None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        full_docs: List[Document],\n",
    "        search_kwargs: Dict[str, Any] = {},\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "        split_docs = splitter.split_documents(documents)\n",
    "        vector_store = FAISS.from_documents(split_docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "        return cls(full_docs=full_docs, base_retriever=vector_store.as_retriever(**search_kwargs), **kwargs)\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:  \n",
    "        results =  self.base_retriever.get_relevant_documents(query=query)\n",
    "        doc_ids = [doc.metadata[\"source\"] for doc in results]\n",
    "        full_retrieved_docs = [d for d in chunked_documents if d.metadata[\"source\"] in doc_ids]\n",
    "        return full_retrieved_docs\n",
    "        \n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 970d91aa9f88825b922f210256a6ed9e in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 970d91aa9f88825b922f210256a6ed9e in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 970d91aa9f88825b922f210256a6ed9e in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 21 Jun 2023 07:49:03 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'algovera', 'openai-processing-ms': '32796', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': '970d91aa9f88825b922f210256a6ed9e', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7daaa635591bf3c5-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 774fa21849ad84ea40afc37ca773195c in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 774fa21849ad84ea40afc37ca773195c in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 774fa21849ad84ea40afc37ca773195c in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 21 Jun 2023 07:49:42 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'algovera', 'openai-processing-ms': '32562', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': '774fa21849ad84ea40afc37ca773195c', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7daaa7304e2ff3c5-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    }
   ],
   "source": [
    "retriever = CustomRetriever.from_documents(chunked_documents, search_kwargs={\"k\": 1})\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.)\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(retriever=retriever, llm=llm, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/marshath/play/chainlink/algovate/algovate/data/auto_eval_pairs_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = TFIDFRetriever.from_documents(chunked_documents, k=2)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.)\n",
    "# chain = RetrievalQAWithSourcesChain.from_chain_type(retriever=retriever, llm=llm, chain_type=\"stuff\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0\n",
      "Done with 1\n",
      "Done with 2\n",
      "Done with 3\n",
      "Done with 4\n",
      "Done with 5\n",
      "Done with 6\n",
      "Done with 7\n",
      "Done with 8\n",
      "Done with 9\n",
      "Done with 10\n",
      "Done with 11\n",
      "Done with 12\n",
      "Done with 13\n",
      "Done with 14\n",
      "Done with 15\n",
      "Done with 16\n",
      "Done with 17\n",
      "Done with 18\n",
      "Done with 19\n",
      "Done with 20\n",
      "Done with 21\n",
      "Done with 22\n",
      "Done with 23\n",
      "Done with 24\n",
      "Done with 25\n",
      "Done with 26\n",
      "Done with 27\n",
      "Done with 28\n",
      "Done with 29\n",
      "Done with 30\n",
      "Done with 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 39730e85512d17397def9acdca023173 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 39730e85512d17397def9acdca023173 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 39730e85512d17397def9acdca023173 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 21 Jun 2023 08:01:30 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'algovera', 'openai-processing-ms': '30038', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': '39730e85512d17397def9acdca023173', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7daab89aedd2f3c5-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 32\n",
      "Done with 33\n",
      "Done with 34\n",
      "Done with 35\n",
      "Done with 36\n",
      "Done with 37\n",
      "Done with 38\n",
      "Done with 39\n",
      "Done with 40\n",
      "Done with 41\n",
      "Done with 42\n",
      "Done with 43\n",
      "Done with 44\n",
      "This model's maximum context length is 16385 tokens. However, your messages resulted in 21437 tokens. Please reduce the length of the messages.\n",
      "Error on 45\n",
      "What is the purpose of the `functions-simulate` command in the Chainlink Functions hardhat starter kit?\n",
      "Done with 45\n",
      "Done with 46\n",
      "Done with 47\n",
      "Done with 48\n",
      "Done with 49\n",
      "Done with 50\n",
      "Done with 51\n",
      "Done with 52\n",
      "Done with 53\n",
      "Done with 54\n",
      "Done with 55\n",
      "Done with 56\n",
      "Done with 57\n",
      "Done with 58\n",
      "Done with 59\n",
      "Done with 60\n",
      "Done with 61\n",
      "Done with 62\n",
      "Done with 63\n",
      "Done with 64\n",
      "Done with 65\n",
      "Done with 66\n",
      "Done with 67\n",
      "Done with 68\n",
      "Done with 69\n",
      "Done with 70\n",
      "Done with 71\n"
     ]
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "for i, gt in df2.iterrows():\n",
    "    try:\n",
    "        res = chain(gt[\"question\"])\n",
    "        df2.loc[i, \"result\"] = res[\"answer\"]\n",
    "        df2.loc[i, \"sources\"] = res[\"sources\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error on {i}\")\n",
    "        print(gt[\"question\"])\n",
    "    print(f\"Done with {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c08a07a7d306cd840b15cc2d1f69319e in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c08a07a7d306cd840b15cc2d1f69319e in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c08a07a7d306cd840b15cc2d1f69319e in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 21 Jun 2023 08:21:21 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'algovera', 'openai-processing-ms': '30009', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': 'c08a07a7d306cd840b15cc2d1f69319e', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7daad5a9ab97f3c5-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c475b68b4c61d4cc91e4bc15bac1c1c1 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c475b68b4c61d4cc91e4bc15bac1c1c1 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID c475b68b4c61d4cc91e4bc15bac1c1c1 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 21 Jun 2023 08:22:02 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'algovera', 'openai-processing-ms': '30012', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': 'c475b68b4c61d4cc91e4bc15bac1c1c1', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7daad6af2b73f3c5-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7590854dc37aacab00066e6406c38a1d in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7590854dc37aacab00066e6406c38a1d in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 7590854dc37aacab00066e6406c38a1d in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 21 Jun 2023 08:25:11 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'algovera', 'openai-processing-ms': '30012', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-reset-requests': '20ms', 'x-request-id': '7590854dc37aacab00066e6406c38a1d', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '7daadb4acb03f3c5-BOM', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df2, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"/home/marshath/play/chainlink/algovate/test_chainlink/autoq_code1_16kmodel.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Richard's Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/marshath/play/chainlink/algovate/algovate/data/ground_truths.pkl\", \"rb\") as f:\n",
    "    ground_truths = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "for i, gt in df2.iterrows():\n",
    "    try:\n",
    "        res = chain(gt[\"question\"])\n",
    "        df2.loc[i, \"result\"] = res[\"answer\"]\n",
    "        df2.loc[i, \"sources\"] = res[\"sources\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error on {i}\")\n",
    "        print(gt[\"question\"])\n",
    "\n",
    "    print(f\"Done with {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c5cdd344af4ce7bd4034f08f27178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cb0941500a4a2e9666a1535e47db62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "524338f577e44e9eaf367f713abb4205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50e2d8981754d17a6d682d9572eeb40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"/home/marshath/play/chainlink/algovate/test_chainlink/16kmodel.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoQ Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/marshath/play/chainlink/algovate/algovate/data/auto_eval_pairs_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0\n",
      "Done with 1\n",
      "Done with 2\n",
      "Done with 3\n",
      "Done with 4\n",
      "Done with 5\n",
      "Done with 6\n",
      "Done with 7\n",
      "Done with 8\n",
      "Done with 9\n",
      "Done with 10\n",
      "Done with 11\n",
      "Done with 12\n",
      "Done with 13\n",
      "Done with 14\n",
      "Done with 15\n",
      "Done with 16\n",
      "Done with 17\n",
      "Done with 18\n",
      "Done with 19\n",
      "Done with 20\n",
      "Done with 21\n",
      "Done with 22\n",
      "Done with 23\n",
      "Done with 24\n",
      "Done with 25\n",
      "Done with 26\n",
      "Done with 27\n",
      "Done with 28\n",
      "Done with 29\n",
      "Done with 30\n",
      "Done with 31\n",
      "Done with 32\n",
      "Done with 33\n",
      "Done with 34\n",
      "Done with 35\n",
      "Done with 36\n",
      "Done with 37\n",
      "Done with 38\n",
      "Done with 39\n",
      "Done with 40\n",
      "Done with 41\n",
      "Done with 42\n",
      "Done with 43\n",
      "Done with 44\n",
      "Done with 45\n",
      "Done with 46\n",
      "Done with 47\n",
      "Done with 48\n",
      "Done with 49\n",
      "Done with 50\n",
      "Done with 51\n",
      "Done with 52\n",
      "Done with 53\n",
      "Done with 54\n",
      "Done with 55\n",
      "Done with 56\n",
      "Done with 57\n",
      "Done with 58\n",
      "Done with 59\n"
     ]
    }
   ],
   "source": [
    "df3 = df.copy()\n",
    "for i, gt in df3.iterrows():\n",
    "    try:\n",
    "        res = chain(gt[\"question\"])\n",
    "        df3.loc[i, \"result\"] = res[\"answer\"]\n",
    "        df3.loc[i, \"sources\"] = res[\"sources\"]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"Error on {i}\")\n",
    "        print(gt[\"question\"])\n",
    "\n",
    "    print(f\"Done with {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(df, retriever):\n",
    "    # Add retrieved_docs column if it doesn't exist\n",
    "    df['retrieved_docs'] = df.get('retrieved_docs', '')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        retrieved_docs = retriever.get_relevant_documents(row['question'])\n",
    "\n",
    "        docs = \"\"\n",
    "        for d in retrieved_docs:\n",
    "            if docs == \"\":\n",
    "                docs = d.page_content\n",
    "            else:\n",
    "                docs = docs + \"\\n\\n:::NEXT DOC:::\\n\\n\" + d.page_content\n",
    "\n",
    "        # Store retrieved docs in the DataFrame\n",
    "        df.at[index, 'retrieved_docs'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df3, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"/home/marshath/play/chainlink/algovate/test_chainlink/autoq_combineddoc_16kmodel_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
