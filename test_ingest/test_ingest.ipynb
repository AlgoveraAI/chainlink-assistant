{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python ./chainlink_stuffs/ingest.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from IPython.display import Markdown\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../algovate/data/documents.pkl', 'rb') as f:\n",
    "    documents = pickle.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving from stackoverflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1 : https://stackoverflow.com/oauth?client_id=26442&scope=read_inbox&redirect_uri=https://stackapps.com\n",
    "\n",
    "step2: WzYapLyZRU2(L5smk9jrzQ))\n",
    "\n",
    "Step3 : !curl -d \"client_id=26442&client_secret=1*L23oud4Rorl*sgpP86uQ((&code=WzYapLyZRU2(L5smk9jrzQ))&redirect_uri=https://stackapps.com\" -X POST https://stackoverflow.com/oauth/access_token\n",
    "\n",
    "step4: access_token=StCiXPrq6YvuE0AJiUM5TA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import html2text\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://api.stackexchange.com/docs/questions\n",
    "def get_questions(tag, page, pagesize, from_date, to_date, key, access_token):\n",
    "    # url = f'https://api.stackexchange.com/2.3/questions?order=desc&sort=creation&tagged={tag}&site={site}&pagesize={pagesize}&key={key}&access_token={access_token}&filter=withbody'\n",
    "    url = f\"https://api.stackexchange.com/2.3/questions?page={page}&pagesize={pagesize}&tagged={tag}&fromdate={from_date}&todate={to_date}&key={key}&access_token={access_token}&order=desc&sort=activity&site=stackoverflow&filter=withbody\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()['items']\n",
    "\n",
    "def get_answers(question_id, site, key, access_token):\n",
    "    url = f'https://api.stackexchange.com/2.3/questions/{question_id}/answers?order=desc&sort=activity&site=stackoverflow&key={key}&access_token={access_token}&filter=withbody'\n",
    "    response = requests.get(url)\n",
    "    return response.json()['items']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'xbyZm8lZ94ZKKl0aQ5iYhA(('\n",
    "access_token = 'StCiXPrq6YvuE0AJiUM5TA))'\n",
    "tag = 'chainlink'\n",
    "pagesize = 99\n",
    "from_date = \"1609459200\"\n",
    "to_date = \"1685577600\"\n",
    "\n",
    "all_questions = None\n",
    "for i in range(1, 5):\n",
    "    page = i\n",
    "    if all_questions is None:\n",
    "        all_questions = get_questions(\n",
    "            tag=tag, pagesize=pagesize, key=key, \n",
    "            access_token=access_token, from_date=from_date, \n",
    "            to_date=to_date, page=page\n",
    "        )\n",
    "    else:\n",
    "        all_questions.extend(get_questions(\n",
    "            tag=tag, pagesize=pagesize, key=key, \n",
    "            access_token=access_token, from_date=from_date, \n",
    "            to_date=to_date, page=page\n",
    "            )\n",
    "        )\n",
    "\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = sorted(all_questions, key=lambda x: x['creation_date'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = False\n",
    "\n",
    "questions_and_answers = {}\n",
    "\n",
    "for question in all_questions:\n",
    "    answers = get_answers(question['question_id'], site, key, access_token)\n",
    "    if not answers:  # Skip questions with no answers\n",
    "        continue\n",
    "    question_body = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', h.handle(question['body']))\n",
    "    question_str = \"Question: (Asked on: \" + datetime.utcfromtimestamp(question['creation_date']).strftime('%Y-%m-%d %H:%M:%S') + \")\\n\" + question['title'] + \"\\n\"\n",
    "    question_url = \"URL: \" + question['link'] + \"\\n\\n\"\n",
    "    question_body_str = \"Question Body:\\n\" + question_body + \"\\n\\n\"\n",
    "    answers_str = \"Answers:\\n\" + \"\\n---\\n\".join([\"(Answered on: \" + datetime.utcfromtimestamp(answer['creation_date']).strftime('%Y-%m-%d %H:%M:%S') + \")\\n\" + re.sub(r'!\\[.*?\\]\\(.*?\\)', '', h.handle(answer['body'])) for answer in answers]) + \"\\n\\n\"\n",
    "    text = question_str + question_url + question_body_str + answers_str\n",
    "\n",
    "    questions_and_answers[question['question_id']] = Document(page_content=text, metadata={\"source\":question['link']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stackoverflow.pkl\", \"wb\") as f:\n",
    "    pickle.dump(questions_and_answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_overflow_documents = []\n",
    "for k,v in questions_and_answers.items():\n",
    "    stack_overflow_documents.append(v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chainlink Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import tempfile\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gh_data( \n",
    "    gh_tuple:tuple, \n",
    "    chunk: bool = True, \n",
    "    chunk_size: int = 1200\n",
    "):\n",
    "    repo_url = gh_tuple[0]\n",
    "    wildcards = gh_tuple[1]\n",
    "    documents = get_github_docs(repo_url, wildcards) \n",
    "    \n",
    "    # splitter = RecursiveCharacterTextSplitter(\n",
    "    #     chunk_size = 1200,\n",
    "    #     chunk_overlap  = 20,\n",
    "    # )\n",
    "    # documents = splitter.split_documents(documents)\n",
    "\n",
    "    return documents\n",
    "\n",
    "def get_github_docs(\n",
    "    repo_url: str,\n",
    "    wildcards: Optional[List[str]] = None\n",
    "):\n",
    "    repo_url = repo_url.replace('.git', '')\n",
    "    url_parts = repo_url.split('/')\n",
    "    if len(url_parts) < 5 or not url_parts[2].endswith(\"github.com\"):\n",
    "        raise ValueError(\"Invalid GitHub URL format\")\n",
    "\n",
    "    repo_owner = url_parts[3]\n",
    "    repo_name = url_parts[4]\n",
    "\n",
    "    if len(url_parts) > 6 and url_parts[5] == 'tree':\n",
    "        branch = '/'.join(url_parts[6:])\n",
    "    else:\n",
    "        branch = None\n",
    "\n",
    "    repo_url = f\"https://github.com/{repo_owner}/{repo_name}\"\n",
    "    if not repo_url.endswith(\".git\"):\n",
    "        repo_url += \".git\"\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as d:\n",
    "        if branch is not None:\n",
    "            git_command = f\"git clone --depth 1 -b {branch} {repo_url} .\"\n",
    "        else:\n",
    "            git_command = f\"git clone --depth 1 {repo_url} .\"\n",
    "\n",
    "        subprocess.check_call(\n",
    "            git_command,\n",
    "            cwd=d,\n",
    "            shell=True,\n",
    "        )\n",
    "        git_sha = (\n",
    "            subprocess.check_output(\"git rev-parse HEAD\", shell=True, cwd=d)\n",
    "            .decode(\"utf-8\")\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "        repo_path = Path(d)\n",
    "        markdown_files = list(repo_path.glob(\"**/*.md\")) + list(\n",
    "            repo_path.glob(\"**/*.mdx\")\n",
    "        )\n",
    "        if wildcards is not None:\n",
    "            wildcards = [f\"*{pattern}/*\" if  not pattern.startswith('*') and not pattern.endswith('/*') else pattern for pattern in wildcards]\n",
    "            filtered_files = []\n",
    "            for wildcard in wildcards:\n",
    "                filtered_files.extend(\n",
    "                    file for file in markdown_files if fnmatch.fnmatch(str(file), wildcard)\n",
    "                )\n",
    "            markdown_files = list(set(filtered_files))  # Remove duplicates\n",
    "\n",
    "        documents = []\n",
    "        for markdown_file in markdown_files:\n",
    "            with open(markdown_file, \"r\") as f:\n",
    "                relative_path = markdown_file.relative_to(repo_path)\n",
    "                github_url = f\"https://github.com/{repo_owner}/{repo_name}/blob/{git_sha}/{relative_path}\"\n",
    "                read = f.read()\n",
    "                documents.append(Document(page_content=read, metadata={\"source\": github_url}))\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into '.'...\n"
     ]
    }
   ],
   "source": [
    "gh_tuple = (\"https://github.com/oceanByte/chainlink-education.git\", \"src/api/src/shared/course\")\n",
    "\n",
    "chainlink_education_documents = load_gh_data(gh_tuple)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all three into one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 309, 306)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents), len(stack_overflow_documents), len(chainlink_education_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_documents = documents + stack_overflow_documents + chainlink_education_documents\n",
    "len(overall_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../algovate/data/combined_documents.pkl\", \"wb\") as f:\n",
    "    pickle.dump(overall_documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "algovate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
