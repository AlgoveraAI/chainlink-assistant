{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/marshath/play/chainlink/algovate/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "\n",
    "with open('/home/marshath/play/chainlink/algovate/algovate/data/documents.pkl', 'rb') as f:\n",
    "    lc_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import GPTVectorStoreIndex, ResponseSynthesizer\n",
    "from llama_index.data_structs.node import Node, DocumentRelationship\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index import LLMPredictor, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
    "from llama_index import GPTListIndex\n",
    "from llama_index.indices.list.retrievers import ListIndexLLMRetriever\n",
    "from llama_index.indices.postprocessor import (\n",
    "    LLMRerank\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "from algovate.llama.evaluate import run_experiment, grade_model_answer, grade_model_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_row_as_markdown(df, row_index, columns_to_observe=[\"question\",\"answer\", \"result\"]):\n",
    "    \"\"\"\n",
    "    Display a specific row of a DataFrame as Markdown, showing only the specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to display.\n",
    "        row_index (int): The index of the row to display.\n",
    "        columns_to_observe (list): The list of columns to observe.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    row = df.iloc[row_index]\n",
    "\n",
    "    markdown_row = '\\n\\n|'.join(str(row[column]) for column in columns_to_observe)\n",
    "    display(Markdown(markdown_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_retrieved_nodes(\n",
    "#     query_str, vector_top_k=10, reranker_top_n=5, with_reranker=True\n",
    "# ):\n",
    "#   query_bundle = QueryBundle(query_str)\n",
    "#   # configure retriever\n",
    "#   retriever = VectorIndexRetriever(\n",
    "#     index=index,\n",
    "#     similarity_top_k=vector_top_k,\n",
    "#   )\n",
    "#   retrieved_nodes = retriever.retrieve(query_bundle)\n",
    "\n",
    "#   if with_reranker:\n",
    "#     # configure reranker\n",
    "#     reranker = LLMRerank(choice_batch_size=5, top_n=reranker_top_n, service_context=service_context)\n",
    "#     retrieved_nodes = reranker.postprocess_nodes(retrieved_nodes, query_bundle)\n",
    "#   return retrieved_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 774)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for doc in lc_docs:\n",
    "    documents.append(Document(text=doc.page_content, extra_info=doc.metadata))\n",
    "\n",
    "parser = SimpleNodeParser()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "len(documents), len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the ground truths\n",
    "\n",
    "with open(\"/home/marshath/play/chainlink/algovate/algovate/data/ground_truths.pkl\", \"rb\") as f:\n",
    "    ground_truths = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ground_truths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple evaluation app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataframe_answer(df):\n",
    "    # Add right_or_wrong column if it doesn't exist\n",
    "    if 'right_or_wrong' not in df.columns:\n",
    "        df['right_or_wrong'] = ''\n",
    "\n",
    "    # Widgets\n",
    "    button_right = widgets.Button(description=\"Right\")\n",
    "    button_wrong = widgets.Button(description=\"Wrong\")\n",
    "    button_partial = widgets.Button(description=\"partial\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    # Starting index\n",
    "    index = [0]  # Use list to allow mutation inside functions\n",
    "\n",
    "    def display_row(row):\n",
    "        \"\"\"\n",
    "        Display the row data in markdown\n",
    "        \"\"\"\n",
    "        with out:\n",
    "            display(Markdown(f\"**Question**: {row['question']}\"))\n",
    "            display(Markdown(f\"**Answer**: {row['answer']}\"))\n",
    "            display(Markdown(f\"**Result**: {row['result']}\\n\"))\n",
    "\n",
    "    def on_button_right_clicked(b):\n",
    "        \"\"\"\n",
    "        Update right_or_wrong column with 'right' and go to next row\n",
    "        \"\"\"\n",
    "        df.at[index[0], 'right_or_wrong'] = 'right'\n",
    "        index[0] += 1\n",
    "        out.clear_output()\n",
    "        if index[0] < len(df):\n",
    "            display_row(df.iloc[index[0]])\n",
    "\n",
    "    def on_button_wrong_clicked(b):\n",
    "        \"\"\"\n",
    "        Update right_or_wrong column with 'wrong' and go to next row\n",
    "        \"\"\"\n",
    "        df.at[index[0], 'right_or_wrong'] = 'wrong'\n",
    "        index[0] += 1\n",
    "        out.clear_output()\n",
    "        if index[0] < len(df):\n",
    "            display_row(df.iloc[index[0]])\n",
    "\n",
    "    def on_button_partial_clicked(b):\n",
    "        \"\"\"\n",
    "        Update right_or_wrong column with 'partial' and go to next row\n",
    "        \"\"\"\n",
    "        df.at[index[0], 'right_or_wrong'] = 'partial'\n",
    "        index[0] += 1\n",
    "        out.clear_output()\n",
    "        if index[0] < len(df):\n",
    "            display_row(df.iloc[index[0]])\n",
    "\n",
    "    button_right.on_click(on_button_right_clicked)\n",
    "    button_wrong.on_click(on_button_wrong_clicked)\n",
    "    button_partial.on_click(on_button_partial_clicked)\n",
    "\n",
    "    display(out)\n",
    "    display(button_right, button_wrong, button_partial)\n",
    "\n",
    "    # Start by displaying the first row\n",
    "    display_row(df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(df, query_engine):\n",
    "    # Add retrieved_docs column if it doesn't exist\n",
    "    df['retrieved_docs'] = df.get('retrieved_docs', '')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        retrieved_docs = query_engine.retrieve(row['question'])\n",
    "\n",
    "        docs = \"\"\n",
    "        for d in retrieved_docs:\n",
    "            if docs == \"\":\n",
    "                docs = d.node.text\n",
    "            else:\n",
    "                docs = docs + \"\\n\\n:::NEXT DOC:::\\n\\n\" + d.node.text\n",
    "\n",
    "        # Store retrieved docs in the DataFrame\n",
    "        df.at[index, 'retrieved_docs'] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataframe_retrieval(df, query_engine):\n",
    "    # Add rd_right_or_wrong column if it doesn't exist\n",
    "    df['rd_right_or_wrong'] = df.get('rd_right_or_wrong', '')\n",
    "\n",
    "    df_wrong = df[df['right_or_wrong'].isin(['wrong', 'partial'])].copy()\n",
    "\n",
    "    # Widgets\n",
    "    button_right = widgets.Button(description=\"Right\")\n",
    "    button_wrong = widgets.Button(description=\"Wrong\")\n",
    "    button_partial = widgets.Button(description=\"Partial\")\n",
    "    out = widgets.Output()\n",
    "\n",
    "    # Starting index\n",
    "    index = [0]  # Use list to allow mutation inside functions\n",
    "\n",
    "    def display_row(row):\n",
    "        \"\"\"\n",
    "        Display the row data in markdown\n",
    "        \"\"\"\n",
    "        with out:\n",
    "            display(Markdown(f\"**Question**: {row['question']}\"))\n",
    "            display(Markdown(f\"**Answer**: {row['answer']}\"))\n",
    "            display(Markdown(f\"**Result**: {row['result']}\\n\"))\n",
    "            display(Markdown(f\"**Retrieved Docs**: {row['retrieved_docs']}\"))\n",
    "\n",
    "    def on_button_right_clicked(b):\n",
    "        \"\"\"\n",
    "        Update right_or_wrong column with 'right' and go to next row\n",
    "        \"\"\"\n",
    "        df.at[df_wrong.index[index[0]], 'rd_right_or_wrong'] = 'right'\n",
    "        index[0] += 1\n",
    "        out.clear_output()\n",
    "        if index[0] < len(df_wrong):\n",
    "            display_row(df_wrong.iloc[index[0]])\n",
    "\n",
    "    def on_button_wrong_clicked(b):\n",
    "        \"\"\"\n",
    "        Update right_or_wrong column with 'wrong' and go to next row\n",
    "        \"\"\"\n",
    "        df.at[df_wrong.index[index[0]], 'rd_right_or_wrong'] = 'wrong'\n",
    "        index[0] += 1\n",
    "        out.clear_output()\n",
    "        if index[0] < len(df_wrong):\n",
    "            display_row(df_wrong.iloc[index[0]])\n",
    "\n",
    "    def on_button_partial_clicked(b):\n",
    "        \"\"\"\n",
    "        Update right_or_wrong column with 'partial' and go to next row\n",
    "        \"\"\"\n",
    "        df.at[df_wrong.index[index[0]], 'rd_right_or_wrong'] = 'partial'\n",
    "        index[0] += 1\n",
    "        out.clear_output()\n",
    "        if index[0] < len(df_wrong):\n",
    "            display_row(df_wrong.iloc[index[0]])\n",
    "\n",
    "    button_right.on_click(on_button_right_clicked)\n",
    "    button_wrong.on_click(on_button_wrong_clicked)\n",
    "    button_partial.on_click(on_button_partial_clicked)\n",
    "\n",
    "    display(out)\n",
    "    display(button_right, button_wrong, button_partial)\n",
    "\n",
    "    # Start by displaying the first row\n",
    "    if len(df_wrong) > 0:\n",
    "        display_row(df_wrong.iloc[0])\n",
    "    else:\n",
    "        print(\"There are no wrong rows in the DataFrame.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a `llm_predictor` using ChatOpenAI model\n",
    "- get a `GPTVectorStoreIndex`\n",
    "- default `query_engine`\n",
    "\n",
    "Test only the first 10 questions from ground truths and evaluate manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:45:33,990 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:45:33,992 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0,))\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "index = GPTVectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:45:34,589 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:45:34,590 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 16:45:44,328 - INFO - > [get_response] Total LLM token usage: 1725 tokens\n",
      "2023-05-29 16:45:44,329 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:45:44,852 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:45:44,853 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 16:45:50,403 - INFO - > [get_response] Total LLM token usage: 1964 tokens\n",
      "2023-05-29 16:45:50,404 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:45:51,117 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:45:51,118 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 16:46:19,478 - INFO - > [get_response] Total LLM token usage: 2082 tokens\n",
      "2023-05-29 16:46:19,479 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:46:20,237 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:46:20,239 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 16:46:44,855 - INFO - > [get_response] Total LLM token usage: 1702 tokens\n",
      "2023-05-29 16:46:44,857 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:46:45,413 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:46:45,414 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 16:47:02,648 - INFO - > [get_response] Total LLM token usage: 2286 tokens\n",
      "2023-05-29 16:47:02,649 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:47:03,065 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:47:03,065 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 16:47:15,071 - INFO - > [get_response] Total LLM token usage: 2103 tokens\n",
      "2023-05-29 16:47:15,073 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:47:15,533 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:47:15,534 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 16:47:36,166 - INFO - > [get_response] Total LLM token usage: 1895 tokens\n",
      "2023-05-29 16:47:36,167 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:47:36,781 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:47:36,782 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 16:47:42,555 - INFO - > [get_response] Total LLM token usage: 1938 tokens\n",
      "2023-05-29 16:47:42,556 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:47:43,050 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:47:43,050 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 16:47:47,948 - INFO - > [get_response] Total LLM token usage: 2092 tokens\n",
      "2023-05-29 16:47:47,949 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:47:48,509 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:47:48,509 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 16:47:52,683 - INFO - > [get_response] Total LLM token usage: 1992 tokens\n",
      "2023-05-29 16:47:52,684 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:47:53,113 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:47:53,114 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 16:47:57,773 - INFO - > [get_response] Total LLM token usage: 1818 tokens\n",
      "2023-05-29 16:47:57,774 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:47:58,318 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:47:58,319 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 16:48:04,015 - INFO - > [get_response] Total LLM token usage: 2091 tokens\n",
      "2023-05-29 16:48:04,015 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:48:04,723 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:48:04,725 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 16:48:12,882 - INFO - > [get_response] Total LLM token usage: 725 tokens\n",
      "2023-05-29 16:48:12,883 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:48:13,718 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:48:13,719 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 16:48:25,118 - INFO - > [get_response] Total LLM token usage: 1637 tokens\n",
      "2023-05-29 16:48:25,120 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:48:25,581 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:48:25,582 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 16:48:33,550 - INFO - > [get_response] Total LLM token usage: 1331 tokens\n",
      "2023-05-29 16:48:33,551 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:48:34,036 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:48:34,036 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 16:48:39,481 - INFO - > [get_response] Total LLM token usage: 1368 tokens\n",
      "2023-05-29 16:48:39,482 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:48:39,930 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:48:39,931 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 16:48:51,284 - INFO - > [get_response] Total LLM token usage: 2064 tokens\n",
      "2023-05-29 16:48:51,285 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:48:51,897 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:48:51,898 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 16:48:58,156 - INFO - > [get_response] Total LLM token usage: 1538 tokens\n",
      "2023-05-29 16:48:58,157 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:48:59,522 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:48:59,523 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 16:49:06,644 - INFO - > [get_response] Total LLM token usage: 1909 tokens\n",
      "2023-05-29 16:49:06,644 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:49:07,052 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:49:07,052 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 16:49:14,202 - INFO - > [get_response] Total LLM token usage: 2008 tokens\n",
      "2023-05-29 16:49:14,203 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:49:15,123 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:49:15,125 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 16:49:18,131 - INFO - > [get_response] Total LLM token usage: 1251 tokens\n",
      "2023-05-29 16:49:18,131 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:49:18,952 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:49:18,954 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 16:49:26,347 - INFO - > [get_response] Total LLM token usage: 1729 tokens\n",
      "2023-05-29 16:49:26,347 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:49:26,759 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:49:26,760 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 16:49:43,192 - INFO - > [get_response] Total LLM token usage: 1739 tokens\n",
      "2023-05-29 16:49:43,193 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:49:43,661 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:49:43,662 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 16:49:47,577 - INFO - > [get_response] Total LLM token usage: 1437 tokens\n",
      "2023-05-29 16:49:47,578 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:49:48,020 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:49:48,022 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 16:49:58,934 - INFO - > [get_response] Total LLM token usage: 2095 tokens\n",
      "2023-05-29 16:49:58,935 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:49:59,360 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:49:59,361 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 16:50:13,098 - INFO - > [get_response] Total LLM token usage: 1424 tokens\n",
      "2023-05-29 16:50:13,099 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 16:50:13,790 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:50:13,791 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 16:50:23,136 - INFO - > [get_response] Total LLM token usage: 1906 tokens\n",
      "2023-05-29 16:50:23,136 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "for i, gt in df1.iterrows():\n",
    "    try:\n",
    "        df1.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df1.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe908f7495ae44099e5fdb248d2dccc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab7986590534db6899799316ef9bb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf50fad3c3294a1898706bbd589e176b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf2f19caa764c59a29c9fd39c4e1a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ee47e59df44a9ca06a6c52e1ff3ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c332c355a934fb481c77ae5b62bdac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b07310a54847faa995d15dc5b3d65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35aa5010abdd4f82be96c8ae5f94290e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:56:19,417 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:56:19,418 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 16:56:19,418 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:56:43,748 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:56:43,749 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:57:10,153 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:57:10,153 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:57:26,836 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:57:26,838 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:57:41,114 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:57:41,115 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df1, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"exp_1.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:59:42,369 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:59:42,370 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "experiment = \"exp_2\"\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.,))\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 16:59:47,952 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 16:59:47,953 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:00:14,278 - INFO - > [get_response] Total LLM token usage: 3549 tokens\n",
      "2023-05-29 17:00:14,279 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:00:14,912 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:00:14,913 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:01:12,099 - INFO - > [get_response] Total LLM token usage: 4901 tokens\n",
      "2023-05-29 17:01:12,101 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:01:12,549 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:01:12,551 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:01:34,495 - INFO - > [get_response] Total LLM token usage: 3846 tokens\n",
      "2023-05-29 17:01:34,496 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:01:35,045 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:01:35,046 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:02:02,031 - INFO - > [get_response] Total LLM token usage: 3213 tokens\n",
      "2023-05-29 17:02:02,033 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:02:02,637 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:02:02,640 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 17:02:53,531 - INFO - > [get_response] Total LLM token usage: 5374 tokens\n",
      "2023-05-29 17:02:53,532 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:02:54,058 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:02:54,059 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 17:03:21,085 - INFO - > [get_response] Total LLM token usage: 3629 tokens\n",
      "2023-05-29 17:03:21,086 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:03:21,652 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:03:21,653 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:03:48,512 - INFO - > [get_response] Total LLM token usage: 3528 tokens\n",
      "2023-05-29 17:03:48,513 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:03:49,088 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:03:49,090 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:03:49,523 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:03:49,526 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:04:00,421 - INFO - > [get_response] Total LLM token usage: 3584 tokens\n",
      "2023-05-29 17:04:00,422 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:04:00,946 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:04:00,947 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:04:06,965 - INFO - > [get_response] Total LLM token usage: 3705 tokens\n",
      "2023-05-29 17:04:06,966 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:04:07,440 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:04:07,441 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:04:07,810 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:04:07,812 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:04:15,682 - INFO - > [get_response] Total LLM token usage: 3722 tokens\n",
      "2023-05-29 17:04:15,683 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:04:16,299 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:04:16,301 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:04:16,659 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:04:16,662 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:04:21,032 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:04:21,033 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:04:25,384 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:04:25,385 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:04:35,145 - INFO - > [get_response] Total LLM token usage: 3312 tokens\n",
      "2023-05-29 17:04:35,147 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:04:35,693 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:04:35,695 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 17:04:43,547 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:04:43,548 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:05:06,763 - INFO - > [get_response] Total LLM token usage: 4259 tokens\n",
      "2023-05-29 17:05:06,764 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:05:07,241 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:05:07,242 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 17:05:22,908 - INFO - > [get_response] Total LLM token usage: 2649 tokens\n",
      "2023-05-29 17:05:22,910 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:05:23,349 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:05:23,350 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:05:41,471 - INFO - > [get_response] Total LLM token usage: 3376 tokens\n",
      "2023-05-29 17:05:41,473 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:05:41,996 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:05:41,997 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 17:05:57,163 - INFO - > [get_response] Total LLM token usage: 3192 tokens\n",
      "2023-05-29 17:05:57,164 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:05:57,717 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:05:57,718 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:06:16,200 - INFO - > [get_response] Total LLM token usage: 3393 tokens\n",
      "2023-05-29 17:06:16,202 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:06:16,641 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:06:16,642 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 17:06:29,911 - INFO - > [get_response] Total LLM token usage: 3717 tokens\n",
      "2023-05-29 17:06:29,914 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:06:30,650 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:06:30,651 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:06:40,293 - INFO - > [get_response] Total LLM token usage: 2853 tokens\n",
      "2023-05-29 17:06:40,294 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:06:40,773 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:06:40,774 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:06:41,200 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:06:41,201 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:06:45,609 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:06:45,613 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:06:49,994 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:06:49,997 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:06:54,381 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:06:54,385 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:07:26,186 - INFO - > [get_response] Total LLM token usage: 4290 tokens\n",
      "2023-05-29 17:07:26,187 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:07:26,732 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:07:26,734 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:07:42,159 - INFO - > [get_response] Total LLM token usage: 3660 tokens\n",
      "2023-05-29 17:07:42,160 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:07:43,624 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:07:43,625 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 17:07:43,979 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:07:43,980 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:07:57,949 - INFO - > [get_response] Total LLM token usage: 3206 tokens\n",
      "2023-05-29 17:07:57,950 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:07:58,690 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:07:58,691 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:07:59,045 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:07:59,046 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:08:03,386 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:08:03,386 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:08:34,586 - INFO - > [get_response] Total LLM token usage: 3900 tokens\n",
      "2023-05-29 17:08:34,587 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:08:35,115 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:08:35,116 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 17:08:45,624 - INFO - > [get_response] Total LLM token usage: 2381 tokens\n",
      "2023-05-29 17:08:45,625 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:08:46,175 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:08:46,175 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:08:50,434 - INFO - > [get_response] Total LLM token usage: 3381 tokens\n",
      "2023-05-29 17:08:50,435 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:08:51,073 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:08:51,074 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:08:51,420 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:08:51,422 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:09:08,887 - INFO - > [get_response] Total LLM token usage: 3788 tokens\n",
      "2023-05-29 17:09:08,888 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:09:09,447 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:09:09,447 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:09:23,123 - INFO - > [get_response] Total LLM token usage: 2593 tokens\n",
      "2023-05-29 17:09:23,124 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:09:23,594 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:09:23,594 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:09:37,836 - INFO - > [get_response] Total LLM token usage: 3229 tokens\n",
      "2023-05-29 17:09:37,836 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "\n",
    "for i, gt in df2.iterrows():\n",
    "    try:\n",
    "        df2.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df2.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dataframe_answer(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df2, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644a5abe13b54c4b960bfdd8306fce5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95f06534f1b455c81269f0d2876b175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98239db18346473dba48ab38ec0ae6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fc8bbd7c8584f9c9a013e5058bcc4c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df2, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2.to_csv(\"exp_2.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 3 - Run 2 with longer outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:23:55,164 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:23:55,166 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "experiment = \"exp_2\"\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.,))\n",
    "max_input_size = 4096\n",
    "num_output = 256*2\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ]\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:24:05,056 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:24:05,057 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:24:05,415 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:24:05,417 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:24:35,245 - INFO - > [get_response] Total LLM token usage: 3549 tokens\n",
      "2023-05-29 17:24:35,246 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:24:35,848 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:24:35,849 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:24:36,256 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:24:36,258 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:25:26,411 - INFO - > [get_response] Total LLM token usage: 4901 tokens\n",
      "2023-05-29 17:25:26,413 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:25:30,240 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:25:30,241 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:25:53,587 - INFO - > [get_response] Total LLM token usage: 3846 tokens\n",
      "2023-05-29 17:25:53,589 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:25:54,031 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:25:54,032 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:25:54,382 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:25:54,383 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:25:58,716 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:25:58,717 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:26:27,147 - INFO - > [get_response] Total LLM token usage: 3213 tokens\n",
      "2023-05-29 17:26:27,151 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:26:27,848 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:26:27,849 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 17:26:28,252 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:26:28,255 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:27:20,710 - INFO - > [get_response] Total LLM token usage: 5374 tokens\n",
      "2023-05-29 17:27:20,710 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:27:21,240 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:27:21,241 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 17:27:43,563 - INFO - > [get_response] Total LLM token usage: 3629 tokens\n",
      "2023-05-29 17:27:43,564 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:27:44,131 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:27:44,132 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:28:10,347 - INFO - > [get_response] Total LLM token usage: 3528 tokens\n",
      "2023-05-29 17:28:10,348 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:28:11,037 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:28:11,037 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:28:17,906 - INFO - > [get_response] Total LLM token usage: 3584 tokens\n",
      "2023-05-29 17:28:17,907 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:28:18,740 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:28:18,740 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:28:24,681 - INFO - > [get_response] Total LLM token usage: 3705 tokens\n",
      "2023-05-29 17:28:24,682 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:28:25,419 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:28:25,420 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:28:25,760 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:28:25,761 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:28:34,355 - INFO - > [get_response] Total LLM token usage: 3722 tokens\n",
      "2023-05-29 17:28:34,355 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:28:34,967 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:28:34,969 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:28:35,327 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:28:35,328 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:28:45,072 - INFO - > [get_response] Total LLM token usage: 3312 tokens\n",
      "2023-05-29 17:28:45,073 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:28:45,676 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:28:45,677 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 17:29:07,347 - INFO - > [get_response] Total LLM token usage: 4259 tokens\n",
      "2023-05-29 17:29:07,348 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:29:07,776 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:29:07,777 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 17:29:16,707 - INFO - > [get_response] Total LLM token usage: 2649 tokens\n",
      "2023-05-29 17:29:16,708 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:29:17,267 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:29:17,268 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:29:32,099 - INFO - > [get_response] Total LLM token usage: 3376 tokens\n",
      "2023-05-29 17:29:32,102 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:29:33,361 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:29:33,362 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 17:29:33,767 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:29:33,768 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:29:38,145 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:29:38,146 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:29:52,070 - INFO - > [get_response] Total LLM token usage: 3204 tokens\n",
      "2023-05-29 17:29:52,071 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:29:52,708 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:29:52,709 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:30:04,524 - INFO - > [get_response] Total LLM token usage: 3393 tokens\n",
      "2023-05-29 17:30:04,527 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:30:05,272 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:30:05,273 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 17:30:13,344 - INFO - > [get_response] Total LLM token usage: 3717 tokens\n",
      "2023-05-29 17:30:13,345 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:30:14,313 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:30:14,314 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:30:14,676 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:30:14,677 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:30:19,004 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:30:19,005 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:30:29,654 - INFO - > [get_response] Total LLM token usage: 2853 tokens\n",
      "2023-05-29 17:30:29,655 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:30:30,132 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:30:30,132 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:30:39,433 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:30:39,434 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:30:54,826 - INFO - > [get_response] Total LLM token usage: 4290 tokens\n",
      "2023-05-29 17:30:54,826 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:30:55,512 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:30:55,513 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:31:13,411 - INFO - > [get_response] Total LLM token usage: 3695 tokens\n",
      "2023-05-29 17:31:13,412 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:31:13,941 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:31:13,942 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 17:31:14,283 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:31:14,284 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:31:27,400 - INFO - > [get_response] Total LLM token usage: 3206 tokens\n",
      "2023-05-29 17:31:27,401 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:31:32,577 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:31:32,578 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:31:58,292 - INFO - > [get_response] Total LLM token usage: 3900 tokens\n",
      "2023-05-29 17:31:58,292 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:31:59,024 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:31:59,024 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 17:32:08,562 - INFO - > [get_response] Total LLM token usage: 2381 tokens\n",
      "2023-05-29 17:32:08,563 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:32:09,812 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:32:09,813 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:32:14,716 - INFO - > [get_response] Total LLM token usage: 3381 tokens\n",
      "2023-05-29 17:32:14,717 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:32:15,272 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:32:15,273 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:32:26,554 - INFO - > [get_response] Total LLM token usage: 3788 tokens\n",
      "2023-05-29 17:32:26,555 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:32:27,256 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:32:27,257 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:32:27,594 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:32:27,595 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:32:46,820 - INFO - > [get_response] Total LLM token usage: 2593 tokens\n",
      "2023-05-29 17:32:46,820 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:32:47,744 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:32:47,745 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:32:59,809 - INFO - > [get_response] Total LLM token usage: 3229 tokens\n",
      "2023-05-29 17:32:59,809 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df3 = df.copy()\n",
    "\n",
    "for i, gt in df3.iterrows():\n",
    "    try:\n",
    "        df3.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df3.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195a2c7629f0426bb5d669b29769378c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558bd575b30840e1a19da038f9918d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7141244efd4b4d92ae28dddbe73f0c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625bb47edea94e989ecbd5011ae25107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:35:52,498 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:52,499 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:35:53,190 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:53,190 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:35:53,631 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:53,632 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:35:54,049 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:54,050 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:35:54,669 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:54,669 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 17:35:55,247 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:55,248 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 17:35:55,657 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:55,658 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:35:56,150 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:56,151 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:35:56,625 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:56,626 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:35:57,066 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:57,067 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:35:57,494 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:57,495 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:35:58,122 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:58,123 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 17:35:58,570 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:58,571 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 17:35:59,119 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:59,121 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:35:59,786 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:35:59,788 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 17:36:01,223 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:01,224 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:36:01,702 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:01,703 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 17:36:02,142 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:02,142 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:36:02,579 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:02,580 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 17:36:03,183 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:03,184 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:36:03,604 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:03,605 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 17:36:04,151 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:04,152 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:36:04,734 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:04,735 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 17:36:05,286 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:05,288 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:36:05,772 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:05,774 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 17:36:06,174 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:06,175 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:36:06,815 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:36:06,816 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df3, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6dad8bc75d342dc9ca4d0cc2606bbfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d48b52b0452e4a4d837d7b4e8b65e44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d354e2e8553047fe870bdf19d6893988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d873610f800c4c71a3e08e4423c4d427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df3, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"exp_3.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 4 - Run 2 with different response mode - refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:40:54,158 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:40:54,160 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "experiment = \"exp_2\"\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.,))\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ],\n",
    "    response_mode = \"refine\", # https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/response/type.py#L7\n",
    "\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 17:40:58,191 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:40:58,192 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:40:58,538 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:40:58,538 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:42:57,601 - INFO - > [get_response] Total LLM token usage: 5967 tokens\n",
      "2023-05-29 17:42:57,603 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:42:58,342 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:42:58,342 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 17:43:25,563 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:43:25,572 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:43:29,892 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:43:29,894 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:43:34,200 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:43:34,201 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:44:32,244 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:44:32,246 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:45:03,966 - INFO - > [get_response] Total LLM token usage: 6041 tokens\n",
      "2023-05-29 17:45:03,968 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:45:04,520 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:45:04,522 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:46:52,925 - INFO - > [get_response] Total LLM token usage: 6139 tokens\n",
      "2023-05-29 17:46:52,926 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:46:53,436 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:46:53,438 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:47:23,415 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:47:23,416 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:48:19,707 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:48:19,708 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:48:52,364 - INFO - > [get_response] Total LLM token usage: 5544 tokens\n",
      "2023-05-29 17:48:52,365 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:48:53,830 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:48:53,831 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 17:50:18,841 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:50:18,842 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:50:48,915 - INFO - > [get_response] Total LLM token usage: 6325 tokens\n",
      "2023-05-29 17:50:48,917 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:50:49,430 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:50:49,431 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 17:52:40,652 - INFO - > [get_response] Total LLM token usage: 5856 tokens\n",
      "2023-05-29 17:52:40,653 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:52:41,155 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:52:41,157 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:52:41,475 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:52:41,478 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:53:55,700 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:53:55,701 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:54:29,081 - INFO - > [get_response] Total LLM token usage: 5059 tokens\n",
      "2023-05-29 17:54:29,081 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:54:30,128 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:54:30,129 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 17:55:16,066 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:55:16,067 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:55:46,020 - INFO - > [get_response] Total LLM token usage: 4858 tokens\n",
      "2023-05-29 17:55:46,021 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:55:46,717 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:55:46,718 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 17:56:12,945 - INFO - > [get_response] Total LLM token usage: 4284 tokens\n",
      "2023-05-29 17:56:12,946 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:56:13,491 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:56:13,492 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 17:56:30,233 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:56:30,234 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:56:49,005 - INFO - > [get_response] Total LLM token usage: 4570 tokens\n",
      "2023-05-29 17:56:49,006 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:56:49,611 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:56:49,611 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 17:57:09,578 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:57:09,579 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:57:31,664 - INFO - > [get_response] Total LLM token usage: 4013 tokens\n",
      "2023-05-29 17:57:31,666 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:57:32,680 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:57:32,680 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 17:57:37,706 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 17:57:37,709 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 17:58:01,484 - INFO - > [get_response] Total LLM token usage: 4383 tokens\n",
      "2023-05-29 17:58:01,484 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:58:02,022 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:58:02,022 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 17:59:01,114 - INFO - > [get_response] Total LLM token usage: 3652 tokens\n",
      "2023-05-29 17:59:01,115 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 17:59:01,938 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 17:59:01,939 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:00:20,029 - INFO - > [get_response] Total LLM token usage: 4732 tokens\n",
      "2023-05-29 18:00:20,030 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:00:20,710 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:00:20,710 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:00:21,080 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:00:21,081 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:01:48,697 - INFO - > [get_response] Total LLM token usage: 4597 tokens\n",
      "2023-05-29 18:01:48,698 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:01:49,179 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:01:49,180 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:03:04,476 - INFO - > [get_response] Total LLM token usage: 4690 tokens\n",
      "2023-05-29 18:03:04,477 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:03:05,075 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:03:05,076 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 18:04:19,699 - INFO - > [get_response] Total LLM token usage: 4968 tokens\n",
      "2023-05-29 18:04:19,700 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:04:20,331 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:04:20,331 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:04:25,350 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:04:25,350 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:05:34,140 - INFO - > [get_response] Total LLM token usage: 4073 tokens\n",
      "2023-05-29 18:05:34,141 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:05:34,780 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:05:34,781 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:05:43,146 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:05:43,159 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:06:58,559 - INFO - > [get_response] Total LLM token usage: 5096 tokens\n",
      "2023-05-29 18:06:58,560 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:06:59,166 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:06:59,167 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:06:59,481 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:06:59,482 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:07:33,699 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:07:33,701 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:08:31,904 - INFO - > [get_response] Total LLM token usage: 4855 tokens\n",
      "2023-05-29 18:08:31,906 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:08:32,801 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:08:32,802 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:08:33,134 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:08:33,135 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:08:37,456 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:08:37,457 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:08:41,767 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:08:41,768 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:09:28,263 - INFO - > [get_response] Total LLM token usage: 3933 tokens\n",
      "2023-05-29 18:09:28,264 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:09:28,773 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:09:28,774 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:10:46,838 - INFO - > [get_response] Total LLM token usage: 5009 tokens\n",
      "2023-05-29 18:10:46,839 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:10:47,360 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:10:47,361 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 18:12:05,258 - INFO - > [get_response] Total LLM token usage: 3737 tokens\n",
      "2023-05-29 18:12:05,259 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:12:05,736 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:12:05,737 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:12:11,233 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:12:11,234 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:12:15,540 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:12:15,541 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:13:06,355 - INFO - > [get_response] Total LLM token usage: 4329 tokens\n",
      "2023-05-29 18:13:06,356 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:13:06,897 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:13:06,898 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:13:17,183 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:13:17,185 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:13:21,500 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:13:21,501 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:13:45,916 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:13:45,917 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:14:48,023 - INFO - > [get_response] Total LLM token usage: 5214 tokens\n",
      "2023-05-29 18:14:48,024 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:14:48,734 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:14:48,736 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:15:58,191 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:15:58,193 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:16:02,490 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:16:02,491 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:16:31,843 - INFO - > [get_response] Total LLM token usage: 4214 tokens\n",
      "2023-05-29 18:16:31,844 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:16:32,279 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:16:32,280 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:17:35,377 - INFO - > [get_response] Total LLM token usage: 4433 tokens\n",
      "2023-05-29 18:17:35,378 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df4 = df.copy()\n",
    "\n",
    "for i, gt in df4.iterrows():\n",
    "    try:\n",
    "        df4.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df4.loc[i, \"result\"] = \"ERROR\"\n",
    "\n",
    "# takes very long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:17:36,095 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:36,096 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:17:36,543 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:36,544 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:17:37,171 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:37,172 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:17:37,772 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:37,773 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:17:38,303 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:38,304 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 18:17:38,895 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:38,896 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 18:17:39,472 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:39,474 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:17:39,909 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:39,911 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:17:40,394 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:40,395 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:17:41,066 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:41,068 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:17:41,694 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:41,696 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:17:42,262 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:42,263 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 18:17:42,693 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:42,693 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 18:17:43,368 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:43,369 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:17:43,967 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:43,968 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:17:44,538 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:44,539 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:17:45,082 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:45,083 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 18:17:45,808 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:45,809 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:17:46,506 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:46,507 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:17:47,210 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:47,211 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:17:47,804 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:47,806 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:17:48,438 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:48,440 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:17:49,159 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:49,161 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 18:17:50,019 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:50,021 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:17:50,714 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:50,715 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:17:51,210 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:51,212 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:17:51,920 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:17:51,921 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df4, query_engine)\n",
    "df4.to_csv(\"exp_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0307d74e3564a84bcc24a837f5e7a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f21bf1876d45a98c7baf13fce3ed9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1414358194944d48a760eee8195f0a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4f7eeaed394994badb0bf697cca7c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "026f76acfb98450a8ed6650251481146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e07ce038364e1489e53117a6d0207e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbf8c6a5278447f816c155bb9ba51db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a1554ad33e745ecbf7d30e2a1a28752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df4, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"exp_4.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 5 - Run 2 with different response mode - tree_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:19:36,666 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:19:36,667 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.,))\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ],\n",
    "    response_mode = \"tree_summarize\", # https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/response/type.py#L7\n",
    "\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:19:37,409 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:19:37,409 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:20:03,753 - INFO - > [get_response] Total LLM token usage: 3719 tokens\n",
      "2023-05-29 18:20:03,754 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:20:03,754 - INFO - > [get_response] Total LLM token usage: 3719 tokens\n",
      "2023-05-29 18:20:03,755 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:20:04,549 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:20:04,550 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:20:04,573 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:20:51,424 - INFO - > [get_response] Total LLM token usage: 515 tokens\n",
      "2023-05-29 18:20:51,425 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:20:51,425 - INFO - > [get_response] Total LLM token usage: 5001 tokens\n",
      "2023-05-29 18:20:51,426 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:20:53,045 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:20:53,045 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:20:53,065 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:22:04,693 - INFO - > [get_response] Total LLM token usage: 997 tokens\n",
      "2023-05-29 18:22:04,695 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:22:04,696 - INFO - > [get_response] Total LLM token usage: 5325 tokens\n",
      "2023-05-29 18:22:04,696 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:22:05,385 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:22:05,386 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:22:32,063 - INFO - > [get_response] Total LLM token usage: 3330 tokens\n",
      "2023-05-29 18:22:32,066 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:22:32,067 - INFO - > [get_response] Total LLM token usage: 3330 tokens\n",
      "2023-05-29 18:22:32,067 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:22:32,806 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:22:32,806 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 18:22:32,834 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:22:33,165 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:22:33,167 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:23:04,033 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:23:04,034 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:23:51,483 - INFO - > [get_response] Total LLM token usage: 1200 tokens\n",
      "2023-05-29 18:23:51,484 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:23:51,484 - INFO - > [get_response] Total LLM token usage: 6199 tokens\n",
      "2023-05-29 18:23:51,485 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:23:52,021 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:23:52,022 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 18:24:19,706 - INFO - > [get_response] Total LLM token usage: 3780 tokens\n",
      "2023-05-29 18:24:19,707 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:24:19,707 - INFO - > [get_response] Total LLM token usage: 3780 tokens\n",
      "2023-05-29 18:24:19,707 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:24:20,396 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:24:20,396 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:24:42,163 - INFO - > [get_response] Total LLM token usage: 3704 tokens\n",
      "2023-05-29 18:24:42,163 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:24:42,164 - INFO - > [get_response] Total LLM token usage: 3704 tokens\n",
      "2023-05-29 18:24:42,164 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:24:42,705 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:24:42,706 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:24:42,726 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:25:02,966 - INFO - > [get_response] Total LLM token usage: 231 tokens\n",
      "2023-05-29 18:25:02,967 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:02,967 - INFO - > [get_response] Total LLM token usage: 4088 tokens\n",
      "2023-05-29 18:25:02,968 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:03,630 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:25:03,630 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:25:03,651 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:25:18,057 - INFO - > [get_response] Total LLM token usage: 162 tokens\n",
      "2023-05-29 18:25:18,058 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:18,058 - INFO - > [get_response] Total LLM token usage: 4132 tokens\n",
      "2023-05-29 18:25:18,059 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:18,579 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:25:18,580 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:25:18,600 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:25:33,504 - INFO - > [get_response] Total LLM token usage: 151 tokens\n",
      "2023-05-29 18:25:33,505 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:33,505 - INFO - > [get_response] Total LLM token usage: 4249 tokens\n",
      "2023-05-29 18:25:33,506 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:33,916 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:25:33,917 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:25:39,404 - INFO - > [get_response] Total LLM token usage: 3478 tokens\n",
      "2023-05-29 18:25:39,405 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:39,405 - INFO - > [get_response] Total LLM token usage: 3478 tokens\n",
      "2023-05-29 18:25:39,405 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:25:39,945 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:25:39,946 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 18:25:39,965 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:25:40,306 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:25:40,306 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:26:00,611 - INFO - > [get_response] Total LLM token usage: 185 tokens\n",
      "2023-05-29 18:26:00,612 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:00,612 - INFO - > [get_response] Total LLM token usage: 4220 tokens\n",
      "2023-05-29 18:26:00,613 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:01,073 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:26:01,074 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 18:26:11,013 - INFO - > [get_response] Total LLM token usage: 2784 tokens\n",
      "2023-05-29 18:26:11,014 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:11,014 - INFO - > [get_response] Total LLM token usage: 2784 tokens\n",
      "2023-05-29 18:26:11,015 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:11,575 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:26:11,576 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:26:11,939 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:26:11,940 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:26:27,397 - INFO - > [get_response] Total LLM token usage: 3540 tokens\n",
      "2023-05-29 18:26:27,398 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:27,398 - INFO - > [get_response] Total LLM token usage: 3540 tokens\n",
      "2023-05-29 18:26:27,398 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:27,810 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:26:27,811 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:26:40,547 - INFO - > [get_response] Total LLM token usage: 3370 tokens\n",
      "2023-05-29 18:26:40,548 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:40,548 - INFO - > [get_response] Total LLM token usage: 3370 tokens\n",
      "2023-05-29 18:26:40,549 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:26:41,300 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:26:41,301 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:26:41,656 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:26:41,657 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:26:45,985 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:26:45,986 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:27:01,644 - INFO - > [get_response] Total LLM token usage: 3533 tokens\n",
      "2023-05-29 18:27:01,645 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:27:01,646 - INFO - > [get_response] Total LLM token usage: 3533 tokens\n",
      "2023-05-29 18:27:01,646 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:27:02,087 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:27:02,088 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 18:27:02,107 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:27:39,982 - INFO - > [get_response] Total LLM token usage: 424 tokens\n",
      "2023-05-29 18:27:39,983 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:27:39,984 - INFO - > [get_response] Total LLM token usage: 4548 tokens\n",
      "2023-05-29 18:27:39,984 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:27:40,543 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:27:40,543 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:27:49,302 - INFO - > [get_response] Total LLM token usage: 2947 tokens\n",
      "2023-05-29 18:27:49,302 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:27:49,303 - INFO - > [get_response] Total LLM token usage: 2947 tokens\n",
      "2023-05-29 18:27:49,303 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:27:49,730 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:27:49,731 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:27:49,752 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:27:50,149 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:27:50,149 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:28:16,079 - INFO - > [get_response] Total LLM token usage: 228 tokens\n",
      "2023-05-29 18:28:16,080 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:28:16,080 - INFO - > [get_response] Total LLM token usage: 4299 tokens\n",
      "2023-05-29 18:28:16,081 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:28:16,589 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:28:16,590 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:28:16,609 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:28:36,153 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:28:36,154 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:28:48,838 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:28:48,839 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:29:10,156 - INFO - > [get_response] Total LLM token usage: 429 tokens\n",
      "2023-05-29 18:29:10,156 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:29:10,157 - INFO - > [get_response] Total LLM token usage: 4347 tokens\n",
      "2023-05-29 18:29:10,157 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:29:10,730 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:29:10,731 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:29:20,452 - INFO - > [get_response] Total LLM token usage: 3354 tokens\n",
      "2023-05-29 18:29:20,452 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:29:20,453 - INFO - > [get_response] Total LLM token usage: 3354 tokens\n",
      "2023-05-29 18:29:20,453 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:29:21,154 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:29:21,155 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:29:21,175 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:30:12,884 - INFO - > [get_response] Total LLM token usage: 503 tokens\n",
      "2023-05-29 18:30:12,884 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:30:12,885 - INFO - > [get_response] Total LLM token usage: 4675 tokens\n",
      "2023-05-29 18:30:12,885 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:30:13,309 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:30:13,310 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 18:30:26,498 - INFO - > [get_response] Total LLM token usage: 2471 tokens\n",
      "2023-05-29 18:30:26,499 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:30:26,499 - INFO - > [get_response] Total LLM token usage: 2471 tokens\n",
      "2023-05-29 18:30:26,500 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:30:26,910 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:30:26,911 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:30:31,432 - INFO - > [get_response] Total LLM token usage: 3530 tokens\n",
      "2023-05-29 18:30:31,432 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:30:31,433 - INFO - > [get_response] Total LLM token usage: 3530 tokens\n",
      "2023-05-29 18:30:31,433 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:30:31,965 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:30:31,966 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:30:31,984 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-29 18:31:04,918 - INFO - > [get_response] Total LLM token usage: 397 tokens\n",
      "2023-05-29 18:31:04,919 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:31:04,919 - INFO - > [get_response] Total LLM token usage: 4492 tokens\n",
      "2023-05-29 18:31:04,920 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:31:05,440 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:31:05,441 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:31:21,870 - INFO - > [get_response] Total LLM token usage: 2715 tokens\n",
      "2023-05-29 18:31:21,871 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:31:21,872 - INFO - > [get_response] Total LLM token usage: 2715 tokens\n",
      "2023-05-29 18:31:21,872 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:31:22,277 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:31:22,278 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:31:34,224 - INFO - > [get_response] Total LLM token usage: 3372 tokens\n",
      "2023-05-29 18:31:34,225 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:31:34,225 - INFO - > [get_response] Total LLM token usage: 3372 tokens\n",
      "2023-05-29 18:31:34,226 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df5 = df.copy()\n",
    "\n",
    "for i, gt in df5.iterrows():\n",
    "    try:\n",
    "        df5.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df5.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:32:47,007 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:47,008 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:32:47,532 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:47,533 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:32:48,037 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:48,037 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:32:48,608 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:48,609 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:32:49,003 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:49,004 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 18:32:49,536 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:49,536 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 18:32:50,555 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:50,556 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:32:51,061 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:51,062 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:32:51,565 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:51,566 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:32:52,113 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:52,114 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:32:52,789 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:52,790 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:32:53,312 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:53,313 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 18:32:53,720 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:53,721 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 18:32:54,150 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:54,150 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:32:54,781 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:54,782 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:32:55,291 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:55,292 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:32:55,884 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:55,885 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 18:32:56,536 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:56,536 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:32:56,951 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:56,951 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:32:57,404 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:57,405 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:32:57,817 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:57,818 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:32:58,361 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:58,362 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:32:59,030 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:59,030 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 18:32:59,553 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:32:59,553 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:33:00,551 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:33:00,552 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:33:01,067 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:33:01,067 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:33:01,811 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:33:01,812 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df5, query_engine)\n",
    "df5.to_csv(\"exp_5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e934dae62c43b08b03b87204d10847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b05568d4d984aa5b7a1c047f9a62781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbcbc9b142b4e79863a2b8e443ab96c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3559a8705caf4d65a973463fda637222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82a9d0e48bd432bb451ab98135f250e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d847f5e0b54c2a98524225c74805cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074d185634434c96a4cd9ccbafa7c587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8224aff5ae7b407a842677321e3c8132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df5, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_csv(\"exp_5.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 6 - Run 2 with different response mode - ACCUMULATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:36:07,065 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:36:07,066 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.,))\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ],\n",
    "    response_mode = \"accumulate\", # https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/response/type.py#L7\n",
    "\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:37:46,976 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:37:46,977 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:39:24,307 - INFO - > [get_response] Total LLM token usage: 4618 tokens\n",
      "2023-05-29 18:39:24,307 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:39:24,714 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:39:24,714 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:40:45,926 - INFO - > [get_response] Total LLM token usage: 4987 tokens\n",
      "2023-05-29 18:40:45,927 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:40:46,544 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:40:46,545 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:42:08,701 - INFO - > [get_response] Total LLM token usage: 4512 tokens\n",
      "2023-05-29 18:42:08,702 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:42:09,248 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:42:09,249 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:43:10,019 - INFO - > [get_response] Total LLM token usage: 3678 tokens\n",
      "2023-05-29 18:43:10,020 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:43:11,689 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:43:11,690 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 18:44:32,290 - INFO - > [get_response] Total LLM token usage: 5115 tokens\n",
      "2023-05-29 18:44:32,290 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:44:32,702 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:44:32,703 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 18:45:50,380 - INFO - > [get_response] Total LLM token usage: 4446 tokens\n",
      "2023-05-29 18:45:50,380 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:45:50,998 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:45:50,998 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:46:24,569 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:46:24,570 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:46:28,918 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:46:28,919 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:47:05,844 - INFO - > [get_response] Total LLM token usage: 4172 tokens\n",
      "2023-05-29 18:47:05,845 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:47:06,314 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:47:06,315 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:47:26,115 - INFO - > [get_response] Total LLM token usage: 3820 tokens\n",
      "2023-05-29 18:47:26,115 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:47:26,648 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:47:26,648 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:47:39,236 - INFO - > [get_response] Total LLM token usage: 3898 tokens\n",
      "2023-05-29 18:47:39,237 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:47:39,644 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:47:39,645 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:48:13,737 - INFO - > [get_response] Total LLM token usage: 4291 tokens\n",
      "2023-05-29 18:48:13,738 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:48:14,485 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:48:14,485 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:48:33,436 - INFO - > [get_response] Total LLM token usage: 3533 tokens\n",
      "2023-05-29 18:48:33,437 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:48:33,844 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:48:33,845 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 18:48:49,613 - INFO - > [get_response] Total LLM token usage: 3999 tokens\n",
      "2023-05-29 18:48:49,614 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:48:50,020 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:48:50,021 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 18:49:08,510 - INFO - > [get_response] Total LLM token usage: 2873 tokens\n",
      "2023-05-29 18:49:08,510 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:49:08,907 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:49:08,908 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:49:39,294 - INFO - > [get_response] Total LLM token usage: 3664 tokens\n",
      "2023-05-29 18:49:39,295 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:49:40,172 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:49:40,173 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:50:15,586 - INFO - > [get_response] Total LLM token usage: 3523 tokens\n",
      "2023-05-29 18:50:15,587 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:50:16,025 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:50:16,025 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:50:41,682 - INFO - > [get_response] Total LLM token usage: 3591 tokens\n",
      "2023-05-29 18:50:41,683 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:50:42,306 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:50:42,306 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 18:51:20,182 - INFO - > [get_response] Total LLM token usage: 4169 tokens\n",
      "2023-05-29 18:51:20,183 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:51:20,687 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:51:20,688 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:51:38,485 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:51:38,486 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:51:42,788 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:51:42,789 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:51:47,100 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:51:47,101 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:51:57,733 - INFO - > [get_response] Total LLM token usage: 3143 tokens\n",
      "2023-05-29 18:51:57,734 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:51:58,129 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:51:58,130 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:52:20,768 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:52:20,769 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:52:29,030 - INFO - > [get_response] Total LLM token usage: 4088 tokens\n",
      "2023-05-29 18:52:29,031 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:52:29,515 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:52:29,515 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:53:22,512 - INFO - > [get_response] Total LLM token usage: 4170 tokens\n",
      "2023-05-29 18:53:22,513 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:53:23,102 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:53:23,103 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:53:44,801 - INFO - > [get_response] Total LLM token usage: 3434 tokens\n",
      "2023-05-29 18:53:44,802 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:53:45,273 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:53:45,273 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:54:18,147 - INFO - > [get_response] Total LLM token usage: 4054 tokens\n",
      "2023-05-29 18:54:18,148 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:54:18,567 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:54:18,567 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 18:54:26,373 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 18:54:26,374 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-29 18:54:53,553 - INFO - > [get_response] Total LLM token usage: 2664 tokens\n",
      "2023-05-29 18:54:53,554 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:54:54,017 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:54:54,018 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:55:13,830 - INFO - > [get_response] Total LLM token usage: 3613 tokens\n",
      "2023-05-29 18:55:13,831 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:55:14,264 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:55:14,265 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:55:47,150 - INFO - > [get_response] Total LLM token usage: 4130 tokens\n",
      "2023-05-29 18:55:47,151 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:55:47,742 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:55:47,742 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:56:42,071 - INFO - > [get_response] Total LLM token usage: 3134 tokens\n",
      "2023-05-29 18:56:42,072 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 18:56:42,493 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:56:42,493 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:57:09,994 - INFO - > [get_response] Total LLM token usage: 3502 tokens\n",
      "2023-05-29 18:57:09,995 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df6 = df.copy()\n",
    "\n",
    "for i, gt in df6.iterrows():\n",
    "    try:\n",
    "        df6.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df6.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:58:47,014 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:47,015 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:58:47,669 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:47,669 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 18:58:48,244 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:48,245 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:58:48,730 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:48,730 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:58:49,439 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:49,440 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 18:58:50,116 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:50,116 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 18:58:50,554 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:50,555 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:58:50,961 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:50,962 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:58:51,362 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:51,362 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 18:58:51,859 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:51,860 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 18:58:52,279 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:52,280 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:58:52,828 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:52,829 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 18:58:53,315 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:53,316 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 18:58:53,982 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:53,983 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:58:54,622 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:54,622 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:58:55,036 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:55,036 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:58:55,488 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:55,488 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 18:58:55,886 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:55,887 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:58:56,361 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:56,362 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 18:58:56,801 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:56,801 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:58:57,469 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:57,469 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 18:58:57,929 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:57,929 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:58:58,409 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:58,409 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 18:58:58,963 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:58,964 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 18:58:59,502 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:58:59,503 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 18:59:00,175 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:59:00,176 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 18:59:00,604 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 18:59:00,605 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df6, query_engine)\n",
    "df6.to_csv(\"exp_6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ef2fa87b1a468ca16250596aaba1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4667e6fab11549b4b80c53f2c4a8c7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833e907f22824aad9c9dd9ec11f6afea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72f35a8378646cda2ee3333fcb08716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't evaluate as there were four responses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 7 - Run 2 with different response mode - compact_accumulate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 19:48:16,544 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 19:48:16,546 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown mode: compact_accumulate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m index \u001b[39m=\u001b[39m GPTVectorStoreIndex\u001b[39m.\u001b[39mfrom_documents(documents)\n\u001b[1;32m     10\u001b[0m retriever \u001b[39m=\u001b[39m VectorIndexRetriever(\n\u001b[1;32m     11\u001b[0m     index\u001b[39m=\u001b[39mindex, \n\u001b[1;32m     12\u001b[0m     similarity_top_k\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m response_synthesizer \u001b[39m=\u001b[39m ResponseSynthesizer\u001b[39m.\u001b[39;49mfrom_args(\n\u001b[1;32m     16\u001b[0m     node_postprocessors\u001b[39m=\u001b[39;49m[\n\u001b[1;32m     17\u001b[0m         SimilarityPostprocessor(similarity_cutoff\u001b[39m=\u001b[39;49m\u001b[39m0.7\u001b[39;49m)\n\u001b[1;32m     18\u001b[0m     ],\n\u001b[1;32m     19\u001b[0m     response_mode \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcompact_accumulate\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/response/type.py#L7\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m query_engine \u001b[39m=\u001b[39m RetrieverQueryEngine(\n\u001b[1;32m     24\u001b[0m     retriever\u001b[39m=\u001b[39mretriever,\n\u001b[1;32m     25\u001b[0m     response_synthesizer\u001b[39m=\u001b[39mresponse_synthesizer,\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/chainlink/lib/python3.10/site-packages/llama_index/indices/query/response_synthesis.py:93\u001b[0m, in \u001b[0;36mResponseSynthesizer.from_args\u001b[0;34m(cls, service_context, streaming, use_async, text_qa_template, refine_template, simple_template, response_mode, response_kwargs, node_postprocessors, optimizer, verbose)\u001b[0m\n\u001b[1;32m     91\u001b[0m response_builder: Optional[BaseResponseBuilder] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m response_mode \u001b[39m!=\u001b[39m ResponseMode\u001b[39m.\u001b[39mNO_TEXT:\n\u001b[0;32m---> 93\u001b[0m     response_builder \u001b[39m=\u001b[39m get_response_builder(\n\u001b[1;32m     94\u001b[0m         service_context,\n\u001b[1;32m     95\u001b[0m         text_qa_template,\n\u001b[1;32m     96\u001b[0m         refine_template,\n\u001b[1;32m     97\u001b[0m         simple_template,\n\u001b[1;32m     98\u001b[0m         response_mode,\n\u001b[1;32m     99\u001b[0m         use_async\u001b[39m=\u001b[39;49muse_async,\n\u001b[1;32m    100\u001b[0m         streaming\u001b[39m=\u001b[39;49mstreaming,\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    103\u001b[0m     response_builder,\n\u001b[1;32m    104\u001b[0m     response_mode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     verbose,\n\u001b[1;32m    109\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/chainlink/lib/python3.10/site-packages/llama_index/indices/response/factory.py:78\u001b[0m, in \u001b[0;36mget_response_builder\u001b[0;34m(service_context, text_qa_template, refine_template, simple_template, mode, use_async, streaming)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m Accumulate(\n\u001b[1;32m     72\u001b[0m         service_context\u001b[39m=\u001b[39mservice_context,\n\u001b[1;32m     73\u001b[0m         text_qa_template\u001b[39m=\u001b[39mtext_qa_template,\n\u001b[1;32m     74\u001b[0m         streaming\u001b[39m=\u001b[39mstreaming,\n\u001b[1;32m     75\u001b[0m         use_async\u001b[39m=\u001b[39muse_async,\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown mode: \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown mode: compact_accumulate"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.,))\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index, \n",
    "    similarity_top_k=4,\n",
    ")\n",
    "\n",
    "response_synthesizer = ResponseSynthesizer.from_args(\n",
    "    node_postprocessors=[\n",
    "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    ],\n",
    "    response_mode = \"compact_accumulate\", # https://github.com/jerryjliu/llama_index/blob/main/llama_index/indices/response/type.py#L7\n",
    "\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df.copy()\n",
    "\n",
    "for i, gt in df7.iterrows():\n",
    "    try:\n",
    "        df7.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df7.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not on pip yet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 8 - LLM Reranker Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 19:55:10,316 - WARNING - Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "2023-05-29 19:58:28,119 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-29 19:58:28,120 - INFO - > [build_index_from_nodes] Total embedding token usage: 965021 tokens\n"
     ]
    }
   ],
   "source": [
    "# based on https://medium.com/llamaindex-blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "reranker = LLMRerank(choice_batch_size=5, top_n=3, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 19:58:40,446 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 19:58:40,448 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 19:59:12,845 - INFO - > [get_response] Total LLM token usage: 1968 tokens\n",
      "2023-05-29 19:59:12,846 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 19:59:14,173 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 19:59:14,177 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 20:00:03,577 - INFO - > [get_response] Total LLM token usage: 2354 tokens\n",
      "2023-05-29 20:00:03,577 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:00:04,361 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:00:04,362 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 20:00:26,520 - INFO - > [get_response] Total LLM token usage: 1277 tokens\n",
      "2023-05-29 20:00:26,521 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:00:27,360 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:00:27,361 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 20:00:59,929 - INFO - > [get_response] Total LLM token usage: 1504 tokens\n",
      "2023-05-29 20:00:59,930 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:01:00,760 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:01:00,761 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 20:01:23,896 - INFO - > [get_response] Total LLM token usage: 1692 tokens\n",
      "2023-05-29 20:01:23,897 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:01:24,811 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:01:24,812 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 20:01:38,483 - INFO - > [get_response] Total LLM token usage: 1516 tokens\n",
      "2023-05-29 20:01:38,484 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:01:39,292 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:01:39,292 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 20:02:23,060 - INFO - > [get_response] Total LLM token usage: 2078 tokens\n",
      "2023-05-29 20:02:23,061 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:02:24,016 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:02:24,016 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 20:02:34,046 - INFO - > [get_response] Total LLM token usage: 1536 tokens\n",
      "2023-05-29 20:02:34,047 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:02:34,707 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:02:34,708 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 20:02:44,181 - INFO - > [get_response] Total LLM token usage: 1581 tokens\n",
      "2023-05-29 20:02:44,181 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:02:44,940 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:02:44,941 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 20:02:53,566 - INFO - > [get_response] Total LLM token usage: 600 tokens\n",
      "2023-05-29 20:02:53,566 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:02:54,390 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:02:54,390 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 20:03:04,877 - INFO - > [get_response] Total LLM token usage: 1590 tokens\n",
      "2023-05-29 20:03:04,878 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:03:05,607 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:03:05,608 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 20:03:16,741 - INFO - > [get_response] Total LLM token usage: 1573 tokens\n",
      "2023-05-29 20:03:16,742 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:03:17,414 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:03:17,415 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 20:03:38,991 - INFO - > [get_response] Total LLM token usage: 1274 tokens\n",
      "2023-05-29 20:03:38,991 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:03:39,812 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:03:39,813 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 20:03:54,849 - INFO - > [get_response] Total LLM token usage: 1571 tokens\n",
      "2023-05-29 20:03:54,850 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:03:55,583 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:03:55,583 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 20:04:27,101 - INFO - > [get_response] Total LLM token usage: 1532 tokens\n",
      "2023-05-29 20:04:27,102 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:04:27,817 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:04:27,818 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 20:04:40,901 - INFO - > [get_response] Total LLM token usage: 1521 tokens\n",
      "2023-05-29 20:04:40,902 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:04:41,741 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:04:41,742 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 20:05:01,842 - INFO - > [get_response] Total LLM token usage: 1620 tokens\n",
      "2023-05-29 20:05:01,842 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:05:02,594 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:05:02,595 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 20:05:15,225 - INFO - > [get_response] Total LLM token usage: 1540 tokens\n",
      "2023-05-29 20:05:15,226 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:05:15,986 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:05:15,987 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 20:05:27,024 - INFO - > [get_response] Total LLM token usage: 1513 tokens\n",
      "2023-05-29 20:05:27,025 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:05:28,362 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:05:28,363 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 20:05:42,155 - INFO - > [get_response] Total LLM token usage: 1548 tokens\n",
      "2023-05-29 20:05:42,157 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:05:46,150 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:05:46,151 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 20:06:16,517 - INFO - error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0377cea693ddac0a950bb333e7b3f1d7 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-29 20:06:16,517 - WARNING - Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 0377cea693ddac0a950bb333e7b3f1d7 in your message.).\n",
      "2023-05-29 20:06:27,899 - INFO - > [get_response] Total LLM token usage: 1326 tokens\n",
      "2023-05-29 20:06:27,900 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:06:28,650 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:06:28,652 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 20:06:40,630 - INFO - > [get_response] Total LLM token usage: 1383 tokens\n",
      "2023-05-29 20:06:40,631 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:06:41,914 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:06:41,914 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 20:06:52,535 - INFO - > [get_response] Total LLM token usage: 1503 tokens\n",
      "2023-05-29 20:06:52,536 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:06:53,618 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:06:53,620 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 20:07:03,500 - INFO - > [get_response] Total LLM token usage: 1459 tokens\n",
      "2023-05-29 20:07:03,500 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:07:04,316 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:07:04,317 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 20:07:07,406 - ERROR - list index out of range\n",
      "2023-05-29 20:07:08,099 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:07:08,100 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 20:07:29,536 - INFO - > [get_response] Total LLM token usage: 1426 tokens\n",
      "2023-05-29 20:07:29,537 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-29 20:07:30,764 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:07:30,764 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 20:07:46,199 - INFO - > [get_response] Total LLM token usage: 1487 tokens\n",
      "2023-05-29 20:07:46,200 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df8 = df.copy()\n",
    "\n",
    "for i, gt in df8.iterrows():\n",
    "    try:\n",
    "        df8.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df8.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5dae9682bb24b2abbf2e2d9c3873cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfd4dc3ac6e42e8ae68b4d2a3d29db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a700a57a646b499f824f928d5fd402a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60be5e1bf6fc4f0db11b6cb4e0b99e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 20:16:39,742 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:39,743 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 20:16:40,586 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:40,587 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-29 20:16:41,414 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:41,416 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 20:16:42,314 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:42,315 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 20:16:43,094 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:43,096 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-29 20:16:43,826 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:43,827 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-29 20:16:44,596 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:44,597 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 20:16:45,367 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:45,367 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 20:16:46,002 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:46,002 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-29 20:16:46,871 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:46,872 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-29 20:16:47,731 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:47,732 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 20:16:48,932 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:48,933 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-29 20:16:49,824 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:49,824 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-29 20:16:52,716 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:52,717 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 20:16:54,104 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:54,105 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 20:16:54,857 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:54,858 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 20:16:55,611 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:55,612 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-29 20:16:56,260 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:56,261 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 20:16:56,999 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:57,000 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-29 20:16:57,636 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:57,636 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 20:16:58,388 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:58,389 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-29 20:16:59,014 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:59,014 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 20:16:59,639 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:16:59,640 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-29 20:17:00,460 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:17:00,460 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-29 20:17:01,175 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:17:01,176 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-29 20:17:01,820 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:17:01,820 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-29 20:17:02,759 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-29 20:17:02,760 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df8, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b469045ffa44635a500e1c8e719f894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96bfb5e0f7e4e0d9153ee32cceaf2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91218453d7524101a12a3ec9515e6f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e556e032b4f48e9a03c9d2a0476aca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df8, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8.to_csv(\"exp_8.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 9 - Run 8 with tree summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 08:42:05,780 - WARNING - Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "2023-05-30 08:45:55,838 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:45:55,839 - INFO - > [build_index_from_nodes] Total embedding token usage: 965021 tokens\n"
     ]
    }
   ],
   "source": [
    "# based on https://medium.com/llamaindex-blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n",
    "index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "reranker = LLMRerank(choice_batch_size=5, top_n=3, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    "    response_mode = \"tree_summarize\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 08:46:02,982 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:46:02,983 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 08:46:35,838 - INFO - > [get_response] Total LLM token usage: 2463 tokens\n",
      "2023-05-30 08:46:35,839 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:46:35,839 - INFO - > [get_response] Total LLM token usage: 2463 tokens\n",
      "2023-05-30 08:46:35,840 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:46:37,143 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:46:37,144 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 08:49:13,302 - INFO - > [get_response] Total LLM token usage: 6904 tokens\n",
      "2023-05-30 08:49:13,303 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:49:13,304 - INFO - > [get_response] Total LLM token usage: 6904 tokens\n",
      "2023-05-30 08:49:13,304 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:49:14,653 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:49:14,655 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 08:50:18,059 - INFO - > [get_response] Total LLM token usage: 3270 tokens\n",
      "2023-05-30 08:50:18,061 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:50:18,064 - INFO - > [get_response] Total LLM token usage: 3270 tokens\n",
      "2023-05-30 08:50:18,066 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:50:19,396 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:50:19,399 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 08:51:40,215 - INFO - > [get_response] Total LLM token usage: 3957 tokens\n",
      "2023-05-30 08:51:40,216 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:51:40,217 - INFO - > [get_response] Total LLM token usage: 3957 tokens\n",
      "2023-05-30 08:51:40,218 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:51:41,174 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:51:41,176 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 08:53:27,569 - INFO - > [get_response] Total LLM token usage: 4096 tokens\n",
      "2023-05-30 08:53:27,569 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:53:27,570 - INFO - > [get_response] Total LLM token usage: 4096 tokens\n",
      "2023-05-30 08:53:27,570 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:53:28,435 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:53:28,435 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 08:55:20,413 - INFO - > [get_response] Total LLM token usage: 4036 tokens\n",
      "2023-05-30 08:55:20,414 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:55:20,415 - INFO - > [get_response] Total LLM token usage: 4036 tokens\n",
      "2023-05-30 08:55:20,416 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:55:21,378 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:55:21,380 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 08:58:05,281 - INFO - > [get_response] Total LLM token usage: 7341 tokens\n",
      "2023-05-30 08:58:05,283 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:58:05,285 - INFO - > [get_response] Total LLM token usage: 7341 tokens\n",
      "2023-05-30 08:58:05,287 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:58:06,255 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:58:06,256 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 08:58:36,541 - INFO - error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a598c0fe751a2c9205ec281f446c95a3 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 08:58:36,545 - WARNING - Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a598c0fe751a2c9205ec281f446c95a3 in your message.).\n",
      "2023-05-30 08:59:07,014 - INFO - > [get_response] Total LLM token usage: 2468 tokens\n",
      "2023-05-30 08:59:07,015 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:59:07,017 - INFO - > [get_response] Total LLM token usage: 2468 tokens\n",
      "2023-05-30 08:59:07,020 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:59:08,049 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:59:08,051 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 08:59:33,116 - INFO - > [get_response] Total LLM token usage: 2409 tokens\n",
      "2023-05-30 08:59:33,117 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:59:33,118 - INFO - > [get_response] Total LLM token usage: 2409 tokens\n",
      "2023-05-30 08:59:33,119 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:59:34,042 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:59:34,043 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 08:59:40,880 - INFO - > [get_response] Total LLM token usage: 603 tokens\n",
      "2023-05-30 08:59:40,880 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:59:40,881 - INFO - > [get_response] Total LLM token usage: 603 tokens\n",
      "2023-05-30 08:59:40,881 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 08:59:41,754 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 08:59:41,755 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:00:09,205 - INFO - > [get_response] Total LLM token usage: 2349 tokens\n",
      "2023-05-30 09:00:09,206 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:00:09,206 - INFO - > [get_response] Total LLM token usage: 2349 tokens\n",
      "2023-05-30 09:00:09,206 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:00:09,988 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:00:09,989 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 09:00:39,315 - INFO - > [get_response] Total LLM token usage: 2475 tokens\n",
      "2023-05-30 09:00:39,316 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:00:39,317 - INFO - > [get_response] Total LLM token usage: 2475 tokens\n",
      "2023-05-30 09:00:39,317 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:00:40,100 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:00:40,100 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 09:01:21,939 - INFO - > [get_response] Total LLM token usage: 2030 tokens\n",
      "2023-05-30 09:01:21,940 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:01:21,940 - INFO - > [get_response] Total LLM token usage: 2030 tokens\n",
      "2023-05-30 09:01:21,941 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:01:22,747 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:01:22,748 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:02:16,154 - INFO - > [get_response] Total LLM token usage: 2907 tokens\n",
      "2023-05-30 09:02:16,156 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:02:16,157 - INFO - > [get_response] Total LLM token usage: 2907 tokens\n",
      "2023-05-30 09:02:16,158 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:02:17,119 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:02:17,120 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:03:18,224 - INFO - > [get_response] Total LLM token usage: 2714 tokens\n",
      "2023-05-30 09:03:18,226 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:03:18,227 - INFO - > [get_response] Total LLM token usage: 2714 tokens\n",
      "2023-05-30 09:03:18,228 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:03:19,421 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:03:19,423 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:04:24,257 - INFO - > [get_response] Total LLM token usage: 3159 tokens\n",
      "2023-05-30 09:04:24,258 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:04:24,258 - INFO - > [get_response] Total LLM token usage: 3159 tokens\n",
      "2023-05-30 09:04:24,259 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:04:25,214 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:04:25,214 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 09:05:02,311 - INFO - error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9728b871ea2716ec3fc075efaa65827c in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:05:02,312 - WARNING - Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9728b871ea2716ec3fc075efaa65827c in your message.).\n",
      "2023-05-30 09:06:04,516 - INFO - > [get_response] Total LLM token usage: 3214 tokens\n",
      "2023-05-30 09:06:04,518 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:06:04,519 - INFO - > [get_response] Total LLM token usage: 3214 tokens\n",
      "2023-05-30 09:06:04,520 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:06:06,116 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:06:06,117 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:06:47,401 - INFO - > [get_response] Total LLM token usage: 2632 tokens\n",
      "2023-05-30 09:06:47,402 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:06:47,403 - INFO - > [get_response] Total LLM token usage: 2632 tokens\n",
      "2023-05-30 09:06:47,404 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:06:48,541 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:06:48,543 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:07:38,535 - INFO - > [get_response] Total LLM token usage: 2774 tokens\n",
      "2023-05-30 09:07:38,536 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:07:38,537 - INFO - > [get_response] Total LLM token usage: 2774 tokens\n",
      "2023-05-30 09:07:38,538 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:07:39,715 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:07:39,716 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:08:38,382 - INFO - > [get_response] Total LLM token usage: 2880 tokens\n",
      "2023-05-30 09:08:38,384 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:08:38,385 - INFO - > [get_response] Total LLM token usage: 2880 tokens\n",
      "2023-05-30 09:08:38,385 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:08:39,318 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:08:39,319 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:09:05,672 - INFO - > [get_response] Total LLM token usage: 1905 tokens\n",
      "2023-05-30 09:09:05,676 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:09:05,678 - INFO - > [get_response] Total LLM token usage: 1905 tokens\n",
      "2023-05-30 09:09:05,682 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:09:06,666 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:09:06,667 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:09:46,340 - INFO - > [get_response] Total LLM token usage: 2172 tokens\n",
      "2023-05-30 09:09:46,343 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:09:46,344 - INFO - > [get_response] Total LLM token usage: 2172 tokens\n",
      "2023-05-30 09:09:46,345 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:09:47,412 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:09:47,415 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 09:10:48,543 - INFO - > [get_response] Total LLM token usage: 2933 tokens\n",
      "2023-05-30 09:10:48,545 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:10:48,545 - INFO - > [get_response] Total LLM token usage: 2933 tokens\n",
      "2023-05-30 09:10:48,546 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:10:49,698 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:10:49,701 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:11:39,000 - INFO - > [get_response] Total LLM token usage: 2630 tokens\n",
      "2023-05-30 09:11:39,002 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:11:39,003 - INFO - > [get_response] Total LLM token usage: 2630 tokens\n",
      "2023-05-30 09:11:39,003 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:11:40,117 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:11:40,119 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:11:43,141 - ERROR - list index out of range\n",
      "2023-05-30 09:11:44,177 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:11:44,181 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:12:29,225 - INFO - > [get_response] Total LLM token usage: 2248 tokens\n",
      "2023-05-30 09:12:29,227 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:12:29,229 - INFO - > [get_response] Total LLM token usage: 2248 tokens\n",
      "2023-05-30 09:12:29,230 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:12:30,312 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:12:30,313 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:13:02,191 - INFO - error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4342fff74f3de9a45d2a9f285a9191f2 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:13:02,194 - WARNING - Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4342fff74f3de9a45d2a9f285a9191f2 in your message.).\n",
      "2023-05-30 09:14:08,732 - INFO - error_code=None error_message='That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9862a10befd7e7e50c23eb3a37ac7641 in your message.)' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:14:08,733 - WARNING - Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 9862a10befd7e7e50c23eb3a37ac7641 in your message.).\n",
      "2023-05-30 09:14:24,101 - INFO - > [get_response] Total LLM token usage: 2692 tokens\n",
      "2023-05-30 09:14:24,102 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:14:24,103 - INFO - > [get_response] Total LLM token usage: 2692 tokens\n",
      "2023-05-30 09:14:24,104 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df9 = df.copy()\n",
    "\n",
    "for i, gt in df9.iterrows():\n",
    "    try:\n",
    "        df9.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df9.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1a036c42724eb49783c1a2b7b276d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a489f3dec849eb95d143c54f1c9092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67fc216d13694798abab9cdddf782256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea01922d42b4cfeacade3b57432ffa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 09:22:21,244 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:21,245 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 09:22:22,102 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:22,103 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 09:22:23,034 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:23,035 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:22:23,892 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:23,892 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 09:22:24,698 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:24,698 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 09:22:25,610 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:25,611 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 09:22:26,423 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:26,423 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 09:22:27,270 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:27,271 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:22:28,136 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:28,137 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 09:22:29,013 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:29,014 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 09:22:29,911 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:29,912 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:22:30,683 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:30,683 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 09:22:31,541 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:31,542 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 09:22:32,447 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:32,448 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:22:33,325 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:33,326 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:22:34,316 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:34,317 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:22:35,112 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:35,113 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 09:22:35,844 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:35,844 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:22:36,837 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:36,838 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:22:37,611 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:37,612 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:22:38,569 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:38,570 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:22:39,310 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:39,311 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:22:40,199 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:40,199 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 09:22:41,070 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:41,070 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:22:41,922 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:41,923 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:22:42,642 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:42,643 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:22:43,546 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:22:43,547 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df9, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6c293e573a4bf1aab17b8f603b3ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be05647475e476baccdfed5127a5eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a960444a310414daee6099a782c3dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0d895255114403b26f8a24a5c56378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df9, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9.to_csv(\"exp_9.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 10 - Forward/Backward Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.indices.postprocessor.node import PrevNextNodePostprocessor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.storage_context import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 09:31:39,377 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:31:39,381 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults()\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "index = GPTVectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_postprocessor = PrevNextNodePostprocessor(docstore=docstore, num_nodes=4)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[node_postprocessor],\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 09:33:03,897 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:33:03,898 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 09:33:28,210 - INFO - > [get_response] Total LLM token usage: 3065 tokens\n",
      "2023-05-30 09:33:28,213 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:33:28,216 - INFO - > [get_response] Total LLM token usage: 3065 tokens\n",
      "2023-05-30 09:33:28,217 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:33:28,962 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:33:28,964 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 09:33:29,105 - INFO - > Building index from nodes: 2 chunks\n",
      "2023-05-30 09:33:55,114 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:33:55,116 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:34:24,432 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:34:24,433 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:34:28,765 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:34:28,766 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:34:33,095 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:34:33,096 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:35:28,284 - INFO - > [get_response] Total LLM token usage: 1186 tokens\n",
      "2023-05-30 09:35:28,287 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:35:28,289 - INFO - > [get_response] Total LLM token usage: 12085 tokens\n",
      "2023-05-30 09:35:28,290 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:35:28,834 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:35:28,836 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:35:49,872 - INFO - > [get_response] Total LLM token usage: 2501 tokens\n",
      "2023-05-30 09:35:49,872 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:35:49,873 - INFO - > [get_response] Total LLM token usage: 2501 tokens\n",
      "2023-05-30 09:35:49,874 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:35:50,347 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:35:50,348 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 09:36:14,674 - INFO - > [get_response] Total LLM token usage: 1673 tokens\n",
      "2023-05-30 09:36:14,676 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:36:14,676 - INFO - > [get_response] Total LLM token usage: 1673 tokens\n",
      "2023-05-30 09:36:14,677 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:36:15,265 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:36:15,267 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 09:36:15,301 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:36:15,691 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:36:15,693 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:37:18,426 - INFO - > [get_response] Total LLM token usage: 882 tokens\n",
      "2023-05-30 09:37:18,427 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:37:18,428 - INFO - > [get_response] Total LLM token usage: 6083 tokens\n",
      "2023-05-30 09:37:18,429 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:37:19,036 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:37:19,038 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 09:37:19,090 - INFO - > Building index from nodes: 2 chunks\n",
      "2023-05-30 09:37:19,949 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:37:19,950 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:38:12,698 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:38:12,700 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:38:45,774 - INFO - > [get_response] Total LLM token usage: 1066 tokens\n",
      "2023-05-30 09:38:45,776 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:38:45,778 - INFO - > [get_response] Total LLM token usage: 9994 tokens\n",
      "2023-05-30 09:38:45,779 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:38:46,626 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:38:46,628 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 09:38:46,701 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:39:57,061 - INFO - > [get_response] Total LLM token usage: 991 tokens\n",
      "2023-05-30 09:39:57,061 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:39:57,062 - INFO - > [get_response] Total LLM token usage: 7591 tokens\n",
      "2023-05-30 09:39:57,063 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:39:57,654 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:39:57,655 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:39:57,724 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:40:10,460 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:40:10,461 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:40:20,873 - INFO - > [get_response] Total LLM token usage: 206 tokens\n",
      "2023-05-30 09:40:20,874 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:40:20,874 - INFO - > [get_response] Total LLM token usage: 6595 tokens\n",
      "2023-05-30 09:40:20,875 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:40:21,356 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:40:21,356 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 09:40:21,445 - INFO - > Building index from nodes: 2 chunks\n",
      "2023-05-30 09:40:51,296 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:40:51,297 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:41:25,593 - INFO - > [get_response] Total LLM token usage: 631 tokens\n",
      "2023-05-30 09:41:25,595 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:25,596 - INFO - > [get_response] Total LLM token usage: 11977 tokens\n",
      "2023-05-30 09:41:25,597 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:26,518 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:41:26,520 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 09:41:29,759 - INFO - > [get_response] Total LLM token usage: 2968 tokens\n",
      "2023-05-30 09:41:29,760 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:29,761 - INFO - > [get_response] Total LLM token usage: 2968 tokens\n",
      "2023-05-30 09:41:29,761 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:30,546 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:41:30,547 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:41:33,760 - INFO - > [get_response] Total LLM token usage: 2376 tokens\n",
      "2023-05-30 09:41:33,762 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:33,763 - INFO - > [get_response] Total LLM token usage: 2376 tokens\n",
      "2023-05-30 09:41:33,764 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:34,358 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:41:34,359 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 09:41:34,387 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:41:48,574 - INFO - > [get_response] Total LLM token usage: 146 tokens\n",
      "2023-05-30 09:41:48,576 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:48,577 - INFO - > [get_response] Total LLM token usage: 4177 tokens\n",
      "2023-05-30 09:41:48,577 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:49,261 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:41:49,262 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 09:41:57,506 - INFO - > [get_response] Total LLM token usage: 752 tokens\n",
      "2023-05-30 09:41:57,507 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:57,508 - INFO - > [get_response] Total LLM token usage: 752 tokens\n",
      "2023-05-30 09:41:57,509 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:41:58,072 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:41:58,073 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:42:08,522 - INFO - > [get_response] Total LLM token usage: 1717 tokens\n",
      "2023-05-30 09:42:08,523 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:42:08,523 - INFO - > [get_response] Total LLM token usage: 1717 tokens\n",
      "2023-05-30 09:42:08,524 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:42:09,111 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:42:09,112 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:42:09,181 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:42:52,058 - INFO - > [get_response] Total LLM token usage: 397 tokens\n",
      "2023-05-30 09:42:52,059 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:42:52,059 - INFO - > [get_response] Total LLM token usage: 5758 tokens\n",
      "2023-05-30 09:42:52,060 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:42:52,831 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:42:52,832 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:42:52,896 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:42:53,288 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:42:53,290 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:43:26,793 - INFO - > [get_response] Total LLM token usage: 294 tokens\n",
      "2023-05-30 09:43:26,794 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:43:26,795 - INFO - > [get_response] Total LLM token usage: 5887 tokens\n",
      "2023-05-30 09:43:26,795 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:43:27,580 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:43:27,581 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 09:43:27,627 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:44:04,617 - INFO - > [get_response] Total LLM token usage: 390 tokens\n",
      "2023-05-30 09:44:04,618 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:44:04,619 - INFO - > [get_response] Total LLM token usage: 7181 tokens\n",
      "2023-05-30 09:44:04,619 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:44:05,404 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:44:05,404 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:44:07,136 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:44:07,138 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:44:19,706 - INFO - > [get_response] Total LLM token usage: 2645 tokens\n",
      "2023-05-30 09:44:19,707 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:44:19,708 - INFO - > [get_response] Total LLM token usage: 2645 tokens\n",
      "2023-05-30 09:44:19,708 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:44:20,484 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:44:20,485 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:44:20,523 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:44:56,426 - INFO - > [get_response] Total LLM token usage: 323 tokens\n",
      "2023-05-30 09:44:56,427 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:44:56,428 - INFO - > [get_response] Total LLM token usage: 6674 tokens\n",
      "2023-05-30 09:44:56,428 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:44:57,989 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:44:57,990 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:44:58,029 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:45:35,956 - INFO - > [get_response] Total LLM token usage: 386 tokens\n",
      "2023-05-30 09:45:35,957 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:45:35,958 - INFO - > [get_response] Total LLM token usage: 7323 tokens\n",
      "2023-05-30 09:45:35,958 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:45:36,931 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:45:36,933 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:45:44,192 - INFO - > [get_response] Total LLM token usage: 1340 tokens\n",
      "2023-05-30 09:45:44,195 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:45:44,196 - INFO - > [get_response] Total LLM token usage: 1340 tokens\n",
      "2023-05-30 09:45:44,197 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:45:45,089 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:45:45,090 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:45:45,491 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:45:45,492 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:46:08,546 - INFO - > [get_response] Total LLM token usage: 3895 tokens\n",
      "2023-05-30 09:46:08,550 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:46:08,553 - INFO - > [get_response] Total LLM token usage: 3895 tokens\n",
      "2023-05-30 09:46:08,555 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:46:09,587 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:46:09,589 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 09:46:19,720 - INFO - > [get_response] Total LLM token usage: 2291 tokens\n",
      "2023-05-30 09:46:19,721 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:46:19,723 - INFO - > [get_response] Total LLM token usage: 2291 tokens\n",
      "2023-05-30 09:46:19,724 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:46:20,663 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:46:20,664 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:46:25,421 - INFO - > [get_response] Total LLM token usage: 1501 tokens\n",
      "2023-05-30 09:46:25,422 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:46:25,423 - INFO - > [get_response] Total LLM token usage: 1501 tokens\n",
      "2023-05-30 09:46:25,423 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:46:26,024 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:46:26,029 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:46:26,078 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:46:44,947 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:46:44,949 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:47:00,339 - INFO - > [get_response] Total LLM token usage: 336 tokens\n",
      "2023-05-30 09:47:00,340 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:47:00,341 - INFO - > [get_response] Total LLM token usage: 5931 tokens\n",
      "2023-05-30 09:47:00,342 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:47:01,135 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:47:01,138 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:47:01,204 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:47:14,083 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:47:14,086 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:47:28,352 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 09:47:28,354 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 09:47:45,381 - INFO - > [get_response] Total LLM token usage: 416 tokens\n",
      "2023-05-30 09:47:45,382 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:47:45,382 - INFO - > [get_response] Total LLM token usage: 5812 tokens\n",
      "2023-05-30 09:47:45,383 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:47:46,452 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:47:46,453 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:47:46,556 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 09:48:11,468 - INFO - > [get_response] Total LLM token usage: 280 tokens\n",
      "2023-05-30 09:48:11,469 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 09:48:11,471 - INFO - > [get_response] Total LLM token usage: 6149 tokens\n",
      "2023-05-30 09:48:11,473 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df10 = df.copy()\n",
    "\n",
    "for i, gt in df10.iterrows():\n",
    "    try:\n",
    "        df10.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df10.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ca6d2140ca4d90a84dc4c6a2b9d261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc232a43add14924963cf34d3062f3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bc01e52cc045de86e7ed6444abb228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b907699845874a29a1dc0a3763639d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 09:52:16,419 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:16,420 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 09:52:18,338 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:18,339 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 09:52:18,809 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:18,811 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:52:19,291 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:19,292 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 09:52:19,952 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:19,953 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 09:52:20,526 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:20,527 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 09:52:21,336 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:21,337 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 09:52:21,919 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:21,920 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:52:24,262 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:24,264 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 09:52:25,363 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:25,364 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 09:52:26,019 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:26,021 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:52:27,092 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:27,094 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 09:52:27,607 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:27,609 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 09:52:28,270 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:28,271 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:52:28,853 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:28,854 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:52:29,689 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:29,690 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:52:30,412 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:30,413 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 09:52:31,086 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:31,087 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:52:31,560 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:31,562 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 09:52:32,182 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:32,183 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:52:33,626 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:33,628 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 09:52:34,162 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:34,164 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:52:34,909 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:34,910 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 09:52:35,601 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:35,602 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 09:52:36,169 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:36,170 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 09:52:36,782 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:36,783 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 09:52:38,053 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:52:38,054 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df10, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a43824a8f241c8ac568d31a1d69267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38ac55fd8004851a08aff0ff76759d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcb2786c6694240b7efe56f23650e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a2f86a13d74fc4ab86146077cb76fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df10, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10.to_csv(\"exp_10.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 11 - run 10 but auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor.node import AutoPrevNextNodePostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 09:58:27,898 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-30 09:58:27,898 - INFO - > [build_index_from_nodes] Total embedding token usage: 701713 tokens\n"
     ]
    }
   ],
   "source": [
    "service_context = ServiceContext.from_defaults()\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(documents)\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "index = GPTVectorStoreIndex(nodes, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_postprocessor = AutoPrevNextNodePostprocessor(docstore=docstore, num_nodes=4, service_context=service_context)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[node_postprocessor],\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:04:01,313 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:04:01,314 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 10:04:02,511 - INFO - > [get_response] Total LLM token usage: 1198 tokens\n",
      "2023-05-30 10:04:02,511 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:02,512 - INFO - > [get_response] Total LLM token usage: 1198 tokens\n",
      "2023-05-30 10:04:02,512 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:03,482 - INFO - > [get_response] Total LLM token usage: 820 tokens\n",
      "2023-05-30 10:04:03,484 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:03,485 - INFO - > [get_response] Total LLM token usage: 820 tokens\n",
      "2023-05-30 10:04:03,486 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:30,118 - INFO - > [get_response] Total LLM token usage: 2063 tokens\n",
      "2023-05-30 10:04:30,118 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:30,119 - INFO - > [get_response] Total LLM token usage: 2063 tokens\n",
      "2023-05-30 10:04:30,119 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:30,800 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:04:30,800 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 10:04:32,192 - INFO - > [get_response] Total LLM token usage: 1168 tokens\n",
      "2023-05-30 10:04:32,193 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:32,194 - INFO - > [get_response] Total LLM token usage: 1168 tokens\n",
      "2023-05-30 10:04:32,195 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:33,400 - INFO - > [get_response] Total LLM token usage: 1182 tokens\n",
      "2023-05-30 10:04:33,400 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:33,401 - INFO - > [get_response] Total LLM token usage: 1182 tokens\n",
      "2023-05-30 10:04:33,401 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:55,491 - INFO - > [get_response] Total LLM token usage: 2332 tokens\n",
      "2023-05-30 10:04:55,492 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:55,493 - INFO - > [get_response] Total LLM token usage: 2332 tokens\n",
      "2023-05-30 10:04:55,493 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:56,272 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:04:56,273 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 10:04:57,916 - INFO - > [get_response] Total LLM token usage: 1205 tokens\n",
      "2023-05-30 10:04:57,916 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:57,917 - INFO - > [get_response] Total LLM token usage: 1205 tokens\n",
      "2023-05-30 10:04:57,918 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:04:58,231 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:04:58,232 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:05:02,564 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:05:02,565 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:05:07,568 - INFO - > [get_response] Total LLM token usage: 812 tokens\n",
      "2023-05-30 10:05:07,568 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:05:07,569 - INFO - > [get_response] Total LLM token usage: 812 tokens\n",
      "2023-05-30 10:05:07,570 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:05:35,660 - INFO - > [get_response] Total LLM token usage: 1994 tokens\n",
      "2023-05-30 10:05:35,661 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:05:35,661 - INFO - > [get_response] Total LLM token usage: 1994 tokens\n",
      "2023-05-30 10:05:35,662 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:05:36,563 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:05:36,563 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 10:05:37,621 - INFO - > [get_response] Total LLM token usage: 1196 tokens\n",
      "2023-05-30 10:05:37,622 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:05:37,623 - INFO - > [get_response] Total LLM token usage: 1196 tokens\n",
      "2023-05-30 10:05:37,623 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:05:37,919 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:05:37,920 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:05:42,217 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:05:42,218 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:05:47,350 - INFO - > [get_response] Total LLM token usage: 478 tokens\n",
      "2023-05-30 10:05:47,351 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:05:47,351 - INFO - > [get_response] Total LLM token usage: 478 tokens\n",
      "2023-05-30 10:05:47,352 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:13,508 - INFO - > [get_response] Total LLM token usage: 1673 tokens\n",
      "2023-05-30 10:06:13,509 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:13,509 - INFO - > [get_response] Total LLM token usage: 1673 tokens\n",
      "2023-05-30 10:06:13,510 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:13,923 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:06:13,924 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 10:06:15,094 - INFO - > [get_response] Total LLM token usage: 1259 tokens\n",
      "2023-05-30 10:06:15,095 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:15,095 - INFO - > [get_response] Total LLM token usage: 1259 tokens\n",
      "2023-05-30 10:06:15,096 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:18,158 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 10:06:18,159 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:18,159 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 10:06:18,160 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:46,044 - INFO - > [get_response] Total LLM token usage: 2463 tokens\n",
      "2023-05-30 10:06:46,044 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:46,045 - INFO - > [get_response] Total LLM token usage: 2463 tokens\n",
      "2023-05-30 10:06:46,045 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:46,459 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:06:46,459 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 10:06:47,541 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 10:06:47,541 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:47,542 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 10:06:47,542 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:48,845 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 10:06:48,845 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:06:48,846 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 10:06:48,847 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:14,921 - INFO - > [get_response] Total LLM token usage: 2431 tokens\n",
      "2023-05-30 10:07:14,922 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:14,923 - INFO - > [get_response] Total LLM token usage: 2431 tokens\n",
      "2023-05-30 10:07:14,923 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:15,652 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:07:15,653 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 10:07:16,639 - INFO - > [get_response] Total LLM token usage: 816 tokens\n",
      "2023-05-30 10:07:16,642 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:16,644 - INFO - > [get_response] Total LLM token usage: 816 tokens\n",
      "2023-05-30 10:07:16,644 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:17,937 - INFO - > [get_response] Total LLM token usage: 1240 tokens\n",
      "2023-05-30 10:07:17,938 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:17,939 - INFO - > [get_response] Total LLM token usage: 1240 tokens\n",
      "2023-05-30 10:07:17,940 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:45,922 - INFO - > [get_response] Total LLM token usage: 2052 tokens\n",
      "2023-05-30 10:07:45,923 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:45,923 - INFO - > [get_response] Total LLM token usage: 2052 tokens\n",
      "2023-05-30 10:07:45,924 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:46,532 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:07:46,533 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 10:07:47,507 - INFO - > [get_response] Total LLM token usage: 1197 tokens\n",
      "2023-05-30 10:07:47,508 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:47,508 - INFO - > [get_response] Total LLM token usage: 1197 tokens\n",
      "2023-05-30 10:07:47,509 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:48,483 - INFO - > [get_response] Total LLM token usage: 1120 tokens\n",
      "2023-05-30 10:07:48,484 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:48,485 - INFO - > [get_response] Total LLM token usage: 1120 tokens\n",
      "2023-05-30 10:07:48,485 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:58,356 - INFO - > [get_response] Total LLM token usage: 2047 tokens\n",
      "2023-05-30 10:07:58,357 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:58,357 - INFO - > [get_response] Total LLM token usage: 2047 tokens\n",
      "2023-05-30 10:07:58,358 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:07:59,294 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:07:59,295 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 10:08:03,925 - INFO - > [get_response] Total LLM token usage: 1274 tokens\n",
      "2023-05-30 10:08:03,926 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:03,926 - INFO - > [get_response] Total LLM token usage: 1274 tokens\n",
      "2023-05-30 10:08:03,927 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:03,927 - ERROR - Invalid prediction: \n",
      "0x72AFAECF99C9d9C8215fF44C77B94B99C28741e8\n",
      "2023-05-30 10:08:04,544 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:08:04,545 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 10:08:07,135 - INFO - > [get_response] Total LLM token usage: 1281 tokens\n",
      "2023-05-30 10:08:07,136 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:07,137 - INFO - > [get_response] Total LLM token usage: 1281 tokens\n",
      "2023-05-30 10:08:07,137 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:07,137 - ERROR - Invalid prediction: \n",
      "0x5586bF404C7A22A4a4077401272cE5945f80189C\n",
      "2023-05-30 10:08:07,957 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:08:07,958 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 10:08:08,356 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:08:08,357 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:08:13,558 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 10:08:13,559 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:13,560 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 10:08:13,560 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:14,941 - INFO - > [get_response] Total LLM token usage: 1075 tokens\n",
      "2023-05-30 10:08:14,942 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:14,942 - INFO - > [get_response] Total LLM token usage: 1075 tokens\n",
      "2023-05-30 10:08:14,943 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:18,789 - INFO - > [get_response] Total LLM token usage: 1891 tokens\n",
      "2023-05-30 10:08:18,790 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:18,790 - INFO - > [get_response] Total LLM token usage: 1891 tokens\n",
      "2023-05-30 10:08:18,791 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:19,236 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:08:19,237 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 10:08:19,566 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:08:19,567 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:08:25,177 - INFO - > [get_response] Total LLM token usage: 1255 tokens\n",
      "2023-05-30 10:08:25,178 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:25,179 - INFO - > [get_response] Total LLM token usage: 1255 tokens\n",
      "2023-05-30 10:08:25,180 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:27,098 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 10:08:27,098 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:27,099 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 10:08:27,099 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:31,500 - INFO - > [get_response] Total LLM token usage: 2152 tokens\n",
      "2023-05-30 10:08:31,501 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:31,501 - INFO - > [get_response] Total LLM token usage: 2152 tokens\n",
      "2023-05-30 10:08:31,502 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:32,407 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:08:32,408 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 10:08:33,269 - INFO - > [get_response] Total LLM token usage: 567 tokens\n",
      "2023-05-30 10:08:33,270 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:33,270 - INFO - > [get_response] Total LLM token usage: 567 tokens\n",
      "2023-05-30 10:08:33,270 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:35,072 - INFO - > [get_response] Total LLM token usage: 456 tokens\n",
      "2023-05-30 10:08:35,073 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:35,073 - INFO - > [get_response] Total LLM token usage: 456 tokens\n",
      "2023-05-30 10:08:35,074 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:43,852 - INFO - > [get_response] Total LLM token usage: 752 tokens\n",
      "2023-05-30 10:08:43,853 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:43,854 - INFO - > [get_response] Total LLM token usage: 752 tokens\n",
      "2023-05-30 10:08:43,854 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:44,616 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:08:44,617 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 10:08:45,582 - INFO - > [get_response] Total LLM token usage: 708 tokens\n",
      "2023-05-30 10:08:45,583 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:45,583 - INFO - > [get_response] Total LLM token usage: 708 tokens\n",
      "2023-05-30 10:08:45,584 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:46,619 - INFO - > [get_response] Total LLM token usage: 1220 tokens\n",
      "2023-05-30 10:08:46,620 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:46,621 - INFO - > [get_response] Total LLM token usage: 1220 tokens\n",
      "2023-05-30 10:08:46,621 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:53,826 - INFO - > [get_response] Total LLM token usage: 1641 tokens\n",
      "2023-05-30 10:08:53,827 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:53,828 - INFO - > [get_response] Total LLM token usage: 1641 tokens\n",
      "2023-05-30 10:08:53,828 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:54,412 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:08:54,413 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 10:08:55,562 - INFO - > [get_response] Total LLM token usage: 1100 tokens\n",
      "2023-05-30 10:08:55,562 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:55,563 - INFO - > [get_response] Total LLM token usage: 1100 tokens\n",
      "2023-05-30 10:08:55,563 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:56,598 - INFO - > [get_response] Total LLM token usage: 541 tokens\n",
      "2023-05-30 10:08:56,599 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:08:56,600 - INFO - > [get_response] Total LLM token usage: 541 tokens\n",
      "2023-05-30 10:08:56,600 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:04,220 - INFO - > [get_response] Total LLM token usage: 1354 tokens\n",
      "2023-05-30 10:09:04,221 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:04,222 - INFO - > [get_response] Total LLM token usage: 1354 tokens\n",
      "2023-05-30 10:09:04,223 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:04,754 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:09:04,755 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 10:09:05,480 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:09:05,481 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:09:10,531 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:09:10,532 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:09:15,717 - INFO - > [get_response] Total LLM token usage: 514 tokens\n",
      "2023-05-30 10:09:15,718 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:15,719 - INFO - > [get_response] Total LLM token usage: 514 tokens\n",
      "2023-05-30 10:09:15,720 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:16,040 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:09:16,041 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:09:21,110 - INFO - > [get_response] Total LLM token usage: 1193 tokens\n",
      "2023-05-30 10:09:21,111 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:21,112 - INFO - > [get_response] Total LLM token usage: 1193 tokens\n",
      "2023-05-30 10:09:21,113 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:32,451 - INFO - > [get_response] Total LLM token usage: 1473 tokens\n",
      "2023-05-30 10:09:32,452 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:32,452 - INFO - > [get_response] Total LLM token usage: 1473 tokens\n",
      "2023-05-30 10:09:32,453 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:32,930 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:09:32,931 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 10:09:34,608 - INFO - > [get_response] Total LLM token usage: 1222 tokens\n",
      "2023-05-30 10:09:34,609 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:34,609 - INFO - > [get_response] Total LLM token usage: 1222 tokens\n",
      "2023-05-30 10:09:34,610 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:35,150 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:09:35,151 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:09:39,467 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:09:39,468 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:09:44,588 - INFO - > [get_response] Total LLM token usage: 1271 tokens\n",
      "2023-05-30 10:09:44,589 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:44,590 - INFO - > [get_response] Total LLM token usage: 1271 tokens\n",
      "2023-05-30 10:09:44,590 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:54,720 - INFO - > [get_response] Total LLM token usage: 2214 tokens\n",
      "2023-05-30 10:09:54,721 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:54,722 - INFO - > [get_response] Total LLM token usage: 2214 tokens\n",
      "2023-05-30 10:09:54,722 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:09:55,470 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:09:55,470 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 10:09:55,781 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:09:55,783 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:10:00,818 - INFO - > [get_response] Total LLM token usage: 643 tokens\n",
      "2023-05-30 10:10:00,819 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:00,820 - INFO - > [get_response] Total LLM token usage: 643 tokens\n",
      "2023-05-30 10:10:00,821 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:01,891 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 10:10:01,891 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:01,892 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 10:10:01,892 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:07,926 - INFO - > [get_response] Total LLM token usage: 1580 tokens\n",
      "2023-05-30 10:10:07,927 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:07,927 - INFO - > [get_response] Total LLM token usage: 1580 tokens\n",
      "2023-05-30 10:10:07,928 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:08,517 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:10:08,518 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 10:10:08,841 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:10:08,841 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:10:14,083 - INFO - > [get_response] Total LLM token usage: 1098 tokens\n",
      "2023-05-30 10:10:14,084 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:14,084 - INFO - > [get_response] Total LLM token usage: 1098 tokens\n",
      "2023-05-30 10:10:14,085 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:14,418 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:10:14,418 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:10:19,459 - INFO - > [get_response] Total LLM token usage: 1155 tokens\n",
      "2023-05-30 10:10:19,460 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:19,460 - INFO - > [get_response] Total LLM token usage: 1155 tokens\n",
      "2023-05-30 10:10:19,461 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:31,177 - INFO - > [get_response] Total LLM token usage: 2028 tokens\n",
      "2023-05-30 10:10:31,177 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:31,178 - INFO - > [get_response] Total LLM token usage: 2028 tokens\n",
      "2023-05-30 10:10:31,178 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:31,784 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:10:31,785 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 10:10:32,788 - INFO - > [get_response] Total LLM token usage: 1158 tokens\n",
      "2023-05-30 10:10:32,789 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:32,789 - INFO - > [get_response] Total LLM token usage: 1158 tokens\n",
      "2023-05-30 10:10:32,790 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:34,229 - INFO - > [get_response] Total LLM token usage: 1212 tokens\n",
      "2023-05-30 10:10:34,229 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:34,230 - INFO - > [get_response] Total LLM token usage: 1212 tokens\n",
      "2023-05-30 10:10:34,231 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:34,607 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:10:34,609 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:10:55,794 - INFO - > [get_response] Total LLM token usage: 2190 tokens\n",
      "2023-05-30 10:10:55,795 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:55,795 - INFO - > [get_response] Total LLM token usage: 2190 tokens\n",
      "2023-05-30 10:10:55,796 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:56,205 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:10:56,206 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 10:10:57,566 - INFO - > [get_response] Total LLM token usage: 1147 tokens\n",
      "2023-05-30 10:10:57,567 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:57,567 - INFO - > [get_response] Total LLM token usage: 1147 tokens\n",
      "2023-05-30 10:10:57,568 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:58,601 - INFO - > [get_response] Total LLM token usage: 458 tokens\n",
      "2023-05-30 10:10:58,602 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:10:58,603 - INFO - > [get_response] Total LLM token usage: 458 tokens\n",
      "2023-05-30 10:10:58,604 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:05,884 - INFO - > [get_response] Total LLM token usage: 1340 tokens\n",
      "2023-05-30 10:11:05,885 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:05,885 - INFO - > [get_response] Total LLM token usage: 1340 tokens\n",
      "2023-05-30 10:11:05,886 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:06,706 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:11:06,707 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 10:11:08,264 - INFO - > [get_response] Total LLM token usage: 842 tokens\n",
      "2023-05-30 10:11:08,265 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:08,266 - INFO - > [get_response] Total LLM token usage: 842 tokens\n",
      "2023-05-30 10:11:08,266 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:09,518 - INFO - > [get_response] Total LLM token usage: 1228 tokens\n",
      "2023-05-30 10:11:09,518 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:09,519 - INFO - > [get_response] Total LLM token usage: 1228 tokens\n",
      "2023-05-30 10:11:09,519 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:09,552 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 10:11:10,428 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:11:10,429 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:11:14,789 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:11:14,790 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:11:52,360 - INFO - > [get_response] Total LLM token usage: 374 tokens\n",
      "2023-05-30 10:11:52,361 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:52,362 - INFO - > [get_response] Total LLM token usage: 6551 tokens\n",
      "2023-05-30 10:11:52,363 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:54,998 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:11:54,999 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 10:11:56,099 - INFO - > [get_response] Total LLM token usage: 808 tokens\n",
      "2023-05-30 10:11:56,100 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:56,101 - INFO - > [get_response] Total LLM token usage: 808 tokens\n",
      "2023-05-30 10:11:56,102 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:57,587 - INFO - > [get_response] Total LLM token usage: 1159 tokens\n",
      "2023-05-30 10:11:57,588 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:11:57,589 - INFO - > [get_response] Total LLM token usage: 1159 tokens\n",
      "2023-05-30 10:11:57,590 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:09,986 - INFO - > [get_response] Total LLM token usage: 1743 tokens\n",
      "2023-05-30 10:12:09,986 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:09,987 - INFO - > [get_response] Total LLM token usage: 1743 tokens\n",
      "2023-05-30 10:12:09,987 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:10,779 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:12:10,780 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 10:12:11,116 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:12:11,117 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:12:15,741 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:12:15,742 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:12:20,075 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:12:20,076 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:12:24,384 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:12:24,385 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:12:32,705 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:12:32,709 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:12:44,034 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 10:12:44,034 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:44,035 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 10:12:44,036 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:46,142 - INFO - > [get_response] Total LLM token usage: 677 tokens\n",
      "2023-05-30 10:12:46,143 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:46,145 - INFO - > [get_response] Total LLM token usage: 677 tokens\n",
      "2023-05-30 10:12:46,146 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:50,303 - INFO - > [get_response] Total LLM token usage: 1501 tokens\n",
      "2023-05-30 10:12:50,303 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:50,304 - INFO - > [get_response] Total LLM token usage: 1501 tokens\n",
      "2023-05-30 10:12:50,305 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:50,768 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:12:50,769 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 10:12:52,051 - INFO - > [get_response] Total LLM token usage: 1201 tokens\n",
      "2023-05-30 10:12:52,052 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:52,052 - INFO - > [get_response] Total LLM token usage: 1201 tokens\n",
      "2023-05-30 10:12:52,053 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:53,285 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 10:12:53,286 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:12:53,287 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 10:12:53,287 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:03,860 - INFO - > [get_response] Total LLM token usage: 2173 tokens\n",
      "2023-05-30 10:13:03,861 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:03,861 - INFO - > [get_response] Total LLM token usage: 2173 tokens\n",
      "2023-05-30 10:13:03,862 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:04,450 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:13:04,451 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 10:13:05,543 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 10:13:05,544 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:05,544 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 10:13:05,545 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:06,447 - INFO - > [get_response] Total LLM token usage: 547 tokens\n",
      "2023-05-30 10:13:06,447 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:06,448 - INFO - > [get_response] Total LLM token usage: 547 tokens\n",
      "2023-05-30 10:13:06,448 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:06,788 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:13:06,789 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:13:25,068 - INFO - > [get_response] Total LLM token usage: 1474 tokens\n",
      "2023-05-30 10:13:25,069 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:25,070 - INFO - > [get_response] Total LLM token usage: 1474 tokens\n",
      "2023-05-30 10:13:25,070 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:25,729 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:13:25,730 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 10:13:26,050 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:13:26,051 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:13:31,531 - INFO - > [get_response] Total LLM token usage: 1131 tokens\n",
      "2023-05-30 10:13:31,532 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:31,533 - INFO - > [get_response] Total LLM token usage: 1131 tokens\n",
      "2023-05-30 10:13:31,534 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:31,890 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:13:31,891 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:13:37,320 - INFO - > [get_response] Total LLM token usage: 1101 tokens\n",
      "2023-05-30 10:13:37,321 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:37,321 - INFO - > [get_response] Total LLM token usage: 1101 tokens\n",
      "2023-05-30 10:13:37,322 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:37,730 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:13:37,731 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:13:54,929 - INFO - > [get_response] Total LLM token usage: 2001 tokens\n",
      "2023-05-30 10:13:54,930 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:13:54,931 - INFO - > [get_response] Total LLM token usage: 2001 tokens\n",
      "2023-05-30 10:13:54,932 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df11 = df.copy()\n",
    "\n",
    "for i, gt in df11.iterrows():\n",
    "    try:\n",
    "        df11.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df11.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a376fd10f2472b97421b59910a69f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdc8b37af9e4c92b21a9d3849941ff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba88a7249b641018768d2aa474d5152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47461f001f94cec8b136d4296ca7f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:18:00,114 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:00,115 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 10:18:00,825 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:00,826 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 10:18:01,911 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:01,912 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 10:18:02,686 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:02,687 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 10:18:03,219 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:03,220 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 10:18:03,937 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:03,938 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 10:18:04,569 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:04,570 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 10:18:05,606 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:05,607 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 10:18:06,166 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:06,167 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 10:18:06,727 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:06,728 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 10:18:07,324 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:07,324 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 10:18:08,070 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:08,071 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 10:18:08,699 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:08,700 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 10:18:09,838 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:09,839 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 10:18:10,363 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:10,364 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 10:18:11,200 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:11,201 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 10:18:11,625 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:11,626 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 10:18:12,297 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:12,298 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 10:18:12,939 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:12,940 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 10:18:13,522 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:13,523 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 10:18:13,971 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:13,972 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 10:18:14,490 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:14,491 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 10:18:15,052 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:15,053 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 10:18:15,567 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:15,568 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 10:18:16,183 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:16,183 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 10:18:16,838 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:16,839 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 10:18:18,131 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 10:18:18,132 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df11, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19f45b32223424fac950eba15a87e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e968752775e24e038de1aba078c18755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08fc11322b4f4e97adace3bf90998ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603b74b49d5d43c6a28c91195a163a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df11, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11.to_csv(\"exp_11.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/llamaindex-blog/a-new-document-summary-index-for-llm-powered-qa-systems-9a32ece2f9ec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    LLMPredictor,\n",
    "    ServiceContext,\n",
    "    ResponseSynthesizer\n",
    ")\n",
    "from llama_index.indices.document_summary import GPTDocumentSummaryIndex\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:24:49,990 - WARNING - Unknown max input size for gpt-3.5-turbo, using defaults.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<llama_index.indices.query.response_synthesis.ResponseSynthesizer object at 0x7f884f478d90>\n",
      "current doc id: ad238f1e-a9f5-431f-bd5b-43d20435888a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:24:53,080 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:24:53,082 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:25:07,858 - INFO - > [get_response] Total LLM token usage: 524 tokens\n",
      "2023-05-30 10:25:07,859 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:25:07,859 - INFO - > [get_response] Total LLM token usage: 524 tokens\n",
      "2023-05-30 10:25:07,859 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:25:07,860 - INFO - > Generated summary for doc ad238f1e-a9f5-431f-bd5b-43d20435888a: \n",
      "This document is a guide to Chainlink, a decentralized oracle network that provides data feeds, functions, automation, and verifiable random number generation for smart contracts. It provides a Getting Started Guide for those new to Chainlink and Smart Contracts, as well as product information and resources for Node Operators, tutorials, and integration support. This document can answer questions about how to use Chainlink, how to start and maintain Chainlink Nodes, and how to access tutorials and integration support.\n",
      "2023-05-30 10:25:07,938 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ed47107f-70c2-4988-97bf-4e1312a55ef6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:25:33,117 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:25:33,118 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:25:37,488 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:25:37,489 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:25:41,878 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:25:41,880 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:25:46,261 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:25:46,262 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:26:15,267 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:26:15,268 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:26:32,087 - INFO - > [get_response] Total LLM token usage: 606 tokens\n",
      "2023-05-30 10:26:32,087 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:26:32,088 - INFO - > [get_response] Total LLM token usage: 14945 tokens\n",
      "2023-05-30 10:26:32,088 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:26:32,088 - INFO - > Generated summary for doc ed47107f-70c2-4988-97bf-4e1312a55ef6: \n",
      "This document provides an overview of the Chainlink API, including functions, events, constants, and modifiers. It also provides an explanation of the Chainlink Request struct, which is used to store the parameters of a Chainlink request and its corresponding response callback. This document can answer questions about how to use the Chainlink API reference to interact with the Chainlink oracle network, what functions are available, what events are emitted, how to use the Chainlink Request struct, how to set up a ChainlinkClient contract, how to send requests to an Oracle or Operator, how to secure fulfillment callbacks, and how to track unfulfilled requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 6817cbe8-b0d1-45d0-8f35-1ccd0b85e752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:26:32,432 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:26:32,433 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:26:54,990 - INFO - > [get_response] Total LLM token usage: 1042 tokens\n",
      "2023-05-30 10:26:54,991 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:26:54,991 - INFO - > [get_response] Total LLM token usage: 1042 tokens\n",
      "2023-05-30 10:26:54,992 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:26:54,992 - INFO - > Generated summary for doc 6817cbe8-b0d1-45d0-8f35-1ccd0b85e752: \n",
      "This document explains how to find an existing Oracle Job to suit the needs of an API call. It introduces Oracles and explains how they can be configured to perform a wide range of tasks. It provides instructions on how to find a job, including joining the Chainlink operator-requests discord channel and using Testnet Oracles. It also provides instructions on how to deploy testnet nodes and external adapters on naas.link and how to run your own testnet nodes. \n",
      "\n",
      "This document can answer questions such as: How do I find an existing Oracle Job? How do I configure an Oracle to perform a wide range of tasks? How do I join the Chainlink operator-requests discord channel? How do I use Testnet Oracles? How do I deploy testnet nodes and external adapters on naas.link? How do I run my own testnet nodes?\n",
      "2023-05-30 10:26:55,018 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ef4fe2eb-2e5c-4643-9f60-4f1f1bcd4e8c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:27:40,619 - INFO - > [get_response] Total LLM token usage: 499 tokens\n",
      "2023-05-30 10:27:40,620 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:27:40,620 - INFO - > [get_response] Total LLM token usage: 5590 tokens\n",
      "2023-05-30 10:27:40,621 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:27:40,622 - INFO - > Generated summary for doc ef4fe2eb-2e5c-4643-9f60-4f1f1bcd4e8c: \n",
      "This document provides an example of how to make an HTTP GET request to an external API from a smart contract using Chainlink's Request & Receive Data cycle. It explains how to call an API, fetch a specific information from the response, and create a Chainlink request to retrieve the API response. It also provides instructions on how to use the contract in Remix, including setting the Chainlink Token address, Oracle contract address, and jobId. Additionally, it explains how to use Chainlink's Any-API to make a GET request and receive an array response. This document can answer questions about how to make an HTTP GET request to an external API from a smart contract, how to fetch a specific information from the response, how to use the contract in Remix, how to use Chainlink's Any-API to make a GET request and receive an array response, as well as how to set the LINK token address, Oracle, and JobId.\n",
      "2023-05-30 10:27:40,657 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: e98c180c-fca8-435b-ad86-ba4af1d7a140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:28:22,171 - INFO - > [get_response] Total LLM token usage: 466 tokens\n",
      "2023-05-30 10:28:22,173 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:28:22,174 - INFO - > [get_response] Total LLM token usage: 5458 tokens\n",
      "2023-05-30 10:28:22,174 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:28:22,175 - INFO - > Generated summary for doc e98c180c-fca8-435b-ad86-ba4af1d7a140: \n",
      "This document provides an overview of how to use an existing Chainlink job to make a GET request to the Etherscan gas tracker API. It explains the prerequisites for using the job, provides an example of the response, and explains how to configure the Chainlink request in a smart contract. It also explains how to acquire testnet LINK and fund the contract, as well as how to use Chainlink's Any-API to make an existing job request. This document can answer questions about how to use an existing Chainlink job to make a GET request, what parameters are needed for the request, how to acquire and fund the contract, the different data types that can be requested, and how to set the LINK token address, Oracle, and JobId.\n",
      "2023-05-30 10:28:22,213 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: af5b6926-b61b-4611-b725-f48c8c065067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:29:04,331 - INFO - > [get_response] Total LLM token usage: 437 tokens\n",
      "2023-05-30 10:29:04,332 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:29:04,333 - INFO - > [get_response] Total LLM token usage: 5234 tokens\n",
      "2023-05-30 10:29:04,334 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:29:04,334 - INFO - > Generated summary for doc af5b6926-b61b-4611-b725-f48c8c065067: \n",
      "This document provides an overview of how to make an HTTP GET request to an external API from a smart contract using Chainlink's Request & Receive Data cycle and receive large responses. It explains the prerequisites, provides an example, and explains how to fund the contract. Additionally, it explains the different data types that can be used, such as uint256, int256, bool, string, bytes32, and bytes, and how to set the LINK token address, Oracle, and JobId for a contract. This document can answer questions about how to make an HTTP GET request to an external API from a smart contract, how to receive large responses, how to fund a contract, the Chainlink Any-API, data types, setting the LINK token address, Oracle, and JobId, and finding existing jobs.\n",
      "2023-05-30 10:29:04,371 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 8ef52f0f-96b2-4858-98e4-3107c9190b5d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:29:43,367 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:29:43,368 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:29:47,662 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:29:47,663 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:30:07,283 - INFO - > [get_response] Total LLM token usage: 528 tokens\n",
      "2023-05-30 10:30:07,284 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:30:07,285 - INFO - > [get_response] Total LLM token usage: 6405 tokens\n",
      "2023-05-30 10:30:07,285 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:30:07,286 - INFO - > Generated summary for doc 8ef52f0f-96b2-4858-98e4-3107c9190b5d: \n",
      "This document provides instructions on how to use the Chainlink Any-API to make a request for multiple variables from a public API. It explains how to make an HTTP GET request to an external API from a smart contract, receive multiple responses, include the necessary parameters in the request, acquire testnet LINK and fund the contract, and use an example contract. It also provides an example contract with hardcoded values and un-audited code. This document can answer questions about how to make an HTTP GET request to an external API from a smart contract, how to receive multiple responses, what parameters should be included in the request, how to acquire testnet LINK and fund the contract, and how to use an example contract.\n",
      "2023-05-30 10:30:07,328 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 549a1553-76a7-4466-acc3-ebd2b6ca969f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:30:52,787 - INFO - > [get_response] Total LLM token usage: 520 tokens\n",
      "2023-05-30 10:30:52,788 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:30:52,788 - INFO - > [get_response] Total LLM token usage: 6496 tokens\n",
      "2023-05-30 10:30:52,788 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:30:52,789 - INFO - > Generated summary for doc 549a1553-76a7-4466-acc3-ebd2b6ca969f: \n",
      "This document provides an example of how to make an HTTP GET request to an external API from a smart contract using Chainlink's Request & Receive Data cycle and receive a single response. It explains how to fetch a single word response in a single call, and provides an example of how to call the Cryptocompare GET /data/pricemultifull API. It also explains how to use the ChainlinkClient contract to build the API request. This document can answer questions about how to make an HTTP GET request to an external API from a smart contract, how to fetch a single word response in a single call, how to use the ChainlinkClient contract to build the API request, what response types are available, and how to set the LINK token address, Oracle, and JobId.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 764b2de3-b1e0-485f-a7db-7854ba3e9885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:30:53,136 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:30:53,137 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:31:12,401 - INFO - > [get_response] Total LLM token usage: 1578 tokens\n",
      "2023-05-30 10:31:12,402 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:31:12,403 - INFO - > [get_response] Total LLM token usage: 1578 tokens\n",
      "2023-05-30 10:31:12,403 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:31:12,404 - INFO - > Generated summary for doc 764b2de3-b1e0-485f-a7db-7854ba3e9885: \n",
      "This document provides an overview of how to make HTTP GET requests to external APIs from smart contracts using Chainlink's Request & Receive Data cycle. It explains how to set the LINK token address, Oracle, and JobId, as well as provides examples of how to make requests for single word responses, multi-variable responses, data from an array, large responses, and existing job requests. This document can answer questions about how to make GET requests to external APIs from smart contracts, how to set the LINK token address, Oracle, and JobId, and how to make requests for different types of data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 596eadc3-49bf-4651-ac49-8aefadc75f9f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:31:25,725 - INFO - > [get_response] Total LLM token usage: 2098 tokens\n",
      "2023-05-30 10:31:25,725 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:31:25,726 - INFO - > [get_response] Total LLM token usage: 2098 tokens\n",
      "2023-05-30 10:31:25,727 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:31:25,727 - INFO - > Generated summary for doc 596eadc3-49bf-4651-ac49-8aefadc75f9f: \n",
      "This document provides an overview of Chainlink Any API, which enables smart contracts to access any external data source through a decentralized oracle network. It explains how to make HTTP GET requests and parse the JSON response to retrieve the values of multiple attributes. It also explains how to call a job that leverages external adapters and returns the relevant data to the smart contract. This document can answer questions about how to make an HTTP GET request, how to parse a JSON response, how to call a job that leverages external adapters, and how to use Chainlink Any API to get data on chain.\n",
      "2023-05-30 10:31:25,748 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 9d121c58-7687-4131-b4d0-7381623b136d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:31:26,094 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:31:26,095 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:31:49,877 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:31:49,878 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:31:54,301 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:31:54,302 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:32:11,327 - INFO - > [get_response] Total LLM token usage: 360 tokens\n",
      "2023-05-30 10:32:11,327 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:11,328 - INFO - > [get_response] Total LLM token usage: 4528 tokens\n",
      "2023-05-30 10:32:11,328 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:11,329 - INFO - > Generated summary for doc 9d121c58-7687-4131-b4d0-7381623b136d: \n",
      "This document provides information about Chainlink's testnet oracles, which can be used to quickly test implementations. It includes information about the LINK token address and faucet details, operator contracts, jobs, job IDs, and examples. Additionally, it outlines how to use Chainlink oracles to access data from the CoinGecko API. This document can answer questions about the LINK token address, operator contracts, jobs, job IDs, examples of how to use the testnet oracles, how to use Chainlink oracles to access data from the CoinGecko API, how to make a request to the API, and how to set up a callback method to handle the response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: a6c8e4ab-7957-43c1-abf3-ddd778efa483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:32:24,521 - INFO - > [get_response] Total LLM token usage: 1699 tokens\n",
      "2023-05-30 10:32:24,521 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:24,522 - INFO - > [get_response] Total LLM token usage: 1699 tokens\n",
      "2023-05-30 10:32:24,522 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:24,523 - INFO - > Generated summary for doc a6c8e4ab-7957-43c1-abf3-ddd778efa483: \n",
      "This document provides an overview of the decentralized data model used by Chainlink Data Feeds. It explains how data aggregation is used to produce Chainlink Data Feeds, how the data feed is updated by a decentralized oracle network, and the components of a decentralized oracle network. It also provides an example of how to create a consumer contract that uses an existing data feed. This document can answer questions about how data aggregation is used to produce Chainlink Data Feeds, how the data feed is updated by a decentralized oracle network, and the components of a decentralized oracle network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: cd023a2a-69e0-431c-a22e-dae95d1eeeae\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:32:37,741 - INFO - > [get_response] Total LLM token usage: 806 tokens\n",
      "2023-05-30 10:32:37,742 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:37,743 - INFO - > [get_response] Total LLM token usage: 806 tokens\n",
      "2023-05-30 10:32:37,743 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:37,745 - INFO - > Generated summary for doc cd023a2a-69e0-431c-a22e-dae95d1eeeae: \n",
      "This document provides an overview of the Chainlink Data Feeds Architecture, which is a decentralized oracle network that connects smart contracts with external data. It covers the Basic Request Model, Decentralized Data Model, and Off-Chain Reporting. The Basic Request Model describes how to make a GET request using a single oracle, the Decentralized Data Model explains how data is aggregated from a decentralized network of independent oracle nodes, and Off-Chain Reporting describes how nodes communicate using a peer to peer network. This document can answer questions about how to make a GET request using a single oracle, how data is aggregated from a decentralized network, and how nodes communicate using a peer to peer network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f8b3405b-5716-4f41-a214-23c01bad0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:32:58,814 - INFO - > [get_response] Total LLM token usage: 1687 tokens\n",
      "2023-05-30 10:32:58,815 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:58,816 - INFO - > [get_response] Total LLM token usage: 1687 tokens\n",
      "2023-05-30 10:32:58,816 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:32:58,817 - INFO - > Generated summary for doc f8b3405b-5716-4f41-a214-23c01bad0770: \n",
      "This document provides an overview of the Chainlink request model, which is a system that enables smart contracts to access data from oracles. It covers the ChainlinkClient contract, the LINK token, the Oracle contract, the off-chain oracle node, and the consumer UML. The document explains how the ChainlinkClient contract initiates a request to an oracle, how the LINK token is used to transfer and call logic in the receiving contract, and how the Oracle contract handles requests and emits an OracleRequest event. It also explains how the off-chain oracle node listens for the OracleRequest event and performs a job, and how the consumer UML is used to structure the contract. \n",
      "\n",
      "This document can answer questions about the Chainlink request model, such as how the ChainlinkClient contract initiates a request, how the LINK token is used, how the Oracle contract handles requests, and how the off-chain oracle node performs a job. It can also answer questions about the consumer UML and how it is used to structure the contract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: efb6cab4-e1d5-4bbe-ae54-8c14c01be133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:33:12,936 - INFO - > [get_response] Total LLM token usage: 1080 tokens\n",
      "2023-05-30 10:33:12,937 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:33:12,937 - INFO - > [get_response] Total LLM token usage: 1080 tokens\n",
      "2023-05-30 10:33:12,937 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:33:12,938 - INFO - > Generated summary for doc efb6cab4-e1d5-4bbe-ae54-8c14c01be133: \n",
      "This document provides an overview of Off-Chain Reporting (OCR), a significant step towards increasing the decentralization and scalability of Chainlink networks. OCR is a protocol that allows nodes to aggregate their observations into a single report off-chain using a secure P2P network. A single node then submits a transaction with the aggregated report to the chain. This saves postage and packaging fees and all effort the carrier associates with transporting multiple boxes. Benefits of OCR include reduced network congestion, lower gas costs, increased scalability, and faster data feed updates. This document can answer questions about the OCR protocol, its benefits, and how it works.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 1241a0cd-6342-4c65-a2d2-88c1ba1eadde\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:33:31,450 - INFO - > [get_response] Total LLM token usage: 1699 tokens\n",
      "2023-05-30 10:33:31,451 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:33:31,451 - INFO - > [get_response] Total LLM token usage: 1699 tokens\n",
      "2023-05-30 10:33:31,451 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:33:31,452 - INFO - > Generated summary for doc 1241a0cd-6342-4c65-a2d2-88c1ba1eadde: \n",
      "This document provides an overview of the economics of Chainlink Automation, including the cost of using the service, how funding works, withdrawing funds, node competition, minimum balance, price selection and gas bumping, and ERC-677 Link. It explains that there is no registration fee or other fees for any off-chain computation, and that the LINK balance of an upkeep will be reduced by a fee when an on-chain transaction is performed. It also explains how to add and withdraw funds, the minimum balance requirement, and how Automation Nodes select the gas price. \n",
      "\n",
      "This document can answer questions such as: What is the cost of using Chainlink Automation? How does funding work? How do I add and withdraw funds? What is the minimum balance requirement? How do Automation Nodes select the gas price?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 12e95c33-f9f4-400b-8ad2-39726e4fd993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:33:31,874 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:33:31,875 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:33:53,406 - INFO - > [get_response] Total LLM token usage: 2856 tokens\n",
      "2023-05-30 10:33:53,407 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:33:53,408 - INFO - > [get_response] Total LLM token usage: 2856 tokens\n",
      "2023-05-30 10:33:53,408 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:33:53,409 - INFO - > Generated summary for doc 12e95c33-f9f4-400b-8ad2-39726e4fd993: \n",
      "This document provides release notes for Chainlink Automation, a decentralized oracle network that enables smart contracts to securely access off-chain data. It covers the launch of Automation on Optimism, the renaming of Chainlink Keepers to Chainlink Automation, the launch of v1.3 and v1.2 on various networks, the introduction of automatic upkeep registration approval, programmatic control, advanced turn-taking algorithm, durable ID and user-triggered migration, configurable upkeeps, off-chain compute improvements, and minimum spend requirement. It also covers the manual migration of upkeeps from v1.1 to v1.2, underfunded upkeep notifications, and Keepers on Fantom, Avalanche, Ethereum Rinkeby, Binance Smart Chain, and Polygon.\n",
      "\n",
      "This document can answer questions about the features of Chainlink Automation, the supported networks, the manual migration of upkeeps from v1.1 to v1.2, and underfunded upkeep notifications.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ddc3f3b6-b612-4084-94cd-71313c0ce9f2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:34:06,775 - INFO - > [get_response] Total LLM token usage: 1442 tokens\n",
      "2023-05-30 10:34:06,776 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:34:06,778 - INFO - > [get_response] Total LLM token usage: 1442 tokens\n",
      "2023-05-30 10:34:06,778 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:34:06,779 - INFO - > Generated summary for doc ddc3f3b6-b612-4084-94cd-71313c0ce9f2: \n",
      "This document outlines best practices for using Chainlink Automation when creating compatible contracts. It recommends revalidating conditions and data in the performUpkeep function before work is performed, performing actions only when conditions are met in performUpkeep, performing upkeep only when data is verified via performData, and testing the contract. This document can answer questions about how to use Chainlink Automation securely and reliably, how to revalidate conditions and data in performUpkeep, how to perform actions only when conditions are met, how to perform upkeep only when data is verified, and how to test a contract.\n",
      "2023-05-30 10:34:06,828 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 895f2513-272d-4222-bda5-6df758ce53ed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:34:07,226 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:34:07,227 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:34:37,473 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:34:37,473 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:34:52,329 - INFO - > [get_response] Total LLM token usage: 421 tokens\n",
      "2023-05-30 10:34:52,329 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:34:52,330 - INFO - > [get_response] Total LLM token usage: 5844 tokens\n",
      "2023-05-30 10:34:52,330 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:34:52,331 - INFO - > Generated summary for doc 895f2513-272d-4222-bda5-6df758ce53ed: \n",
      "This document provides an overview of how to create compatible contracts for Chainlink Automation. It includes an example contract, considerations and best practices, and a description of the functions checkUpkeep and performUpkeep. It also provides instructions on how to deploy the contract and links to more complex examples. This document can answer questions about how to create compatible contracts for Chainlink Automation, how to deploy the contract, how to use the `checkUpkeep` and `performUpkeep` functions, how to determine the gas limit for `performUpkeep`, and how to ensure that the function is idempotent.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 859a493a-c4bc-43ba-b742-4a7d407374ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:35:07,027 - INFO - > [get_response] Total LLM token usage: 2877 tokens\n",
      "2023-05-30 10:35:07,028 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:35:07,029 - INFO - > [get_response] Total LLM token usage: 2877 tokens\n",
      "2023-05-30 10:35:07,030 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:35:07,031 - INFO - > Generated summary for doc 859a493a-c4bc-43ba-b742-4a7d407374ce: \n",
      "This document provides answers to frequently asked questions about Chainlink Automation, a network that enables users to automate tasks on the Ethereum blockchain. It covers topics such as the cost of using Chainlink Automation, how to determine the gas limit for an upkeep, the maximum gas that can be used for checkUpkeep and performUpkeep, how often an upkeep will be checked, how to join the Chainlink Automation Network as a node operator, and how to debug an upkeep that has stopped performing. It also provides information on how to fund and withdraw funds from an upkeep, and how to convert ERC-20 LINK tokens to ERC-677 LINK tokens.\n",
      "2023-05-30 10:35:07,082 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 9a6b3eba-c5a4-4f2b-8f90-8a704a7847f0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:35:07,448 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:35:07,450 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:36:10,599 - INFO - > [get_response] Total LLM token usage: 626 tokens\n",
      "2023-05-30 10:36:10,599 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:10,600 - INFO - > [get_response] Total LLM token usage: 9337 tokens\n",
      "2023-05-30 10:36:10,600 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:10,601 - INFO - > Generated summary for doc 9a6b3eba-c5a4-4f2b-8f90-8a704a7847f0: \n",
      "This document provides a guide on how to use Chainlink Automation to reduce gas fees, enhance the resilience of dApps, and improve end-user experience. It provides examples of how to deploy a contract to Chainlink Automation, register UpKeep for a contract, convert Chainlink tokens (LINK) to be ERC-677 compatible, and perform complex computations with no gas fees. It can answer questions about how to deploy a contract to Chainlink Automation, register UpKeep for a contract, convert Chainlink tokens (LINK) to be ERC-677 compatible, reduce gas fees and enhance the resilience of dApps, and perform complex computations with no gas fees.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: bd95e652-1be5-4c5f-917a-3ce99e1adf03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:36:21,885 - INFO - > [get_response] Total LLM token usage: 2568 tokens\n",
      "2023-05-30 10:36:21,887 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:21,887 - INFO - > [get_response] Total LLM token usage: 2568 tokens\n",
      "2023-05-30 10:36:21,888 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:21,889 - INFO - > Generated summary for doc bd95e652-1be5-4c5f-917a-3ce99e1adf03: \n",
      "This document provides an introduction to Chainlink Automation, a decentralized automation platform that uses external node operators to securely execute smart contracts. It explains how to use time-based and custom logic triggers to automate smart contracts, as well as how to register and manage Upkeeps in the Chainlink Automation App. It also covers the cost of using Chainlink Automation, the supported networks, and other tutorials. This document can answer questions about how to use Chainlink Automation, the cost of using it, the supported networks, and other tutorials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: e6fa2ce8-c63b-4af0-b630-447e2fb1672d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:36:32,791 - INFO - > [get_response] Total LLM token usage: 1667 tokens\n",
      "2023-05-30 10:36:32,793 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:32,794 - INFO - > [get_response] Total LLM token usage: 1667 tokens\n",
      "2023-05-30 10:36:32,796 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:32,798 - INFO - > Generated summary for doc e6fa2ce8-c63b-4af0-b630-447e2fb1672d: \n",
      "This document provides a guide on how to register a time-based Upkeep on the Chainlink Automation network. It explains how to connect a wallet, select a trigger, specify a time schedule using a CRON expression, and enter Upkeep details. It also provides information on the Job Scheduler Gas requirements and the need to use ERC-677 LINK for registration. This document can answer questions about registering a time-based Upkeep on the Chainlink Automation network, connecting a wallet, selecting a trigger, specifying a time schedule, entering Upkeep details, Job Scheduler Gas requirements, and the need to use ERC-677 LINK for registration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b3b313d9-016f-42c5-910f-fb1823ef1a15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:36:33,194 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:36:33,195 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:36:47,882 - INFO - > [get_response] Total LLM token usage: 973 tokens\n",
      "2023-05-30 10:36:47,883 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:47,883 - INFO - > [get_response] Total LLM token usage: 973 tokens\n",
      "2023-05-30 10:36:47,884 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:36:47,885 - INFO - > Generated summary for doc b3b313d9-016f-42c5-910f-fb1823ef1a15: \n",
      "This document provides an overview of how to manage Upkeeps on the Chainlink Automation Network. It explains how to fund an Upkeep, maintain a minimum balance, withdraw funds, and interact directly with the Chainlink Automation Registry. This document can answer questions about how to fund an Upkeep, how to maintain a minimum balance, how to withdraw funds, and how to interact directly with the Chainlink Automation Registry.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 4b785204-e834-4425-b337-1f97646e768a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:37:02,344 - INFO - > [get_response] Total LLM token usage: 2011 tokens\n",
      "2023-05-30 10:37:02,345 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:37:02,346 - INFO - > [get_response] Total LLM token usage: 2011 tokens\n",
      "2023-05-30 10:37:02,346 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:37:02,347 - INFO - > Generated summary for doc 4b785204-e834-4425-b337-1f97646e768a: \n",
      "This document provides an overview of Chainlink Automation, a decentralized system that enables developers to execute smart contract functions based on conditions that they specify. It describes the three main actors in the ecosystem (Upkeeps, Automation Registry, and Automation Nodes) and the architecture of the Chainlink Automation Network. It also provides information about the Automation Contracts, how the system works, and the internal monitoring and cost associated with using Chainlink Automation. This document can answer questions about the actors in the Chainlink Automation ecosystem, the architecture of the network, the Automation Contracts, how the system works, and the cost associated with using Chainlink Automation.\n",
      "2023-05-30 10:37:02,389 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 96595346-5469-4ec7-b794-3e234fe9d1a9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:37:25,532 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:37:25,533 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:37:40,428 - INFO - > [get_response] Total LLM token usage: 384 tokens\n",
      "2023-05-30 10:37:40,430 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:37:40,430 - INFO - > [get_response] Total LLM token usage: 7983 tokens\n",
      "2023-05-30 10:37:40,431 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:37:40,432 - INFO - > Generated summary for doc 96595346-5469-4ec7-b794-3e234fe9d1a9: \n",
      "This document provides instructions on how to register a custom logic Upkeep using the Chainlink Automation App or from within a contract. It outlines the parameters needed for the registerAndPredictID function, provides an example contract, and explains how to fund the Upkeep, use ERC677 LINK, and follow best practices. This document can answer questions about how to register an Upkeep, what parameters are needed, how to fund an Upkeep, and how to test and follow best practices.\n",
      "2023-05-30 10:37:40,496 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c3a9c968-3987-4d6e-8e61-ed9058a6f4a4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:38:15,975 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:38:15,976 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:38:36,349 - INFO - > [get_response] Total LLM token usage: 584 tokens\n",
      "2023-05-30 10:38:36,351 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:38:36,352 - INFO - > [get_response] Total LLM token usage: 10903 tokens\n",
      "2023-05-30 10:38:36,352 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:38:36,353 - INFO - > Generated summary for doc c3a9c968-3987-4d6e-8e61-ed9058a6f4a4: \n",
      "This document provides an overview of the supported blockchain networks for Chainlink Automation, including Ethereum, BNB Chain, Polygon (Matic), Avalanche, Fantom, Arbitrum, and Optimism. It includes a list of the networks, parameters, and configurations for each network, such as the registry address, registrar address, payment premium %, block count per turn, check gas limit, perform gas limit, gas ceiling multiplier, and minimum upkeep spend (LINK). This document can answer questions about which networks are supported by Chainlink Automation, what parameters are used for each network, what configurations are available for each network, what is the registry address for a given network, what is the payment premium for a given network, and what is the minimum upkeep spend for a given network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f5129cdf-bdf4-4b5e-8e2a-ef6bebf302a4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:38:52,037 - INFO - > [get_response] Total LLM token usage: 1119 tokens\n",
      "2023-05-30 10:38:52,040 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:38:52,041 - INFO - > [get_response] Total LLM token usage: 1119 tokens\n",
      "2023-05-30 10:38:52,043 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:38:52,045 - INFO - > Generated summary for doc f5129cdf-bdf4-4b5e-8e2a-ef6bebf302a4: \n",
      "This document provides an overview of Chainlink Automation example contracts, which are tools to help quickly deploy Chainlink Automation for specific use-cases. It includes information about the EthBalanceMonitor, Vault Harvester, Batch NFT Reveal, Dynamic NFTs, VRF Subscription Balance Monitor, Counting dNFT, and Batch reveal Demo app contracts. This document can answer questions about how to use Chainlink Automation to automate the monitoring of Upkeep for registered contracts, how to use Chainlink Automation to automate the process of compounding yield farm reward tokens, how to use Chainlink Automation with batch NFT reveals, how to create dynamic NFTs using Chainlink Automation, and how to automate the monitoring of a VRF subscription balance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: cdba7bed-597e-4341-b8e5-0d51fdad2ee6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:38:52,568 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:38:52,571 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:38:56,903 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:38:56,904 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:39:13,201 - INFO - > [get_response] Total LLM token usage: 3060 tokens\n",
      "2023-05-30 10:39:13,202 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:39:13,204 - INFO - > [get_response] Total LLM token usage: 3060 tokens\n",
      "2023-05-30 10:39:13,205 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:39:13,207 - INFO - > Generated summary for doc cdba7bed-597e-4341-b8e5-0d51fdad2ee6: \n",
      "This document provides an overview of the EthBalanceMonitor contract, which is an Automation contract that monitors and funds Ethereum addresses based on a configurable threshold. It explains the features of the contract, such as its ownable, pauseable, and compatible properties, as well as its functions. It also provides instructions on how to register Upkeep and run the contract. This document can answer questions about the EthBalanceMonitor contract, such as what its properties are, what its functions are, and how to use it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 3776a070-166c-45e7-a230-23a5d72f8d11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:39:26,194 - INFO - > [get_response] Total LLM token usage: 1613 tokens\n",
      "2023-05-30 10:39:26,195 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:39:26,195 - INFO - > [get_response] Total LLM token usage: 1613 tokens\n",
      "2023-05-30 10:39:26,196 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:39:26,196 - INFO - > Generated summary for doc 3776a070-166c-45e7-a230-23a5d72f8d11: \n",
      "This document provides an overview of Chainlink Functions, a self-service solution that provides access to off-chain computation without having to run and configure a Chainlink Node. It explains when to use Chainlink Functions, supported networks, and how to get started. It also provides examples of use cases, such as connecting to public data, transforming data, connecting to password-protected data sources, connecting to external decentralized databases, and fetching data from Web2 systems. This document can answer questions about Chainlink Functions, such as what it is, when to use it, and what use cases it can be used for.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 6c43a516-1b9e-4957-aa43-480b2afdb2a6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:39:26,616 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:39:26,617 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:39:38,961 - INFO - > [get_response] Total LLM token usage: 2508 tokens\n",
      "2023-05-30 10:39:38,962 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:39:38,962 - INFO - > [get_response] Total LLM token usage: 2508 tokens\n",
      "2023-05-30 10:39:38,963 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:39:38,964 - INFO - > Generated summary for doc 6c43a516-1b9e-4957-aa43-480b2afdb2a6: \n",
      "This document is an API reference for the Chainlink Functions library. It provides an overview of the types and constants, request structure, errors, and functions available in the library. It also provides instructions on how to initialize a request and add inline or remote secrets and arguments. This document can answer questions about the types and constants, request structure, errors, and functions available in the Chainlink Functions library.\n",
      "2023-05-30 10:39:38,989 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 85816232-91e9-4930-ae5c-3861d5a9e0a0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:39:55,564 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:39:55,565 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:39:59,930 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:39:59,930 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:40:13,174 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:40:13,175 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:40:17,483 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:40:17,484 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:40:21,791 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:40:21,792 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:40:42,311 - INFO - > [get_response] Total LLM token usage: 409 tokens\n",
      "2023-05-30 10:40:42,311 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:40:42,312 - INFO - > [get_response] Total LLM token usage: 4411 tokens\n",
      "2023-05-30 10:40:42,312 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:40:42,313 - INFO - > Generated summary for doc 85816232-91e9-4930-ae5c-3861d5a9e0a0: \n",
      "This document provides an API reference for the FunctionsClient contract, which is used by consumer contract developers to create Chainlink Functions requests. It includes information on events, errors, methods, and modifiers related to the FunctionsClient contract. This document can answer questions about the constructor, getDONPublicKey, estimateCost, sendRequest, fulfillRequest, handleOracleFulfillment, setOracle, getChainlinkOracleAddress, addExternalRequest, recordChainlinkFulfillment, and notPendingRequest methods, as well as the RequestSent, RequestFulfilled, SenderIsNotRegistry, RequestIsAlreadyPending, and RequestIsNotPending events. Additionally, it provides a link to join a Discord community to connect with other members, ask questions, and share ideas.\n",
      "2023-05-30 10:40:42,336 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f3c80391-2daf-4f53-9762-49d352bc902d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:41:21,616 - INFO - > [get_response] Total LLM token usage: 439 tokens\n",
      "2023-05-30 10:41:21,617 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:41:21,617 - INFO - > [get_response] Total LLM token usage: 4668 tokens\n",
      "2023-05-30 10:41:21,617 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:41:21,619 - INFO - > Generated summary for doc f3c80391-2daf-4f53-9762-49d352bc902d: \n",
      "This document provides a guide to getting started with Chainlink Functions, a decentralized oracle network. It covers setting up a Web3 wallet, installing the required frameworks, configuring the starter kit, simulating a Chainlink Functions request, setting up a subscription, and sending a Chainlink Functions request. It also provides instructions for deploying an on-chain FunctionsConsumer.sol contract, creating a subscription, and funding the subscription with LINK. This document can answer questions about setting up a Web3 wallet, installing the required frameworks, configuring the starter kit, simulating a Chainlink Functions request, setting up a subscription, and sending a Chainlink Functions request, as well as how to set up and use Chainlink Functions, and provide more information about the tool.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 38313afa-32ce-46d5-b266-80c91211491a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:41:30,684 - INFO - > [get_response] Total LLM token usage: 355 tokens\n",
      "2023-05-30 10:41:30,685 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:41:30,685 - INFO - > [get_response] Total LLM token usage: 355 tokens\n",
      "2023-05-30 10:41:30,686 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:41:30,686 - INFO - > Generated summary for doc 38313afa-32ce-46d5-b266-80c91211491a: \n",
      "This document provides resources related to Chainlink Functions, a service that allows users to access data from external sources and trigger smart contracts. It covers topics such as concepts, architecture, managing subscriptions, billing, supported networks, and service limits. This document can answer questions related to how Chainlink Functions works, how to manage subscriptions, what networks are supported, and what the service limits are.\n",
      "2023-05-30 10:41:30,737 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 2d3e94e6-6b16-4320-99f3-d648dff3b474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:41:47,159 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:41:47,159 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:42:11,767 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:42:11,768 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:42:43,311 - INFO - > [get_response] Total LLM token usage: 611 tokens\n",
      "2023-05-30 10:42:43,315 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:42:43,317 - INFO - > [get_response] Total LLM token usage: 10275 tokens\n",
      "2023-05-30 10:42:43,318 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:42:43,320 - INFO - > Generated summary for doc 2d3e94e6-6b16-4320-99f3-d648dff3b474: \n",
      "This document provides a guide for adding Chainlink Functions to an existing project. It outlines the steps necessary to configure Hardhat, get the dependency contracts and scripts, configure on-chain resources, create a consumer contract, deploy a consumer contract, create and fund a subscription, send requests, and create a request script. Questions that this document can answer include: How do I deploy a consumer contract? How do I create and fund a subscription? How do I send requests to the DON? How do I define arguments and encrypt secrets? How do I read the fulfillment response?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 18c53f00-40c0-40e9-a5d5-25bb225988f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:42:55,976 - INFO - > [get_response] Total LLM token usage: 2110 tokens\n",
      "2023-05-30 10:42:55,977 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:42:55,978 - INFO - > [get_response] Total LLM token usage: 2110 tokens\n",
      "2023-05-30 10:42:55,978 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:42:55,979 - INFO - > Generated summary for doc 18c53f00-40c0-40e9-a5d5-25bb225988f4: \n",
      "This document provides an overview of the architecture of Chainlink Functions, a decentralized oracle network that enables smart contracts to securely access off-chain data. It explains the Request & Receive Data cycle, Subscription Management, and how to Create, Fund, Add, Remove, and Cancel Subscriptions. It also explains how to Transfer ownership of a Subscription. This document can answer questions about how to use Chainlink Functions, how to manage Subscriptions, and how to transfer ownership of a Subscription.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: e8669fd0-f1f3-4b98-9e5e-3e372be0d808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:43:06,682 - INFO - > [get_response] Total LLM token usage: 1789 tokens\n",
      "2023-05-30 10:43:06,683 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:43:06,685 - INFO - > [get_response] Total LLM token usage: 1789 tokens\n",
      "2023-05-30 10:43:06,689 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:43:06,691 - INFO - > Generated summary for doc e8669fd0-f1f3-4b98-9e5e-3e372be0d808: \n",
      "This document provides an overview of the billing process for Chainlink Functions. It explains the concepts of billing, cost simulation, cost calculation, and fees. It also provides formulas for estimating and calculating the total cost of a Chainlink Functions request in LINK tokens. Additionally, it explains the accounting movements that occur when a request is fulfilled. This document can answer questions about the billing process for Chainlink Functions, the cost of fulfilling a request, and the fees associated with a request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 2a002046-bc01-4345-a548-c674674eecd2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:43:07,185 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:43:07,188 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:43:30,280 - INFO - > [get_response] Total LLM token usage: 1512 tokens\n",
      "2023-05-30 10:43:30,281 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:43:30,282 - INFO - > [get_response] Total LLM token usage: 1512 tokens\n",
      "2023-05-30 10:43:30,282 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:43:30,283 - INFO - > Generated summary for doc 2a002046-bc01-4345-a548-c674674eecd2: \n",
      "This document provides an overview of Chainlink Functions, a decentralized oracle network that allows consumer contracts to make requests without needing to hold LINK tokens. It explains the process of requesting and receiving data, the Decentralized Oracle Network (DON), and Subscriptions. It also explains the Subscriptions App, Subscriptions Contract, Subscription Account, Subscription ID, Subscription Owner, Subscription Balance, Subscription Reservation, Effective Balance, and Subscription Consumers. \n",
      "\n",
      "This document can answer questions about the process of requesting and receiving data, the Decentralized Oracle Network (DON), and Subscriptions. It can also answer questions about the Subscriptions App, Subscriptions Contract, Subscription Account, Subscription ID, Subscription Owner, Subscription Balance, Subscription Reservation, Effective Balance, and Subscription Consumers.\n",
      "2023-05-30 10:43:30,341 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c50d7739-0d95-48d6-813b-98bc7615f556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:43:40,569 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:43:40,570 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:44:13,018 - INFO - > [get_response] Total LLM token usage: 392 tokens\n",
      "2023-05-30 10:44:13,018 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:44:13,019 - INFO - > [get_response] Total LLM token usage: 8113 tokens\n",
      "2023-05-30 10:44:13,020 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:44:13,021 - INFO - > Generated summary for doc c50d7739-0d95-48d6-813b-98bc7615f556: \n",
      "This document provides an overview of the service limits for Chainlink Functions. It outlines the supported languages, maximum requests in flight, maximum callback gas limit, maximum subscriptions, maximum consumer contracts per subscription, request fulfillment timeout, maximum request size, and maximum returned value size. Additionally, it outlines the maximum source code execution time, memory allocated to the source code, maximum number of HTTP requests, query timeout, maximum URL length, maximum request length, and maximum response length. This document can answer questions about the limits of Chainlink Functions, such as the maximum number of simultaneous requests, the maximum amount of gas for a callback function, the maximum size of a request, how much memory can be allocated to a source code, and how long an HTTP request can take before it times out.\n",
      "2023-05-30 10:44:13,045 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b948290c-34aa-4456-87e5-bce4993e7991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:44:46,649 - INFO - > [get_response] Total LLM token usage: 352 tokens\n",
      "2023-05-30 10:44:46,649 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:44:46,650 - INFO - > [get_response] Total LLM token usage: 5630 tokens\n",
      "2023-05-30 10:44:46,650 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:44:46,651 - INFO - > Generated summary for doc b948290c-34aa-4456-87e5-bce4993e7991: \n",
      "This document provides a guide to managing Chainlink Functions subscriptions. It explains how to create and fund a subscription, get subscription details, add and remove consumer contracts, transfer ownership, and cancel a subscription. It also provides instructions on how to set up the necessary tools and environment for creating and managing subscriptions. Questions that this document can answer include: How do I create and manage Chainlink Functions subscriptions? How do I fund a subscription? How do I add and remove consumer contracts? How do I transfer ownership of a subscription? How do I cancel a subscription? How do I remove a consumer contract address from a subscription?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: fbf736fb-74c7-4e1c-a00b-4d20db588295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:44:47,088 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:44:47,089 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:45:03,433 - INFO - > [get_response] Total LLM token usage: 1095 tokens\n",
      "2023-05-30 10:45:03,435 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:45:03,436 - INFO - > [get_response] Total LLM token usage: 1095 tokens\n",
      "2023-05-30 10:45:03,437 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:45:03,438 - INFO - > Generated summary for doc fbf736fb-74c7-4e1c-a00b-4d20db588295: \n",
      "This document provides a list of the networks that Chainlink Functions is supported on, including Ethereum Sepolia, Polygon Mumbai, and Avalanche Fuji testnets. It also provides the contract addresses for the Functions Oracle proxy, Functions Oracle Registry proxy, and LINK/ETH price feed for each of the networks. This document can answer questions about which networks Chainlink Functions is supported on, as well as the contract addresses for the various components of Chainlink Functions on each of the networks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: a052ab56-a59b-4a35-af26-aec80a3f96ce\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:45:20,862 - INFO - > [get_response] Total LLM token usage: 525 tokens\n",
      "2023-05-30 10:45:20,863 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:45:20,863 - INFO - > [get_response] Total LLM token usage: 525 tokens\n",
      "2023-05-30 10:45:20,863 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:45:20,864 - INFO - > Generated summary for doc a052ab56-a59b-4a35-af26-aec80a3f96ce: \n",
      "This document provides tutorials on Chainlink Functions, a tool for connecting smart contracts to external data sources. It covers topics such as requesting computations, calling APIs, returning custom data types, POSTing data to an API, using secrets in requests, calling multiple data sources, using off-chain secrets in requests, and automating functions. This document can answer questions about how to use Chainlink Functions to connect smart contracts to external data sources, how to request computations, how to call APIs, how to return custom data types, how to POST data to an API, how to use secrets in requests, how to call multiple data sources, how to use off-chain secrets in requests, and how to automate functions.\n",
      "2023-05-30 10:45:20,920 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 0861c006-52e5-4e77-ab4a-389f426c062a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:45:36,843 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:45:36,844 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:46:16,050 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:46:16,051 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:46:20,481 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:46:20,482 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:46:42,259 - INFO - > [get_response] Total LLM token usage: 634 tokens\n",
      "2023-05-30 10:46:42,262 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:46:42,265 - INFO - > [get_response] Total LLM token usage: 9709 tokens\n",
      "2023-05-30 10:46:42,266 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:46:42,270 - INFO - > Generated summary for doc 0861c006-52e5-4e77-ab4a-389f426c062a: \n",
      "This document provides a tutorial on how to use Chainlink Functions to make a request to a Decentralized Oracle Network and how to write a Chainlink Functions consumer contract. It also provides instructions on how to use the Chainlink Functions library to make an API request and return a custom response. This document can answer questions about how to use Chainlink Functions to make a request to a Decentralized Oracle Network, how to set up the environment, how to use the simulator to test the code, how to write a Chainlink Functions consumer contract, how to import the FunctionsClient.sol library, how to use the Functions.sol library, how to define the OCRResponse event, how to pass the oracle address for the network when deploying the contract, how to change the oracle address, and how to use the executeRequest and fulfillRequest functions.\n",
      "2023-05-30 10:46:42,446 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c4dd6dd8-7e74-4beb-a989-2391c0dcaf5b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:46:57,196 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:46:57,197 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:47:39,559 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:47:39,560 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:47:43,927 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:47:43,930 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:48:12,707 - INFO - > [get_response] Total LLM token usage: 746 tokens\n",
      "2023-05-30 10:48:12,707 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:48:12,708 - INFO - > [get_response] Total LLM token usage: 9766 tokens\n",
      "2023-05-30 10:48:12,709 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:48:12,710 - INFO - > Generated summary for doc c4dd6dd8-7e74-4beb-a989-2391c0dcaf5b: \n",
      "This document provides a tutorial on how to make multiple API calls from a smart contract to a Decentralized Oracle Network (DON) in order to fetch the median asset price of Bitcoin (BTC/USD). It provides instructions on how to set up the environment, simulate the code locally, and send a request to the DON. It also explains how to use secrets and how to get a free API key from CoinMarketCap. Additionally, it provides an overview of the Chainlink Functions consumer contract, and instructions on how to write a JavaScript source code to fetch asset prices from CoinMarketCap, CoinGecko, and Coinpaprika. This document can answer questions about how to make multiple API calls from a smart contract, how to use secrets, how to get a free API key from CoinMarketCap, what is a Chainlink Functions consumer contract, how to write a consumer contract, how to send a request, how to implement the callback, what is an example of a successful request, what is the total cost of a request, what is the response returned to the client contract, and what is the off-chain secrets gist.\n",
      "2023-05-30 10:48:12,769 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 8c6cd877-2750-47de-aa7e-4711e4595eb2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:48:13,133 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:48:13,134 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:48:37,229 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:48:37,231 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:48:42,181 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:48:42,182 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:48:46,525 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:48:46,526 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:49:41,333 - INFO - > [get_response] Total LLM token usage: 729 tokens\n",
      "2023-05-30 10:49:41,334 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:49:41,334 - INFO - > [get_response] Total LLM token usage: 9310 tokens\n",
      "2023-05-30 10:49:41,335 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:49:41,335 - INFO - > Generated summary for doc 8c6cd877-2750-47de-aa7e-4711e4595eb2: \n",
      "This document provides a tutorial on how to use Chainlink Functions to send a request to a Decentralized Oracle Network, as well as a detailed explanation of how to write a Chainlink Functions consumer contract and a JavaScript source code. It explains how to write a function that sends a GraphQL query, the maximum response size, and the importance of making idempotent requests. It also provides instructions on how to complete the setup steps, how to use the Chainlink Functions Hardhat Starter Kit to simulate the code, and how to send a request to the Decentralized Oracle Network. This document can answer questions about how to use Chainlink Functions to send a request to a Decentralized Oracle Network, how to write a function that sends a GraphQL query, the maximum response size, and the importance of making idempotent requests, how to write a Chainlink Functions consumer contract, how to use the Functions.sol library, how to define the OCRResponse event, how to pass the oracle address, how to change the oracle address, and how to use the executeRequest and fulfillRequest functions, and how to write a JavaScript source code to make an HTTP request, what parameters must be provided, and how to return the result\n",
      "2023-05-30 10:49:41,374 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 88e5aee0-a3ef-4cc1-9432-72f11d05d6dc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:50:31,548 - INFO - > [get_response] Total LLM token usage: 547 tokens\n",
      "2023-05-30 10:50:31,548 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:50:31,549 - INFO - > [get_response] Total LLM token usage: 8179 tokens\n",
      "2023-05-30 10:50:31,549 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:50:31,550 - INFO - > Generated summary for doc 88e5aee0-a3ef-4cc1-9432-72f11d05d6dc: \n",
      "This document provides a tutorial on how to use Chainlink Functions to call an API with HTTP Query Parameters. It explains how to configure HTTP query parameters, use the Chainlink Functions Hardhat Starter Kit to test the code locally, and send a request to the Decentralized Oracle Network. Additionally, it provides an overview of how to build a Chainlink Functions consumer contract, including how to import the FunctionsClient.sol contract, use the Functions library, define the OCRResponse event, pass the oracle address, change the oracle address, and execute and fulfill requests. Lastly, it provides a link to join a Discord community. This document can answer questions about how to use Chainlink Functions to call an API, how to configure HTTP query parameters, how to use the Chainlink Functions Hardhat Starter Kit, how to build a Chainlink Functions consumer contract, how to use the Functions library, and how to write a compatible JavaScript source code. It can also answer questions about what other members are discussing, what resources are available, and how to connect with other members in the Discord community.\n",
      "2023-05-30 10:50:31,603 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: a829fcd4-37c1-46ab-81e7-5e8d9ce35c7f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:51:08,676 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:51:08,677 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:51:23,546 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:51:23,547 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:51:47,711 - INFO - > [get_response] Total LLM token usage: 709 tokens\n",
      "2023-05-30 10:51:47,712 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:51:47,712 - INFO - > [get_response] Total LLM token usage: 10459 tokens\n",
      "2023-05-30 10:51:47,713 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:51:47,713 - INFO - > Generated summary for doc a829fcd4-37c1-46ab-81e7-5e8d9ce35c7f: \n",
      "This document provides a tutorial on how to use off-chain secrets in Chainlink Functions requests. It explains how to encrypt secrets with the public key of the Decentralized Oracle Network (DON), store them in an offchain-secrets.json file, host the file off-chain, and include the HTTP URL to the file in the Chainlink Functions request. Additionally, it provides instructions on how to use the Chainlink Functions Hardhat Starter Kit to simulate the request source code locally. This document can answer questions such as: How do I use off-chain secrets in Chainlink Functions requests? How do I encrypt secrets with the public key of the Decentralized Oracle Network (DON)? How do I store secrets in an offchain-secrets.json file? How do I host the file off-chain? How do I include the HTTP URL to the file in the Chainlink Functions request? How do I use the Chainlink Functions Hardhat Starter Kit to simulate the request source code locally?\n",
      "2023-05-30 10:51:47,768 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 614020f2-ce6a-411b-a1a0-920a3156fd61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:52:51,616 - INFO - > [get_response] Total LLM token usage: 713 tokens\n",
      "2023-05-30 10:52:51,617 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:52:51,617 - INFO - > [get_response] Total LLM token usage: 10205 tokens\n",
      "2023-05-30 10:52:51,618 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:52:51,618 - INFO - > Generated summary for doc 614020f2-ce6a-411b-a1a0-920a3156fd61: \n",
      "This document provides a tutorial on how to use secrets in requests to the Decentralized Oracle Network (DON) using the Chainlink Functions starter kit. It explains how to write a Chainlink Functions consumer contract, use the Functions.sol library, define the OCRResponse event, pass the oracle address when deploying the contract, change the oracle address, send a request, and implement the callback. It also provides an example of the config.js settings and source.js code. Additionally, it explains how to use the Chainlink API to access the price of an asset from CoinMarketCap. This document can answer questions about how to use secrets in requests, how to set up the environment, how to simulate and send requests to the DON, how to write a Chainlink Functions consumer contract, how to use the Functions.sol library, how to define the OCRResponse event, how to pass the oracle address when deploying the contract, how to change the oracle address, how to send a request, how to implement the callback, what are the config.js settings and source.js code, and how to use the Chainlink API to access asset prices from CoinMarketCap.\n",
      "2023-05-30 10:52:51,663 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ef798d00-1dd0-40ea-80af-19739f35261f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:53:08,811 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:53:08,812 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:53:13,150 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:53:13,151 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:53:17,565 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:53:17,566 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:53:34,640 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:53:34,641 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:54:07,460 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:54:07,460 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:54:32,034 - INFO - > [get_response] Total LLM token usage: 772 tokens\n",
      "2023-05-30 10:54:32,035 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:54:32,035 - INFO - > [get_response] Total LLM token usage: 10088 tokens\n",
      "2023-05-30 10:54:32,035 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:54:32,036 - INFO - > Generated summary for doc ef798d00-1dd0-40ea-80af-19739f35261f: \n",
      "This document provides a tutorial on how to use Chainlink Automation to automate Chainlink Functions. It explains how to deploy the AutomatedFunctionsConsumer.sol contract, configure Chainlink Automation, and check the result. It also provides instructions on how to set up the environment with the necessary tools, get a free API key from CoinMarketCap, and create a Github fine-grained personal access token. This document can answer questions about how to use Chainlink Automation to automate Chainlink Functions, how to set up the environment with the necessary tools, how to get a free API key from CoinMarketCap, how to create a Github fine-grained personal access token, how to define state variables to keep track of the time interval between requests, how to define the OCRResponse event that the smart contract will emit during the callback, how to pass the oracle address, Chainlink functions ID, fulfillment gas limit, and update interval when deploying the contract, and how to use the checkUpkeep and performUpkeep functions to send requests to the oracle.\n",
      "2023-05-30 10:54:32,076 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: a2ce8ff0-338c-4f10-9876-3307535dfe2c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:54:41,459 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:54:41,461 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:55:16,264 - INFO - > [get_response] Total LLM token usage: 426 tokens\n",
      "2023-05-30 10:55:16,265 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:16,266 - INFO - > [get_response] Total LLM token usage: 7161 tokens\n",
      "2023-05-30 10:55:16,267 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:16,268 - INFO - > Generated summary for doc a2ce8ff0-338c-4f10-9876-3307535dfe2c: \n",
      "This document provides a tutorial on how to use Chainlink Functions to run computations on the Chainlink Decentralized Oracle Network (DON). It explains how to set up the environment, simulate the code locally, and send a request to the DON. It also provides an example of computing the geometric mean of a list of numbers. Additionally, it provides an overview of how to write a Chainlink Functions consumer contract. This document can answer questions about how to set up the environment, simulate code, send requests to the DON, write a Chainlink Functions consumer contract, what settings are needed for the request configuration, what JavaScript code is needed for the source, and why buffers are important.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 1789542f-dd95-4399-b06e-cf4c22b4d19c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:55:25,501 - INFO - > [get_response] Total LLM token usage: 407 tokens\n",
      "2023-05-30 10:55:25,501 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:25,502 - INFO - > [get_response] Total LLM token usage: 407 tokens\n",
      "2023-05-30 10:55:25,502 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:25,503 - INFO - > Generated summary for doc 1789542f-dd95-4399-b06e-cf4c22b4d19c: \n",
      "This document provides an overview of Chainlink nodes and step-by-step tutorials and documentation on how to set up a Chainlink node, fulfill job requests, and add external adapters. It also provides a Getting Started Guide for those new to Chainlink and smart contracts. This document can answer questions about how to set up a Chainlink node, how to fulfill job requests, how to add external adapters, and how to get started with Chainlink and smart contracts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ad7d4a7e-fb56-4f94-8780-7a977b503054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:55:25,853 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:55:25,853 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:55:30,185 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:55:30,186 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:55:46,875 - INFO - > [get_response] Total LLM token usage: 2684 tokens\n",
      "2023-05-30 10:55:46,875 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:46,876 - INFO - > [get_response] Total LLM token usage: 2684 tokens\n",
      "2023-05-30 10:55:46,876 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:46,877 - INFO - > Generated summary for doc ad7d4a7e-fb56-4f94-8780-7a977b503054: \n",
      "This document provides instructions on how to configure a Chainlink node using TOML configuration. It explains how to migrate from environment variables to TOML, export the current config, validate the configuration, restart the Chainlink node using the TOML config, and use multiple config files. This document can answer questions about how to configure a Chainlink node, how to migrate from environment variables to TOML, how to export the current config, how to validate the configuration, how to restart the Chainlink node using the TOML config, and how to use multiple config files.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: cf3df619-2362-46c3-ba51-75d71c9c017d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:55:47,183 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:55:47,184 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:55:59,855 - INFO - > [get_response] Total LLM token usage: 522 tokens\n",
      "2023-05-30 10:55:59,856 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:59,857 - INFO - > [get_response] Total LLM token usage: 522 tokens\n",
      "2023-05-30 10:55:59,858 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:55:59,859 - INFO - > Generated summary for doc cf3df619-2362-46c3-ba51-75d71c9c017d: \n",
      "This document provides the addresses of the Chainlink Operator Factory for different networks, including Ethereum Mainnet, Sepolia, and Goerli. It also provides links to the Etherscan, Sepolia Etherscan, and Goerli Etherscan pages for each address. This document can answer questions about the addresses of the Chainlink Operator Factory for different networks, as well as provide links to the Etherscan pages for each address.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 5d8d245b-cfd2-4cc3-ad89-e5d27b563fb6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:56:17,109 - INFO - > [get_response] Total LLM token usage: 2095 tokens\n",
      "2023-05-30 10:56:17,110 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:56:17,110 - INFO - > [get_response] Total LLM token usage: 2095 tokens\n",
      "2023-05-30 10:56:17,110 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:56:17,111 - INFO - > Generated summary for doc 5d8d245b-cfd2-4cc3-ad89-e5d27b563fb6: \n",
      "This document provides an overview of the Forwarder contract, which is used by Chainlink nodes to manage multiple externally-owned accounts (EOAs) and make them look like a single address. It explains how the Forwarder contract works, its features, and its API reference. It also explains how node operators can use the Forwarder contract to set up different transaction-sending strategies more securely while lowering their infrastructure costs. \n",
      "\n",
      "This document can answer questions such as: What is the Forwarder contract? How does it work? What are its features? What is the API reference for the Forwarder contract? How can node operators use the Forwarder contract to set up different transaction-sending strategies?\n",
      "2023-05-30 10:56:17,151 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 58918cc5-b20c-47a0-880f-ed12edccd067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:56:17,537 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:56:17,538 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:56:21,883 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:56:21,884 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:56:37,140 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:56:37,141 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:56:41,467 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:56:41,468 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:56:59,428 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:56:59,429 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:57:16,712 - INFO - > [get_response] Total LLM token usage: 395 tokens\n",
      "2023-05-30 10:57:16,713 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:16,714 - INFO - > [get_response] Total LLM token usage: 7935 tokens\n",
      "2023-05-30 10:57:16,714 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:16,715 - INFO - > Generated summary for doc 58918cc5-b20c-47a0-880f-ed12edccd067: \n",
      "This document provides an overview of the Chainlink Node Operator contract, which is an on-chain contract used by oracles to handle requests made through the LINK token. It explains the features of the Operator contract, such as multi-word response, factory deployment, distributing funds to multiple addresses, flexibility and security, and provides an API reference with methods and events. This document can answer questions about the features of the Operator contract, how to deploy it, how to use it to distribute funds, how to transfer ownership of contracts, how to set authorized senders, how to withdraw LINK tokens, and how to cancel oracle requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 37b39be0-f7f0-4bed-913d-45ed3af4600b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:57:17,037 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:57:17,038 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:57:21,389 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:57:21,390 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:57:33,802 - INFO - > [get_response] Total LLM token usage: 928 tokens\n",
      "2023-05-30 10:57:33,803 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:33,804 - INFO - > [get_response] Total LLM token usage: 928 tokens\n",
      "2023-05-30 10:57:33,804 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:33,805 - INFO - > Generated summary for doc 37b39be0-f7f0-4bed-913d-45ed3af4600b: \n",
      "This document provides an overview of the Operator Factory design pattern, which is a programming pattern that allows users to compile and create instances of a contract without having to do it manually. It also provides an API reference and details on the methods and events associated with the Operator Factory. This document can answer questions about the type and version of the contract, how to deploy operators and forwarders, and how to verify if a factory has deployed a given contract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 2a330a36-7ac9-4a3b-a9ea-7313bec5492b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:57:43,781 - INFO - > [get_response] Total LLM token usage: 531 tokens\n",
      "2023-05-30 10:57:43,782 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:43,782 - INFO - > [get_response] Total LLM token usage: 531 tokens\n",
      "2023-05-30 10:57:43,783 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:43,783 - INFO - > Generated summary for doc 2a330a36-7ac9-4a3b-a9ea-7313bec5492b: \n",
      "This document provides an overview of the Ownership contract, which is inherited by the Operator and Forwarder contracts. It includes an API reference with methods such as transferOwnership, acceptOwnership, and owner, as well as events such as OwnershipTransferRequested and OwnershipTransferred. This document can answer questions about how to transfer ownership of a contract, how to accept ownership of a contract, and who the current owner of a contract is.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 30cafbf3-5087-4671-924e-d4ec77a53ac6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:57:53,269 - INFO - > [get_response] Total LLM token usage: 989 tokens\n",
      "2023-05-30 10:57:53,270 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:53,270 - INFO - > [get_response] Total LLM token usage: 989 tokens\n",
      "2023-05-30 10:57:53,271 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:57:53,271 - INFO - > Generated summary for doc 30cafbf3-5087-4671-924e-d4ec77a53ac6: \n",
      "This document provides an overview of the Receiver contract, which is an abstract contract inherited by the Operator and Forwarder contracts. It provides an API reference with methods such as setAuthorizedSenders, getAuthorizedSenders, and isAuthorizedSender, as well as an AuthorizedSendersChanged event. This document can answer questions about the Receiver contract, such as what methods it provides and how to set authorized senders.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 9adf095b-ce9e-4a71-93c9-3cf814d690f6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:58:04,143 - INFO - > [get_response] Total LLM token usage: 1004 tokens\n",
      "2023-05-30 10:58:04,143 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:04,144 - INFO - > [get_response] Total LLM token usage: 1004 tokens\n",
      "2023-05-30 10:58:04,144 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:04,145 - INFO - > Generated summary for doc 9adf095b-ce9e-4a71-93c9-3cf814d690f6: \n",
      "This document provides an overview of how to use external adapters in Solidity, including how to use parameters and the Copy adapter. It explains how to use the Request and add methods to create a run parameter for each required value, as well as how to use the Copy adapter to get data from the external adapter's response. It also provides an example of how to use dot-notation JSONPath to simplify the process. This document can answer questions about how to use external adapters in Solidity, how to use parameters with an external adapter, and how to use the Copy adapter with an external adapter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 9369d086-46ed-4603-83b5-869983dba9c0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:58:13,811 - INFO - > [get_response] Total LLM token usage: 2719 tokens\n",
      "2023-05-30 10:58:13,811 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:13,812 - INFO - > [get_response] Total LLM token usage: 2719 tokens\n",
      "2023-05-30 10:58:13,813 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:13,814 - INFO - > Generated summary for doc 9369d086-46ed-4603-83b5-869983dba9c0: \n",
      "This document provides an overview of building external adapters for Chainlink nodes. It explains how the Chainlink node requests data from the external adapter, how the data should be formatted for a response, and how to return errors. It also explains how to use asynchronous callbacks and provides an example of a serverless function external adapter. This document can answer questions about how to build an external adapter, how to format data for a response, how to return errors, and how to use asynchronous callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b6faceec-fa96-4ac8-a613-2624af15eff5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:58:26,310 - INFO - > [get_response] Total LLM token usage: 631 tokens\n",
      "2023-05-30 10:58:26,310 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:26,311 - INFO - > [get_response] Total LLM token usage: 631 tokens\n",
      "2023-05-30 10:58:26,312 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:26,313 - INFO - > Generated summary for doc b6faceec-fa96-4ac8-a613-2624af15eff5: \n",
      "This document provides an introduction to external adapters, which are services that the core of the Chainlink node communicates with via its API. It is broken up into three main categories: contract creators, developers, and node operators. Contract creators will need to know how to specify an external adapter in their request for external data, developers will need to know how to implement an external adapter for an API, and node operators will need to know how to add an external adapter to their node. This document can answer questions about how to specify an external adapter, how to implement an external adapter, and how to add an external adapter to a node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ef43e862-12d4-4e8d-a075-30ff92d91a2a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:58:39,008 - INFO - > [get_response] Total LLM token usage: 2199 tokens\n",
      "2023-05-30 10:58:39,008 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:39,009 - INFO - > [get_response] Total LLM token usage: 2199 tokens\n",
      "2023-05-30 10:58:39,009 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:39,010 - INFO - > Generated summary for doc ef43e862-12d4-4e8d-a075-30ff92d91a2a: \n",
      "This document provides an overview of how to add external adapters to a Chainlink node by creating a bridge in the Node Operators Interface. It explains how to create a bridge, the bridge name and URL requirements, and how to add jobs that use the bridge. It also provides instructions on how to test external adapters and bridges using a Webhook Job. This document can answer questions about how to create a bridge, the bridge name and URL requirements, and how to add jobs that use the bridge. It can also answer questions about how to test external adapters and bridges using a Webhook Job.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 23b1b541-7a78-4650-9cd2-70cc50def7d6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:58:53,437 - INFO - > [get_response] Total LLM token usage: 999 tokens\n",
      "2023-05-30 10:58:53,437 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:53,438 - INFO - > [get_response] Total LLM token usage: 999 tokens\n",
      "2023-05-30 10:58:53,438 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:58:53,439 - INFO - > Generated summary for doc 23b1b541-7a78-4650-9cd2-70cc50def7d6: \n",
      "This document provides an overview of how to build an external initiator, which is a web initiator that can be used to trigger a run for any webhook job that it has been linked to. It explains how to make an API call to the node with two added headers, \"X-Chainlink-EA-AccessKey\" and \"X-Chainlink-EA-Secret\", which are keys generated when the external initiator is registered with the node. It also provides a pseudo code example of a simple external initiator. \n",
      "\n",
      "This document can answer questions about how to build an external initiator, how to make an API call to the node, and what the two added headers are.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: d497a1d2-bd38-4184-85d4-4208a98b7703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:59:06,086 - INFO - > [get_response] Total LLM token usage: 1949 tokens\n",
      "2023-05-30 10:59:06,086 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:59:06,087 - INFO - > [get_response] Total LLM token usage: 1949 tokens\n",
      "2023-05-30 10:59:06,087 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:59:06,088 - INFO - > Generated summary for doc d497a1d2-bd38-4184-85d4-4208a98b7703: \n",
      "This document provides instructions on how to add, delete, and list external initiators on Chainlink nodes. It explains how to use the remote API or the Chainlink client to create an external initiator, and how to set the environment variables needed to run the external initiator. It also explains how to delete an external initiator and how to list all installed external initiators. This document can answer questions about how to add, delete, and list external initiators on Chainlink nodes, as well as how to set the environment variables needed to run the external initiator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: d5c35f31-968d-4367-bec4-00dab91acff6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 10:59:06,500 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:59:06,502 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:59:10,807 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 10:59:10,809 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 10:59:28,884 - INFO - > [get_response] Total LLM token usage: 813 tokens\n",
      "2023-05-30 10:59:28,885 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:59:28,886 - INFO - > [get_response] Total LLM token usage: 813 tokens\n",
      "2023-05-30 10:59:28,886 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 10:59:28,888 - INFO - > Generated summary for doc d5c35f31-968d-4367-bec4-00dab91acff6: \n",
      "This document provides an introduction to external initiators, which are used to initiate jobs in a node depending on an external condition. It explains how to enable external initiators, how to create a bridge, and how to use the external initiator in job specs. It also provides instructions for adding external initiators to nodes and building external initiators. \n",
      "\n",
      "This document can answer questions about how to enable external initiators, how to create a bridge, how to use the external initiator in job specs, and how to add external initiators to nodes and build external initiators.\n",
      "2023-05-30 10:59:28,954 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b935cabc-3ec2-45ee-94f0-bf63d2e9e793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:00:07,596 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:00:07,597 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:00:43,998 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:00:43,998 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:01:12,978 - INFO - > [get_response] Total LLM token usage: 968 tokens\n",
      "2023-05-30 11:01:12,980 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:01:12,982 - INFO - > [get_response] Total LLM token usage: 14219 tokens\n",
      "2023-05-30 11:01:12,982 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:01:12,983 - INFO - > Generated summary for doc b935cabc-3ec2-45ee-94f0-bf63d2e9e793: \n",
      "This document provides an overview of the changes made to Chainlink nodes in versions 1.9.0, 1.8.1, 1.8.0, 1.7.1, 1.7.0, 1.6.0, 1.5.0, 1.4.1, 1.4.0, 1.3.0, 1.2.0, 1.1.0, and 1.0.0/1.0.1. It includes information about added features, changed features, removed features, and fixed features, as well as TOML configuration, environment variables, and prometheus metrics. This document can answer questions about the changes in each version of Chainlink nodes, as well as how to configure and use Chainlink nodes, bug fixes, new features, improvements, new commands, job-spec attributes, job-type limits, environment variables, CLI arguments, password complexity requirements, Keepers job spec, JSON parse tasks, disk rotating logs, version compatibility check, locking mode, EVM RPC nodes, EIP-1559, and Chainlink Automation. It also provides instructions on how to upgrade from previous versions and the requirements for doing so.\n",
      "2023-05-30 11:01:13,048 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c3c5e3de-74b0-4382-88a3-4687738614b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:01:13,428 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:01:13,429 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:01:31,359 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:01:31,359 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:01:35,701 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:01:35,702 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:01:40,072 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:01:40,073 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:02:10,768 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:02:10,769 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:02:26,559 - INFO - > [get_response] Total LLM token usage: 535 tokens\n",
      "2023-05-30 11:02:26,560 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:02:26,561 - INFO - > [get_response] Total LLM token usage: 10958 tokens\n",
      "2023-05-30 11:02:26,562 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:02:26,564 - INFO - > Generated summary for doc c3c5e3de-74b0-4382-88a3-4687738614b2: \n",
      "This document provides an overview of the different types of jobs that can be created and run on a Chainlink node. It outlines the shared fields and unique fields for each job type, as well as the job type specific pipeline variables. It also provides an example of each job type. This document can answer questions about the different job types available for Chainlink nodes, the spec format for each job type, the shared and unique fields for each job type, the job type specific pipeline variables for each job type, and how to create and manage key bundles.\n",
      "2023-05-30 11:02:26,625 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: aa07acb6-37a2-4eb9-a039-613c6cd5de98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:02:48,813 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:02:48,815 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:03:36,547 - INFO - > [get_response] Total LLM token usage: 658 tokens\n",
      "2023-05-30 11:03:36,548 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:03:36,549 - INFO - > [get_response] Total LLM token usage: 11140 tokens\n",
      "2023-05-30 11:03:36,549 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:03:36,550 - INFO - > Generated summary for doc aa07acb6-37a2-4eb9-a039-613c6cd5de98: \n",
      "This document provides an overview of the tasks available for Chainlink nodes, including Any, Base64 Decode, Base64 Encode, Bridge, CBOR Parse, Divide, ETH ABI Decode Log, ETH ABI Decode, ETH ABI Encode, ETH Call, ETH Tx, Hex Decode, Hex Encode, HTTP, JSON Parse, Length, Less Than, Lowercase, Mean, Median, Memo, Merge, Mode, Multiply, Sum, and Uppercase tasks. Each task is described in detail, including parameters, inputs, outputs, and examples. This document can answer questions about the different task types available for Chainlink nodes, as well as how to use them. It can also answer questions such as what is the length of a given string, is a given number less than another number, and what is the response body of an HTTP request.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ccc03d13-e826-4621-a49b-6754ba9e4b6a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:03:36,903 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:03:36,904 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:03:49,994 - INFO - > [get_response] Total LLM token usage: 1643 tokens\n",
      "2023-05-30 11:03:49,995 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:03:49,995 - INFO - > [get_response] Total LLM token usage: 1643 tokens\n",
      "2023-05-30 11:03:49,996 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:03:49,996 - INFO - > Generated summary for doc ccc03d13-e826-4621-a49b-6754ba9e4b6a: \n",
      "This document provides an overview of Chainlink v2 jobs, which are called TOML jobs. It explains what a job is, provides an example v2 job spec, and outlines the shared fields that all job types supported by a node have. This document can answer questions about the types of jobs supported by Chainlink nodes, the format of v2 job specs, and the shared fields that all job types have.\n",
      "2023-05-30 11:03:50,031 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 913417d0-566e-410e-aab6-a00d767f04ec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:04:25,326 - INFO - > [get_response] Total LLM token usage: 389 tokens\n",
      "2023-05-30 11:04:25,328 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:04:25,329 - INFO - > [get_response] Total LLM token usage: 7462 tokens\n",
      "2023-05-30 11:04:25,330 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:04:25,332 - INFO - > Generated summary for doc 913417d0-566e-410e-aab6-a00d767f04ec: \n",
      "This document provides an overview of the migration from v1 to v2 jobs in Chainlink nodes. It explains the benefits of v2 jobs, such as increased job complexity, better performance, easier scaling, and improved security. It also provides example migrations from v1 to v2 jobs, and explains the syntax for specifying dependencies and variables, as well as the use of angle brackets for quoting. This document can answer questions about the differences between v1 and v2 jobs, the syntax for specifying dependencies and variables, and how to migrate from v1 to v2 jobs, including how to convert v1 specs to v2 specs for Direct Request, Cron, and Web (-> Webhook) jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 9acb4153-11be-4c6b-bfd7-f74cac1b9a63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:04:34,718 - INFO - > [get_response] Total LLM token usage: 1045 tokens\n",
      "2023-05-30 11:04:34,719 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:04:34,719 - INFO - > [get_response] Total LLM token usage: 1045 tokens\n",
      "2023-05-30 11:04:34,720 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:04:34,720 - INFO - > Generated summary for doc 9acb4153-11be-4c6b-bfd7-f74cac1b9a63: \n",
      "This document provides an overview of tasks in Chainlink v2 jobs, which are a replacement for core adapters from v1 jobs. It explains what tasks are, their shared attributes, and how to write pipelines. It also provides information on Chainlink's built-in tasks and external adapters, as well as how to use the index parameter and timeout attribute. This document can answer questions about tasks, their attributes, and how to write pipelines.\n",
      "2023-05-30 11:04:34,762 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b47fc39f-827c-4c23-946d-bd4f27e0d27f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:04:53,318 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:04:53,319 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:04:57,667 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:04:57,667 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:05:02,024 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:05:02,025 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:05:26,705 - INFO - > [get_response] Total LLM token usage: 435 tokens\n",
      "2023-05-30 11:05:26,706 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:05:26,707 - INFO - > [get_response] Total LLM token usage: 6317 tokens\n",
      "2023-05-30 11:05:26,707 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:05:26,708 - INFO - > Generated summary for doc b47fc39f-827c-4c23-946d-bd4f27e0d27f: \n",
      "This document provides an overview of the Core Adapters available for Chainlink nodes running version 1.0.0 and later. It explains the different adapters, their parameters, and how to use them in Solidity and job specifications. This document can answer questions about the different Core Adapters, their parameters, and how to use them in Solidity and job specifications. It can also answer questions about the restrictions on which IPs may be fetched, and how to use the HTTPGetWithUnrestrictedNetworkAccess and HTTPPostWithUnrestrictedNetworkAccess adapters. Additionally, it can provide information on how to use the external adapters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 539e80fa-e080-4443-94c0-a5e706448b46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:05:36,447 - INFO - > [get_response] Total LLM token usage: 3769 tokens\n",
      "2023-05-30 11:05:36,448 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:05:36,448 - INFO - > [get_response] Total LLM token usage: 3769 tokens\n",
      "2023-05-30 11:05:36,449 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:05:36,449 - INFO - > Generated summary for doc 539e80fa-e080-4443-94c0-a5e706448b46: \n",
      "This document provides an overview of the initiators available for v1 Jobs in Chainlink nodes running version 1.0.0 and later. Initiators are used to trigger job runs, and the document covers the parameters and examples for the Cron, EthLog, External, FluxMonitor, RunAt, RunLog, and Web initiators. This document can answer questions about how to create an external initiator, what parameters are needed for each initiator, and how to migrate from v1 to v2 jobs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b9bb5a70-79d6-4d12-8a89-9a4647805f43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:05:45,455 - INFO - > [get_response] Total LLM token usage: 2161 tokens\n",
      "2023-05-30 11:05:45,456 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:05:45,456 - INFO - > [get_response] Total LLM token usage: 2161 tokens\n",
      "2023-05-30 11:05:45,457 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:05:45,457 - INFO - > Generated summary for doc b9bb5a70-79d6-4d12-8a89-9a4647805f43: \n",
      "This document provides an overview of job specifications for Chainlink nodes running version 1.0.0 and later. It explains what a job is, how it is composed of initiators and adapters, and provides an example of a job spec. It also explains additional parameters that can be specified on a job, such as startAt, endAt, and minPayment. This document can answer questions about job specifications, initiators, adapters, and additional parameters for Chainlink nodes.\n",
      "2023-05-30 11:05:45,487 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 7d9b1464-ebfc-422b-8ff8-63bf595eac85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:06:05,593 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:06:05,595 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:06:43,651 - INFO - > [get_response] Total LLM token usage: 596 tokens\n",
      "2023-05-30 11:06:43,652 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:06:43,652 - INFO - > [get_response] Total LLM token usage: 6363 tokens\n",
      "2023-05-30 11:06:43,653 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:06:43,653 - INFO - > Generated summary for doc 7d9b1464-ebfc-422b-8ff8-63bf595eac85: \n",
      "This document provides best practices for deploying a Chainlink node on AWS using the AWS Quick Start. It outlines the resources created, IAM roles, billable services, and best practices for protecting the AWS account and monitoring the Chainlink node's health. It also provides instructions for recovering or upgrading the Chainlink node container, generating the .env, .password, and .api files, and stopping and removing the existing Chainlink node container. Additionally, it provides instructions for starting the Chainlink node container in Docker and for backing up and restoring the database and EC2 instance. This document can answer questions about the resources created, IAM roles, billable services, best practices, and instructions for recovering or upgrading the Chainlink node container, generating the .env, .password, and .api files, and stopping and removing the existing Chainlink node container. It can also answer questions about starting the Chainlink node container in Docker, backing up and restoring the database and EC2 instance, setting up failover capabilities and disaster recovery, and troubleshooting and designing a Chainlink node on AWS.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b5d449cb-4a5e-4235-b912-4f5a9a5f25b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:07:03,446 - INFO - > [get_response] Total LLM token usage: 2965 tokens\n",
      "2023-05-30 11:07:03,447 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:07:03,448 - INFO - > [get_response] Total LLM token usage: 2965 tokens\n",
      "2023-05-30 11:07:03,448 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:07:03,449 - INFO - > Generated summary for doc b5d449cb-4a5e-4235-b912-4f5a9a5f25b0: \n",
      "This document provides a set of security and operation best practices for Chainlink node operators to enhance the security and reliability of their infrastructure. It covers topics such as restricting access, failover capabilities, disaster recovery, active monitoring, frequent updates, jobs and config, addresses, and Infrastructure as Code (IaC). It provides minimum requirements and recommendations for each topic, as well as specific instructions for setting up a secure environment.\n",
      "\n",
      "Questions this document can answer include:\n",
      "- What security and operation best practices should I use for my Chainlink node?\n",
      "- How can I restrict access to my Chainlink node?\n",
      "- How can I set up failover capabilities for my Chainlink node?\n",
      "- How can I set up disaster recovery for my Chainlink node?\n",
      "- How can I set up active monitoring for my Chainlink node?\n",
      "- How often should I update my Chainlink node?\n",
      "- What job specifications and configuration settings should I use for my Chainlink node?\n",
      "- What addresses do I need for my Chainlink node?\n",
      "- How can I set up Infrastructure as Code (IaC) for my Chainlink node?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 3f254baf-d095-4f54-a8c9-a524cf54ab0f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:07:15,845 - INFO - > [get_response] Total LLM token usage: 982 tokens\n",
      "2023-05-30 11:07:15,846 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:07:15,847 - INFO - > [get_response] Total LLM token usage: 982 tokens\n",
      "2023-05-30 11:07:15,847 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:07:15,848 - INFO - > Generated summary for doc 3f254baf-d095-4f54-a8c9-a524cf54ab0f: \n",
      "This document provides a guide on how to connect a Chainlink node to a remote PostgreSQL database. It outlines the steps to obtain information about the database, such as the server hostname or IP, port, username, password, and database name. It also provides instructions on how to set the DATABASE_URL environment variable in the Chainlink node's .env configuration file. This document can answer questions about how to connect a Chainlink node to a remote PostgreSQL database, what information is needed to do so, and how to set the DATABASE_URL environment variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 9c8c7c16-da22-433d-825b-f05e6c6a44da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:07:24,666 - INFO - > [get_response] Total LLM token usage: 1614 tokens\n",
      "2023-05-30 11:07:24,667 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:07:24,668 - INFO - > [get_response] Total LLM token usage: 1614 tokens\n",
      "2023-05-30 11:07:24,668 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:07:24,668 - INFO - > Generated summary for doc 9c8c7c16-da22-433d-825b-f05e6c6a44da: \n",
      "This document provides a guide on how to generate self-signed certificates for use by a Chainlink node. It also explains how to add the TLS_CERT_PATH and TLS_KEY_PATH environment variables to the .env file, as well as how to update the run command to forward port 6689 to the container instead of 6688. This document can answer questions about how to generate self-signed certificates for a Chainlink node, how to add environment variables to the .env file, and how to update the run command.\n",
      "2023-05-30 11:07:24,712 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f7bc38e5-eb34-4a51-a0f7-a9a8f2a8ef48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:07:25,054 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:07:25,056 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:07:29,421 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:07:29,422 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:07:33,757 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:07:33,758 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:07:38,099 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:07:38,101 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:08:19,879 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:08:19,880 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:08:44,417 - INFO - > [get_response] Total LLM token usage: 538 tokens\n",
      "2023-05-30 11:08:44,418 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:08:44,418 - INFO - > [get_response] Total LLM token usage: 6353 tokens\n",
      "2023-05-30 11:08:44,419 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:08:44,419 - INFO - > Generated summary for doc f7bc38e5-eb34-4a51-a0f7-a9a8f2a8ef48: \n",
      "This document provides information on how to configure Chainlink and EVM nodes for high reliability and throughput. It covers topics such as using multiple nodes, automatic load balancing and failover, configuring websocket and HTTP URLs, increasing transaction throughput, optimizing RPC nodes, removing rejections on expensive transactions, adjusting minimum outgoing confirmations for high throughput jobs, and increasing ORM_MAX_OPEN_CONNS and ORM_MAX_IDLE_CONNS. This document can answer questions such as how to configure multiple primary nodes, how to enable automatic load balancing and failover, how to configure websocket and HTTP URLs, how to increase transaction throughput, how to optimize RPC nodes, how to remove rejections on expensive transactions, how to adjust minimum outgoing confirmations for high throughput jobs, and how to increase ORM_MAX_OPEN_CONNS and ORM_MAX_IDLE_CONNS.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: eb1c9075-b014-4c35-928d-bd32d9fd7f02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:08:56,975 - INFO - > [get_response] Total LLM token usage: 3629 tokens\n",
      "2023-05-30 11:08:56,976 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:08:56,977 - INFO - > [get_response] Total LLM token usage: 3629 tokens\n",
      "2023-05-30 11:08:56,977 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:08:56,978 - INFO - > Generated summary for doc eb1c9075-b014-4c35-928d-bd32d9fd7f02: \n",
      "This document provides an overview of miscellaneous topics related to Chainlink nodes, such as executing commands running Docker, transferring funds from the node wallet, changing the API password, multi-user and role-based access control, and key management. It provides instructions on how to list, create, export, delete, and import Ethereum keys, as well as a full example of running a node in detached mode.\n",
      "\n",
      "This document can answer questions such as: How do I execute commands running Docker? How do I transfer funds from the node wallet? How do I change my API password? How do I manage Ethereum keys? How do I run a node in detached mode?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: d4e70435-3c72-45d0-a38c-6f85e4176585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:08:57,395 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:08:57,396 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:09:11,386 - INFO - > [get_response] Total LLM token usage: 2873 tokens\n",
      "2023-05-30 11:09:11,386 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:09:11,387 - INFO - > [get_response] Total LLM token usage: 2873 tokens\n",
      "2023-05-30 11:09:11,387 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:09:11,388 - INFO - > Generated summary for doc d4e70435-3c72-45d0-a38c-6f85e4176585: \n",
      "This document provides instructions on how to perform system maintenance and updates on a Chainlink node using Docker. It explains how to pull the latest Docker image, how to run multiple instances of the node, and how to set up a failover node. This document can answer questions about how to restart a Chainlink node without downtime, how to run multiple instances of the node, and how to set up a failover node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f314b6a2-fda0-4edc-8ded-c2691ec6aa75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:09:20,612 - INFO - > [get_response] Total LLM token usage: 1395 tokens\n",
      "2023-05-30 11:09:20,613 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:09:20,613 - INFO - > [get_response] Total LLM token usage: 1395 tokens\n",
      "2023-05-30 11:09:20,614 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:09:20,615 - INFO - > Generated summary for doc f314b6a2-fda0-4edc-8ded-c2691ec6aa75: \n",
      "This document provides an overview of the hardware, software, and blockchain connectivity requirements for running a Chainlink node. It outlines the minimum and recommended hardware requirements for running a node in a production environment, as well as the software dependencies and blockchain connectivity requirements. This document can answer questions about the hardware and software needed to run a Chainlink node, as well as the requirements for running a PostgreSQL database and a network client.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 254cce12-adba-4307-bdb1-e1ebf4b8aba7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:09:36,928 - INFO - > [get_response] Total LLM token usage: 3571 tokens\n",
      "2023-05-30 11:09:36,929 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:09:36,929 - INFO - > [get_response] Total LLM token usage: 3571 tokens\n",
      "2023-05-30 11:09:36,930 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:09:36,930 - INFO - > Generated summary for doc 254cce12-adba-4307-bdb1-e1ebf4b8aba7: \n",
      "This document provides instructions on how to run an Ethereum client with websockets connectivity for use with a Chainlink node. It covers running the Geth and Nethermind clients, as well as using external services such as Alchemy, Chainstack, Fiews, GetBlock, Infura, LinkPool, and QuikNode. It also provides instructions on how to configure the ETH node, including disabling the default RPC gas and txfee caps. Additionally, it provides a link to the Chainlink ETH Failover Proxy. \n",
      "\n",
      "This document can answer questions about how to run an Ethereum client with websockets connectivity, how to configure an ETH node, and how to use external services for Ethereum clients.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 1f839679-8c64-4331-b2f3-36e091598049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:09:37,152 - INFO - > Building index from nodes: 9 chunks\n",
      "2023-05-30 11:09:37,543 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:09:37,543 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:09:41,910 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:09:41,911 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:09:46,239 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:09:46,239 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:09:50,575 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:09:50,577 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:10:19,213 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:10:19,215 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:11:19,495 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:11:19,496 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:12:36,528 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:12:36,529 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:13:11,522 - INFO - > [get_response] Total LLM token usage: 1769 tokens\n",
      "2023-05-30 11:13:11,523 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:13:11,524 - INFO - > [get_response] Total LLM token usage: 35921 tokens\n",
      "2023-05-30 11:13:11,524 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:13:11,525 - INFO - > Generated summary for doc 1f839679-8c64-4331-b2f3-36e091598049: \n",
      "This document provides an overview of the configuration settings for Chainlink nodes. It covers topics such as essential environment variables, database settings, logging, web server settings, EVM/Ethereum settings, and more. It explains the hierarchy of configuration settings, the purpose of each setting, the default value, and how to configure it. It also provides information on setting environment variables, TOML configuration, and changes to node configuration starting in v1.1.0 nodes. This document can answer questions about the configuration of Chainlink nodes, such as what environment variables are essential, what database settings are available, and what settings are available for the web server. It can also answer questions about database locking, database backups, logging, and audit logging, as well as how to configure an optional HTTP logger for audit log events.\n",
      "2023-05-30 11:13:11,552 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 3d44d26f-d2b0-4e5d-a9bc-e6b733fedb4b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:13:48,452 - INFO - > [get_response] Total LLM token usage: 379 tokens\n",
      "2023-05-30 11:13:48,453 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:13:48,454 - INFO - > [get_response] Total LLM token usage: 5239 tokens\n",
      "2023-05-30 11:13:48,454 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:13:48,455 - INFO - > Generated summary for doc 3d44d26f-d2b0-4e5d-a9bc-e6b733fedb4b: \n",
      "This document provides a guide on how to use Chainlink to retrieve data from an external API and update a consumer contract. It covers the requirements, address types, setup of an operator contract, whitelisting of a node address, adding a job to the node, creating a request to the node, and withdrawing LINK. This document can answer questions about how to run a Chainlink node, how to deploy an operator contract, how to whitelist a node address, how to add a job to the node, how to create a request to the node, and how to withdraw LINK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 94d354f0-154c-4dbe-8563-9ff928bcc424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:13:48,835 - INFO - > Building index from nodes: 18 chunks\n",
      "2023-05-30 11:13:49,366 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:13:49,367 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:14:16,739 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:14:16,740 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:14:21,156 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:14:21,157 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:14:25,515 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:14:25,517 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:15:10,341 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:15:10,342 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:15:25,742 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:15:25,743 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:15:30,171 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:15:30,173 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:15:59,560 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:15:59,560 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:16:15,824 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:16:15,825 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:16:35,245 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:16:35,246 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:18:00,611 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:18:00,612 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:18:04,977 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:18:04,978 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:18:10,494 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:18:10,497 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:18:14,889 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:18:14,890 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:18:23,245 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:18:23,246 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:18:47,702 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:18:47,703 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:18:52,160 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:18:52,161 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:19:57,220 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:19:57,221 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:20:01,558 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:20:01,559 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:20:27,789 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 11:21:52,926 - INFO - > [get_response] Total LLM token usage: 811 tokens\n",
      "2023-05-30 11:21:52,927 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:21:52,930 - INFO - > [get_response] Total LLM token usage: 74854 tokens\n",
      "2023-05-30 11:21:52,931 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:21:52,932 - INFO - > Generated summary for doc 94d354f0-154c-4dbe-8563-9ff928bcc424: \n",
      "This document provides a comprehensive overview of the configuration settings for Chainlink nodes. It covers topics such as headers, log settings, web server settings, job pipeline settings, GasTipCapBufferPercent, BaseFeeBufferPercent, MaxGracePeriod, TurnLookBack, CheckGasOverhead, PerformGasOverhead, SyncInterval, MaxPerformDataSize, SyncUpkeepQueueSize, AutoPprof, Pyroscope, Sentry, EVM, HeadTracker, OCR2, OCR, P2P, Keeper settings, FinalityDepth, LinkContractAddress, LogBackfillBatchSize, LogPollInterval, LogKeepBlocksDepth, MinIncomingConfirmations, MinContractPayment, NonceAutoSync, NoNewHeadsThreshold, RPCDefaultBatchSize, RPCBlockQueryDelay, ForwardersEnabled, MaxInFlight, MaxQueued, ReaperInterval, ReaperThreshold, ResendAfterThreshold, Enabled, Mode, Price, GasEstimator, BalanceMonitor, and more. This document can answer questions such as: What is the ChainID? What is the minimum payment in LINK required to execute a direct request job? What is the minimum required confirmations before a log event\n",
      "2023-05-30 11:21:52,965 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 099ebc40-8e26-4c53-aacf-92a87597ba65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:22:15,480 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:22:15,481 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:22:34,661 - INFO - > [get_response] Total LLM token usage: 418 tokens\n",
      "2023-05-30 11:22:34,662 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:22:34,663 - INFO - > [get_response] Total LLM token usage: 5527 tokens\n",
      "2023-05-30 11:22:34,663 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:22:34,664 - INFO - > Generated summary for doc 099ebc40-8e26-4c53-aacf-92a87597ba65: \n",
      "This document provides an overview of the Role-Based Access Control (RBAC) system used by Chainlink Nodes. It outlines the four roles available (admin, edit, run, and view) and the specific actions that each role is allowed to perform. It also provides a table that lists the actions that have role-based access and the role that is required to run that action. This document can answer questions such as: What roles are available for Chainlink Nodes? What actions are allowed for each role? What permissions and level of access can be set for multiple users? What tasks can be performed by each role? What is the run command and what does it enable?\n",
      "2023-05-30 11:22:34,687 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f1cb76d4-46e1-4646-b6a4-4ff41dd7f8bf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:22:45,755 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:22:45,756 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:23:11,390 - INFO - > [get_response] Total LLM token usage: 350 tokens\n",
      "2023-05-30 11:23:11,391 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:23:11,392 - INFO - > [get_response] Total LLM token usage: 4516 tokens\n",
      "2023-05-30 11:23:11,392 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:23:11,393 - INFO - > Generated summary for doc f1cb76d4-46e1-4646-b6a4-4ff41dd7f8bf: \n",
      "This document provides instructions on how to run a Chainlink node locally using Docker. It covers the requirements, how to run PostgreSQL, and how to configure and start the Chainlink node. It also explains how to connect the node to the Ethereum Sepolia or Goerli testnet, as well as other supported networks. This document can answer questions about the requirements for running a Chainlink node, how to configure and start the node, how to connect the node to different networks, how to optimize EVM performance, how to perform system maintenance, and what security and operation best practices should be followed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: bf5f73bf-331b-44a6-a73c-9fc2480fe2a5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:23:11,717 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:23:11,718 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:23:16,113 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:23:16,114 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:23:31,305 - INFO - > [get_response] Total LLM token usage: 1815 tokens\n",
      "2023-05-30 11:23:31,306 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:23:31,307 - INFO - > [get_response] Total LLM token usage: 1815 tokens\n",
      "2023-05-30 11:23:31,307 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:23:31,308 - INFO - > Generated summary for doc bf5f73bf-331b-44a6-a73c-9fc2480fe2a5: \n",
      "This document describes the TOML format for secrets used in Chainlink nodes. It provides an overview of the secrets configuration, an example, and details on each secret, such as the Database URL, Password Keystore, VRF, Pyroscope AuthToken, Prometheus AuthToken, and Mercury credentials. It also provides the corresponding environment variables for each secret. This document can answer questions about the secrets configuration for Chainlink nodes, such as what environment variables are associated with each secret, what the Database URL is, and what the Mercury credentials are.\n",
      "2023-05-30 11:23:31,358 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 710ee6e4-7345-44e1-8186-52c65556f597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:23:31,713 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:23:31,714 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:23:36,045 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:23:36,047 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:24:07,391 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:24:07,393 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:24:23,421 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:24:23,422 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:24:27,756 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:24:27,757 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:24:32,083 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:24:32,084 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:24:52,346 - INFO - > [get_response] Total LLM token usage: 583 tokens\n",
      "2023-05-30 11:24:52,347 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:24:52,347 - INFO - > [get_response] Total LLM token usage: 11322 tokens\n",
      "2023-05-30 11:24:52,348 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:24:52,348 - INFO - > Generated summary for doc 710ee6e4-7345-44e1-8186-52c65556f597: \n",
      "This document provides instructions on how to set up a Chainlink node with a forwarder contract and two externally owned accounts (EOAs) for two direct request jobs. It covers topics such as setting up multiple EOAs, deploying the operator and forwarder contracts, whitelisting the forwarder and Chainlink node EOAs, activating the forwarder, creating direct request jobs, testing the transaction-sending strategy, creating API requests, and checking the forwarder. This document can answer questions such as how to set up a Chainlink node with a forwarder contract, how to whitelist the forwarder and Chainlink node EOAs, how to activate the forwarder, how to create direct request jobs, how to use a Chainlink forwarder, how to create jobs, and how to test the transaction-sending strategy.\n",
      "2023-05-30 11:24:52,388 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 374eb4ef-0697-4106-bdb2-4780b020b719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:24:52,779 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:24:52,780 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:25:55,470 - INFO - > [get_response] Total LLM token usage: 621 tokens\n",
      "2023-05-30 11:25:55,471 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:25:55,471 - INFO - > [get_response] Total LLM token usage: 5908 tokens\n",
      "2023-05-30 11:25:55,472 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:25:55,472 - INFO - > Generated summary for doc 374eb4ef-0697-4106-bdb2-4780b020b719: \n",
      "This document provides an overview of Chainlink Data Feeds, which are used to connect smart contracts to real-world data. It explains the different types of data feeds, including Price Feeds, Proof of Reserve Feeds, NFT Floor Price Feeds, Rate and Volatility Feeds, and L2 sequencer uptime feeds. It also explains the components of a data feed, including the consumer, proxy contract, and aggregator contract. Additionally, it explains how to read proxy and aggregator configurations, as well as how to create a consumer contract that uses an existing data feed. Finally, it explains how to monitor data feeds and check the latest answer against reasonable limits. This document can answer questions about Chainlink Data Feeds, such as what types of data feeds are available, how to read proxy and aggregator configurations, and how to create a consumer contract that uses an existing data feed. It can also answer questions about monitoring data feeds, such as how to check the latest answer against reasonable limits, what the heartbeat and deviation thresholds are, and how they can differ for the same asset across different blockchains.\n",
      "2023-05-30 11:25:55,554 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 126ab030-5b09-46eb-87b5-c6d9f2d51594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:25:55,975 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:25:55,976 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:26:19,203 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:26:19,203 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:26:50,251 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:26:50,252 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:27:20,249 - INFO - > [get_response] Total LLM token usage: 736 tokens\n",
      "2023-05-30 11:27:20,249 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:27:20,250 - INFO - > [get_response] Total LLM token usage: 12969 tokens\n",
      "2023-05-30 11:27:20,251 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:27:20,252 - INFO - > Generated summary for doc 126ab030-5b09-46eb-87b5-c6d9f2d51594: \n",
      "This document provides an API reference for Chainlink Data Feeds and AccessControlledOffchainAggregator 2.0.0. It outlines the functions available to interact with the data feed, such as getting the contract version, checking access, retrieving the billing configuration, and getting the number of observations an oracle is due to be reimbursed for. It also provides information on how to get the full historical data for a round, the amount of LINK available for payment, and the most recent answer and transmission details. This document can answer questions about how to use the AggregatorV3Interface to run functions on the proxy, how to call variables and functions in the AccessControlledOffchainAggregator contract, and what variables and functions are available in the AccessControlledOffchainAggregator contract.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 322b531f-795f-41f6-918d-f8c82e3c14be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:27:35,565 - INFO - > [get_response] Total LLM token usage: 1458 tokens\n",
      "2023-05-30 11:27:35,565 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:27:35,566 - INFO - > [get_response] Total LLM token usage: 1458 tokens\n",
      "2023-05-30 11:27:35,566 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:27:35,567 - INFO - > Generated summary for doc 322b531f-795f-41f6-918d-f8c82e3c14be: \n",
      "This document provides information about the deprecation of Chainlink Data Feeds. It outlines the process of evaluating Data Feeds for their usage and economic viability, and explains the shift towards Chainlink Economics 2.0. It also provides a list of Data Feeds that are scheduled for deprecation, along with their corresponding shutdown date. Additionally, it provides instructions on how to stay updated on Data Feeds, and how to reach out with questions. \n",
      "\n",
      "This document can answer questions about the deprecation of Chainlink Data Feeds, the process of evaluating Data Feeds, the shift towards Chainlink Economics 2.0, and the list of Data Feeds that are scheduled for deprecation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 41d87f3a-07ff-46ae-81fe-bd3238313326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:27:36,013 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:27:36,014 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:27:50,223 - INFO - > [get_response] Total LLM token usage: 2530 tokens\n",
      "2023-05-30 11:27:50,224 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:27:50,224 - INFO - > [get_response] Total LLM token usage: 2530 tokens\n",
      "2023-05-30 11:27:50,225 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:27:50,225 - INFO - > Generated summary for doc 41d87f3a-07ff-46ae-81fe-bd3238313326: \n",
      "This document provides an overview of using ENS with Chainlink data feeds. It explains the naming structure, subdomains, architecture, and resolver for data feeds. It also provides examples of how to use Javascript and Solidity to obtain the address of a data feed. This document can answer questions about the naming structure of Chainlink data feeds, the architecture of the ENS system, and how to use Javascript and Solidity to obtain the address of a data feed.\n",
      "2023-05-30 11:27:50,275 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 54dde410-701a-46fa-8608-31abd76a78a2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:28:07,118 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:28:07,119 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:29:02,401 - INFO - > [get_response] Total LLM token usage: 679 tokens\n",
      "2023-05-30 11:29:02,403 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:29:02,404 - INFO - > [get_response] Total LLM token usage: 9226 tokens\n",
      "2023-05-30 11:29:02,405 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:29:02,407 - INFO - > Generated summary for doc 54dde410-701a-46fa-8608-31abd76a78a2: \n",
      "This document provides code examples and instructions for reading data feeds on-chain and off-chain using Solidity, Vyper, Javascript, Python, and Golang. It explains how to specify the RPC endpoint URL, LINK token contract address, and Feed contract address for the network and feed type that you are using, and provides an example of how to use web3.js to retrieve feed data from the BTC/USD feed on the Sepolia testnet. It also provides examples of how to use Chainlink Data Feeds to retrieve price data from the Sepolia testnet, how to use the Feed Registry to reference data feed assets by name or currency identifier, and how to use two data feeds to derive denominated price pairs in other currencies. Additionally, it provides an example of how to use Chainlink Price Feeds to retrieve the latest price data from an aggregator, and how to scale the price to the desired decimals. This document can answer questions about how to read data feeds on-chain and off-chain, what variables need to be specified for different networks and feed types, how to use web3.js to retrieve feed data, how to use Chainlink Data Feeds to retrieve price data, how to use the Feed Registry, how\n",
      "2023-05-30 11:29:02,530 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 65d8f487-f166-4024-b672-80e988cbc363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:29:14,089 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:29:14,090 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:29:38,780 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:29:38,781 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:30:09,281 - INFO - > [get_response] Total LLM token usage: 632 tokens\n",
      "2023-05-30 11:30:09,282 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:30:09,283 - INFO - > [get_response] Total LLM token usage: 13431 tokens\n",
      "2023-05-30 11:30:09,284 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:30:09,285 - INFO - > Generated summary for doc 65d8f487-f166-4024-b672-80e988cbc363: \n",
      "This document provides an overview of the Chainlink Feed Registry, a decentralized oracle network that provides access to real-time data from external sources. It includes functions to confirm a feed, get the current phase ID, get the answer to a round ID, get the next round ID, get the phase feed, get the phase range, get the previous round ID, get the proposed feed, and more. This document can answer questions about the data feeds from Chainlink Aggregator contracts, such as what the current phase ID is, what the answer to a round ID is, and what the proposed feed is, as well as questions about the address of the Feed Registry on the Ethereum Mainnet, how to use the Feed Registry API, and the FeedRegistryInterface contract.\n",
      "2023-05-30 11:30:09,310 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 3f3f3577-947c-409c-add0-e5a2e9da568e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:30:32,859 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:30:32,860 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:31:10,253 - INFO - > [get_response] Total LLM token usage: 623 tokens\n",
      "2023-05-30 11:31:10,254 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:31:10,255 - INFO - > [get_response] Total LLM token usage: 5161 tokens\n",
      "2023-05-30 11:31:10,255 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:31:10,257 - INFO - > Generated summary for doc 3f3f3577-947c-409c-add0-e5a2e9da568e: \n",
      "This document outlines the functions available for Chainlink's Feed Registry, which allows on-chain contracts to read from aggregators. It provides information about the decimals, description, getRoundData, latestRoundData, version, getFeed, getPhaseFeed, isFeedEnabled, getPhase, getRoundFeed, getPhaseRange, getPreviousRoundId, getNextRoundId, and getCurrentPhaseId functions. This document can answer questions such as the number of decimals in the response, the description of the aggregator that the proxy points to, the price from the latest round, the version representing the type of aggregator the proxy points to, the primary aggregator address of a base/quote pair, the underlying aggregator address of a base/quote pair at a specified phase, if an aggregator is enabled as primary on the registry, the starting and ending round ids of a base/quote pair at a specified phase, the previous round id of a base/quote pair given a specified round, the next round id of a base/quote pair given a specified round, and the current phase id of a base/quote pair.\n",
      "2023-05-30 11:31:10,294 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: d91f9fd9-7fa1-48f0-8895-b5f52788fc67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:31:33,964 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:31:33,965 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:31:54,424 - INFO - > [get_response] Total LLM token usage: 430 tokens\n",
      "2023-05-30 11:31:54,424 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:31:54,425 - INFO - > [get_response] Total LLM token usage: 7269 tokens\n",
      "2023-05-30 11:31:54,426 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:31:54,426 - INFO - > Generated summary for doc d91f9fd9-7fa1-48f0-8895-b5f52788fc67: \n",
      "This document provides an overview of how to get historical data from Chainlink Data Feeds. It explains the two parameters that can cause Chainlink nodes to update, the roundIds in the Aggregator and Proxy, and the return values of the getRoundData function. It also provides examples of code that can be used to retrieve historical price data from a Chainlink Price Feed. This document can answer questions about how to get historical data from Chainlink Data Feeds, how to compute the roundIds in the Aggregator and Proxy, how to loop off-chain and on-chain to fetch historical data, and how to retrieve historical price data from a Chainlink Price Feed, as well as how to set up a contract instance and call the getRoundData function.\n",
      "2023-05-30 11:31:54,452 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 0389ba51-c5fb-4fa7-aad8-c180479f008c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:31:54,796 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:31:54,797 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:32:46,910 - INFO - > [get_response] Total LLM token usage: 478 tokens\n",
      "2023-05-30 11:32:46,911 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:32:46,912 - INFO - > [get_response] Total LLM token usage: 5756 tokens\n",
      "2023-05-30 11:32:46,912 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:32:46,913 - INFO - > Generated summary for doc 0389ba51-c5fb-4fa7-aad8-c180479f008c: \n",
      "This document provides an overview of L2 Sequencer Uptime Feeds, which are used to track the status of a sequencer on layer 2 (L2) networks such as Arbitrum, Optimism, and Metis. It explains how the feeds update and how a consumer can retrieve the status of the sequencer, as well as how to handle outages on the L2 networks and create a consumer contract for sequencer uptime feeds. This document can answer questions such as how to use a sequencer and price feed to get the latest price from a data feed, how to check the status of the data feed, and how to wait for the grace period to pass before retrieving the latest price.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 20c34d53-aea8-4d32-9cc8-ebe9221db125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:32:59,985 - INFO - > [get_response] Total LLM token usage: 1661 tokens\n",
      "2023-05-30 11:32:59,985 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:32:59,986 - INFO - > [get_response] Total LLM token usage: 1661 tokens\n",
      "2023-05-30 11:32:59,986 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:32:59,987 - INFO - > Generated summary for doc 20c34d53-aea8-4d32-9cc8-ebe9221db125: \n",
      "This document provides an overview of Chainlink NFT Floor Price Feeds, which are used to provide a conservative and risk averse floor price estimate for an NFT collection. It explains how to use NFT Floor Price Feeds, including how to read answers from Data Feeds and how to get Historical Price Data. It also provides a list of testnet feeds and a link to sign up for access to NFT Floor Price feeds on Ethereum Mainnet. This document can answer questions about how to use NFT Floor Price Feeds, what use cases they are suitable for, and how to access them.\n",
      "2023-05-30 11:33:00,009 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 24d168d3-fa78-47b6-9257-25ed031b8e1f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:33:36,638 - INFO - > [get_response] Total LLM token usage: 389 tokens\n",
      "2023-05-30 11:33:36,639 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:33:36,640 - INFO - > [get_response] Total LLM token usage: 4543 tokens\n",
      "2023-05-30 11:33:36,640 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:33:36,641 - INFO - > Generated summary for doc 24d168d3-fa78-47b6-9257-25ed031b8e1f: \n",
      "This document provides a list of Ethereum Mainnet and Goerli Testnet NFT Floor Price Feed Addresses, as well as information about the data feed categories. It also provides guidance on how to select data feeds for custom deployments, and outlines the importance of doing close diligence on the feeds before implementing them in contracts. Questions this document can answer include: What is the importance of doing close diligence on data feeds before implementing them in contracts? How can I learn more about making responsible data quality decisions? How can I join the community?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 488b2894-7e88-4708-b7a9-23515e4d0cc3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:33:37,562 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:33:37,563 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:33:41,862 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:33:41,863 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:33:46,195 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:33:46,195 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:34:02,315 - INFO - > [get_response] Total LLM token usage: 504 tokens\n",
      "2023-05-30 11:34:02,316 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:34:02,317 - INFO - > [get_response] Total LLM token usage: 504 tokens\n",
      "2023-05-30 11:34:02,317 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:34:02,318 - INFO - > Generated summary for doc 488b2894-7e88-4708-b7a9-23515e4d0cc3: \n",
      "This document provides an overview of Chainlink Data Feeds, which are data aggregated from multiple sources by a decentralized set of independent node operators. It also provides information on how to select quality data feeds, how to read answers from data feeds, how to get historical price data, and where to find contract addresses for price feeds. Additionally, it provides an API reference and links to join the community. This document can answer questions about data feeds, selecting quality data feeds, getting historical price data, and finding contract addresses for price feeds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 65bcceef-f773-4d63-a40f-17a1021bb3ee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:34:02,560 - INFO - > Building index from nodes: 11 chunks\n",
      "2023-05-30 11:36:31,046 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 11:37:16,593 - INFO - > [get_response] Total LLM token usage: 468 tokens\n",
      "2023-05-30 11:37:16,594 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:37:16,595 - INFO - > [get_response] Total LLM token usage: 48530 tokens\n",
      "2023-05-30 11:37:16,595 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:37:16,596 - INFO - > Generated summary for doc 65bcceef-f773-4d63-a40f-17a1021bb3ee: \n",
      "This document provides a list of Ethereum addresses for Chainlink Price Feeds on the Sepolia Testnet. Each address is associated with a pair of assets, a type, a deviation, a heartbeat, and a decimal. This document can be used to answer questions about the addresses of Chainlink Price Feeds on the Sepolia Testnet, the assets associated with each address, the type of asset, the deviation, the heartbeat, and the decimal associated with each address. Questions that this document can answer include what Ethereum addresses are associated with Chainlink Price Feeds, what assets are associated with each address, what type of asset is associated with each address, what deviation is associated with each address, what heartbeat is associated with each address, and what decimal precision is associated with each address.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 2b8e7977-6abb-48d3-8804-0d02b0ecf89c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:37:28,368 - INFO - > [get_response] Total LLM token usage: 2410 tokens\n",
      "2023-05-30 11:37:28,369 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:37:28,369 - INFO - > [get_response] Total LLM token usage: 2410 tokens\n",
      "2023-05-30 11:37:28,370 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:37:28,371 - INFO - > Generated summary for doc 2b8e7977-6abb-48d3-8804-0d02b0ecf89c: \n",
      "This document provides an overview of Chainlink Proof of Reserve Feeds, which provide the status of the reserves for several assets. It explains the different types of Proof of Reserve Feeds, such as off-chain reserves and cross-chain reserves, and how to use them. It also provides a Solidity example of how to read answers from Data Feeds. This document can answer questions about the different types of Proof of Reserve Feeds, how to use them, and how to read answers from Data Feeds.\n",
      "2023-05-30 11:37:28,392 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 84e30ea3-2a40-463f-bff8-3f0ff973a23f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:37:55,669 - INFO - > [get_response] Total LLM token usage: 343 tokens\n",
      "2023-05-30 11:37:55,671 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:37:55,671 - INFO - > [get_response] Total LLM token usage: 4551 tokens\n",
      "2023-05-30 11:37:55,672 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:37:55,673 - INFO - > Generated summary for doc 84e30ea3-2a40-463f-bff8-3f0ff973a23f: \n",
      "This document provides information about Proof of Reserve Feed Addresses on the Ethereum Mainnet and Goerli Testnet. It outlines the risks associated with using wallet address manager contracts, as well as the different categories of data feeds. It also provides a list of Proof of Reserve Feeds, including their deviation, heartbeat, and address information. This document can answer questions about the different types of data feeds, the risks associated with using wallet address manager contracts, the configuration of the Proof of Reserve feed, the data source, and the user's responsibility when using the feed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b7d3809f-7fe4-40e9-8880-8b5e747fa9c6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:38:10,754 - INFO - > [get_response] Total LLM token usage: 1635 tokens\n",
      "2023-05-30 11:38:10,756 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:38:10,757 - INFO - > [get_response] Total LLM token usage: 1635 tokens\n",
      "2023-05-30 11:38:10,758 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:38:10,760 - INFO - > Generated summary for doc b7d3809f-7fe4-40e9-8880-8b5e747fa9c6: \n",
      "This document provides an overview of Chainlink's rate and volatility feeds, which provide data for interest rates, interest rate curves, and asset volatility. It covers Bitcoin Interest Rate Curve, ETH Staking APR, and Realized Volatility feeds. These feeds can be used to evaluate interest rate risk for lending and borrowing contracts, asset valuation for derivatives contracts, and an underlying rate for interest rate swap contracts. The document also provides information on how to read the feeds, as well as the addresses for the feeds. \n",
      "\n",
      "This document can answer questions such as: What are Chainlink's rate and volatility feeds? What data do they provide? How can they be used? How can I read the feeds? What are the addresses for the feeds?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 49061963-cf11-4cbc-8047-dea1b9ec1411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:38:23,175 - INFO - > [get_response] Total LLM token usage: 2100 tokens\n",
      "2023-05-30 11:38:23,178 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:38:23,179 - INFO - > [get_response] Total LLM token usage: 2100 tokens\n",
      "2023-05-30 11:38:23,180 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:38:23,181 - INFO - > Generated summary for doc 49061963-cf11-4cbc-8047-dea1b9ec1411: \n",
      "This document provides a list of addresses for rate and volatility feeds on the Sepolia Testnet. It includes information such as the asset type, deviation, heartbeat, and decimal for each feed. It also provides instructions on how to use the data feeds and LINK token and Faucet details. Additionally, it provides a cautionary note about using feeds built by other community members. \n",
      "\n",
      "This document can answer questions such as: What are the addresses for rate and volatility feeds on the Sepolia Testnet? What is the asset type, deviation, heartbeat, and decimal for each feed? How do I use the data feeds? What are the LINK token and Faucet details?\n",
      "2023-05-30 11:38:23,285 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 39646568-d514-49f1-821b-4fe367627397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:38:23,792 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:38:23,794 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:39:00,543 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:39:00,543 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:39:29,174 - INFO - > [get_response] Total LLM token usage: 637 tokens\n",
      "2023-05-30 11:39:29,175 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:39:29,175 - INFO - > [get_response] Total LLM token usage: 7583 tokens\n",
      "2023-05-30 11:39:29,176 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:39:29,176 - INFO - > Generated summary for doc 39646568-d514-49f1-821b-4fe367627397: \n",
      "This document provides an overview of the different types of data feeds available on the Chainlink Network and the associated risks. It outlines the categories of data feeds, which include Verified Feeds, Monitored Feeds, Provisional Feeds, Custom Feeds, Specialized Feeds, and Deprecating Feeds. It also provides information on risk mitigation, Chainlink Community Deployments, and Evaluating Data Sources and Risks. This document can answer questions about the different types of data feeds available on the Chainlink Network, the associated risks, and how to mitigate those risks. It can also provide information on Chainlink Community Deployments and Evaluating Data Sources and Risks, as well as best practices for selecting and evaluating data feeds for smart contracts, liquidity and its distribution, single source data providers, crypto and blockchain actions, market failures resulting from extreme events, periods of high network congestion, unknown and known users, assessing how to price wrapped or bridged assets, extreme events causing price deviations in wrapped or bridged assets, and front running risk. Questions this document can answer include: What are the best practices for selecting and evaluating data feeds for smart contracts? What should I consider when assessing a Chainlink Price Feed for a wrapped or bridged asset? How can\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 4d864aae-fde2-4915-904e-5c4c1ac02acb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:39:29,738 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:39:29,739 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:39:48,070 - INFO - > [get_response] Total LLM token usage: 2668 tokens\n",
      "2023-05-30 11:39:48,071 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:39:48,071 - INFO - > [get_response] Total LLM token usage: 2668 tokens\n",
      "2023-05-30 11:39:48,072 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:39:48,072 - INFO - > Generated summary for doc 4d864aae-fde2-4915-904e-5c4c1ac02acb: \n",
      "This document provides an overview of Chainlink's data feeds on the Solana network. It explains the use of Off-Chain Reporting (OCR) to aggregate data from data providers, and provides information on the languages, tools, and frameworks used to develop applications on Solana. It also explains how to use wallets and keypairs when deploying programs to the Solana Mainnet. \n",
      "\n",
      "This document can answer questions about Chainlink's data feeds on the Solana network, the languages, tools, and frameworks used to develop applications on Solana, and how to use wallets and keypairs when deploying programs to the Solana Mainnet.\n",
      "2023-05-30 11:39:48,119 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b42ff2cb-c155-403f-a40c-8d8c7d0f509f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:39:48,477 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:39:48,478 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:39:52,856 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:39:52,857 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:39:57,234 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:39:57,235 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:40:34,904 - INFO - > [get_response] Total LLM token usage: 397 tokens\n",
      "2023-05-30 11:40:34,904 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:40:34,905 - INFO - > [get_response] Total LLM token usage: 6260 tokens\n",
      "2023-05-30 11:40:34,905 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:40:34,906 - INFO - > Generated summary for doc b42ff2cb-c155-403f-a40c-8d8c7d0f509f: \n",
      "This document provides a guide on how to read Chainlink Data Feeds on the Solana Devnet using off-chain examples in the Chainlink Solana Starter Kit. It explains how to install the necessary tools, run the example program, and add Data Feeds to an existing off-chain project. It also provides information on selecting quality data feeds and setting the Anchor environment variables. This document can answer questions about how to read Chainlink Data Feeds off-chain, how to install the necessary tools, how to use the Solana Starter Kit to read off-chain data, how to set the Anchor environment variables, and provide more information about Solana and Anchor.\n",
      "2023-05-30 11:40:34,980 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 952d25d1-3dc5-4522-948c-0fa1d30e0a7a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:40:35,366 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:40:35,367 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:41:11,136 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:41:11,138 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:41:52,497 - INFO - > [get_response] Total LLM token usage: 704 tokens\n",
      "2023-05-30 11:41:52,497 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:41:52,498 - INFO - > [get_response] Total LLM token usage: 11987 tokens\n",
      "2023-05-30 11:41:52,498 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:41:52,499 - INFO - > Generated summary for doc 952d25d1-3dc5-4522-948c-0fa1d30e0a7a: \n",
      "This document provides a guide on how to use Chainlink Data Feeds on-chain in Solana. It explains how to deploy a program to the Solana Devnet, call the deployed program to retrieve price data from a Chainlink data feed, and clean up the program when it is no longer needed. It also provides instructions for writing and deploying a program to the Solana Devnet cluster using Anchor, as well as instructions for retrieving price data using the Solana Web3 JavaScript API with Node.js. This document can answer questions about how to use Chainlink Data Feeds on-chain in Solana, how to write and deploy programs to the Solana Devnet cluster, how to retrieve price data using the Solana Web3 JavaScript API, how to install the required tools and deploy the example program from the solana-starter-kit repository, how to call a deployed program to retrieve price data from a Chainlink data feed, how to check the balance of a wallet, and how to transfer SOL tokens from a temporary wallet to a web wallet.\n",
      "2023-05-30 11:41:52,523 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ba0a3fef-b767-45b1-9132-fc42ecb82e01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:41:52,870 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:41:52,872 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:42:13,651 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:42:13,652 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:42:17,955 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:42:17,955 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:42:47,912 - INFO - > [get_response] Total LLM token usage: 482 tokens\n",
      "2023-05-30 11:42:47,913 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:42:47,914 - INFO - > [get_response] Total LLM token usage: 5376 tokens\n",
      "2023-05-30 11:42:47,915 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:42:47,915 - INFO - > Generated summary for doc ba0a3fef-b767-45b1-9132-fc42ecb82e01: \n",
      "This document provides instructions on how to use Chainlink Data Feeds on StarkNet, a permissionless decentralized ZK-Rollup operating as an L2 network over Ethereum. It outlines the requirements for setting up the environment, running the on-chain example, and running the off-chain example. This document can answer questions such as: How do I use Chainlink Data Feeds on StarkNet? What are the requirements for setting up the environment? How do I run the on-chain and off-chain examples? How do I read data from a data feed proxy address on the StarkNet testnet? How do I use the `starknet call` CLI command?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 1850a9ee-3a96-444a-8f01-cbb047d2837c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:43:01,176 - INFO - > [get_response] Total LLM token usage: 2797 tokens\n",
      "2023-05-30 11:43:01,177 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:43:01,177 - INFO - > [get_response] Total LLM token usage: 2797 tokens\n",
      "2023-05-30 11:43:01,177 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:43:01,179 - INFO - > Generated summary for doc 1850a9ee-3a96-444a-8f01-cbb047d2837c: \n",
      "This document provides an overview of how to use Chainlink Data Feeds to connect smart contracts to real-world services or off-chain data. It includes instructions on how to compile, deploy, and run a sample contract that obtains the latest price answer from the BTC/USD feed on the Sepolia testnet. This document can answer questions about how to use Data Feeds on EVM Chains, how to configure and fund a MetaMask wallet, how to deploy a smart contract, and how to generate random numbers.\n",
      "2023-05-30 11:43:01,211 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 58cfdc8c-f384-4058-be5e-72e50c1216d3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:43:13,765 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:43:13,766 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:43:51,834 - INFO - > [get_response] Total LLM token usage: 467 tokens\n",
      "2023-05-30 11:43:51,835 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:43:51,835 - INFO - > [get_response] Total LLM token usage: 6656 tokens\n",
      "2023-05-30 11:43:51,836 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:43:51,836 - INFO - > Generated summary for doc 58cfdc8c-f384-4058-be5e-72e50c1216d3: \n",
      "This document provides an overview of how to use Chainlink to make API calls in a smart contract and connect a smart contract to an external API. It explains the request and receive cycle, what jobs and tasks are, how to find the Oracle Jobs and Tasks for a contract, how to set up the contract with the Oracle address, Job ID, and LINK fee, how to build and send a request to the oracle, how to define the fulfillment function, and how to withdraw LINK tokens from the contract. This document can answer questions about how to use Chainlink to make API calls, how to find Oracle Jobs and Tasks, how to parse the response from an API call, how to set up the contract, how to build and send a request to the oracle, how to define the fulfillment function, and how to withdraw LINK tokens from the contract.\n",
      "2023-05-30 11:43:51,866 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: a655bf8e-5a07-4978-b492-96dc7d724a9f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:44:34,180 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:44:34,181 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:44:58,252 - INFO - > [get_response] Total LLM token usage: 594 tokens\n",
      "2023-05-30 11:44:58,253 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:44:58,254 - INFO - > [get_response] Total LLM token usage: 5470 tokens\n",
      "2023-05-30 11:44:58,255 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:44:58,256 - INFO - > Generated summary for doc a655bf8e-5a07-4978-b492-96dc7d724a9f: \n",
      "This document provides an overview of smart contract development and oracle networks, including topics such as what a smart contract is, what language it is written in, what a LINK token is, and what oracles are. It also explains Chainlink data feeds and oracles, Remix, and MetaMask. It can answer questions such as what a smart contract is, what language it is written in, what a LINK token is, what oracles are, how to use Chainlink data feeds to obtain accurate real-time asset prices in smart contracts, how to use oracles to generate verifiable random numbers, call external APIs, and automate smart contract functions, how to use Remix to create, run, and debug smart contracts, and how to use MetaMask to create an address, store funds, and interact with Ethereum compatible blockchains.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 800f2e03-5c8e-45b3-882d-3e90055c9c76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:45:10,577 - INFO - > [get_response] Total LLM token usage: 2834 tokens\n",
      "2023-05-30 11:45:10,578 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:45:10,578 - INFO - > [get_response] Total LLM token usage: 2834 tokens\n",
      "2023-05-30 11:45:10,579 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:45:10,579 - INFO - > Generated summary for doc 800f2e03-5c8e-45b3-882d-3e90055c9c76: \n",
      "This document provides a tutorial on how to consume data feeds in smart contracts. It explains how to use the AggregatorV3Interface to connect a smart contract to a proxy aggregator contract that is already deployed. It also explains how to compile, deploy, and run the contract, as well as how to configure and fund a MetaMask wallet. This document can answer questions about how to connect a smart contract to real-world services or off-chain data, how to use the AggregatorV3Interface, how to compile, deploy, and run a contract, and how to configure and fund a MetaMask wallet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 0ec80e1f-6c0e-4426-837f-bf8b1460867c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:45:26,862 - INFO - > [get_response] Total LLM token usage: 2901 tokens\n",
      "2023-05-30 11:45:26,863 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:45:26,863 - INFO - > [get_response] Total LLM token usage: 2901 tokens\n",
      "2023-05-30 11:45:26,865 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:45:26,866 - INFO - > Generated summary for doc 0ec80e1f-6c0e-4426-837f-bf8b1460867c: \n",
      "This document provides a step-by-step guide on how to write, compile, and deploy a smart contract using the Solidity language, MetaMask wallet, and the Remix Development Environment. It explains how to install and fund a MetaMask wallet with free testnet ETH, and how to write a simple 'HelloWorld.sol' example contract. It also explains how to compile the contract, deploy it to the blockchain, and run functions in the contract. This document can answer questions about how to write, compile, and deploy a smart contract, how to install and fund a MetaMask wallet, and how to run functions in a contract.\n",
      "2023-05-30 11:45:26,943 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 16fb266a-84f6-4ba1-9b4e-c29f132b969b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:45:52,482 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:45:52,483 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:45:56,793 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:45:56,794 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:46:28,966 - INFO - > [get_response] Total LLM token usage: 553 tokens\n",
      "2023-05-30 11:46:28,967 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:46:28,968 - INFO - > [get_response] Total LLM token usage: 9147 tokens\n",
      "2023-05-30 11:46:28,969 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:46:28,970 - INFO - > Generated summary for doc 16fb266a-84f6-4ba1-9b4e-c29f132b969b: \n",
      "This document provides a guide on how to use Chainlink VRF to generate random numbers in smart contracts. It explains the Request and Receive cycle, the payment process for generating a random number, and how to use Chainlink VRF. It also provides an example of a contract that uses Chainlink VRF to generate random numbers between 1 and 20, each representing a house from the Game of Thrones. This document can answer questions about how to generate randomness on blockchains, how to use Chainlink VRF, how to implement a Request and Receive cycle with Chainlink oracles, how to create a VRF-based contract, how to deploy it to a test network, how to pass parameters to the constructor, deploying a VRF contract, adding it to a subscription account, and testing the `rollDice` function.\n",
      "2023-05-30 11:46:29,040 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 2e0b9a73-b05e-4f51-a304-3c8a140e1571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:46:50,410 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:46:50,412 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:47:07,773 - INFO - > [get_response] Total LLM token usage: 343 tokens\n",
      "2023-05-30 11:47:07,774 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:47:07,774 - INFO - > [get_response] Total LLM token usage: 5795 tokens\n",
      "2023-05-30 11:47:07,774 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:47:07,775 - INFO - > Generated summary for doc 2e0b9a73-b05e-4f51-a304-3c8a140e1571: \n",
      "This document provides a list of resources to help users learn about Chainlink, a blockchain-based platform. It includes links to video tutorials, coding bootcamps, starter kits, and external tutorials. It also provides a list of applications of Chainlink's products and services sorted by difficulty and type. This document can answer questions about how to use Chainlink products and services, how to build smart contracts, and how to use Chainlink with Hardhat. It can also answer questions about how to get started with Chainlink, what resources are available for learning more about Chainlink, and how to join the Chainlink community.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 97b14c11-5624-4448-bdf4-c042f104ade7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:47:08,080 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:47:08,081 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:47:20,340 - INFO - > [get_response] Total LLM token usage: 771 tokens\n",
      "2023-05-30 11:47:20,341 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:47:20,342 - INFO - > [get_response] Total LLM token usage: 771 tokens\n",
      "2023-05-30 11:47:20,342 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:47:20,343 - INFO - > Generated summary for doc 97b14c11-5624-4448-bdf4-c042f104ade7: \n",
      "This document provides instructions on how to acquire testnet LINK tokens and add them to a MetaMask wallet. It explains how to configure MetaMask to use LINK tokens, and how to get testnet LINK from a faucet. Questions that this document can answer include: How do I configure MetaMask to use LINK tokens? How do I get testnet LINK from a faucet?\n",
      "2023-05-30 11:47:20,367 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 5607c78c-1280-4439-96b3-9e8b0108e29d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:47:20,818 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:47:20,819 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:47:25,201 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:47:25,202 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:48:24,699 - INFO - > [get_response] Total LLM token usage: 616 tokens\n",
      "2023-05-30 11:48:24,700 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:48:24,701 - INFO - > [get_response] Total LLM token usage: 4841 tokens\n",
      "2023-05-30 11:48:24,701 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:48:24,702 - INFO - > Generated summary for doc 5607c78c-1280-4439-96b3-9e8b0108e29d: \n",
      "This document provides an overview of cross-chain bridges and associated risks. It explains what a cross-chain bridge is, the trade-offs associated with them, and the risks that come with using them. It outlines four general interoperability solutions for validating the state of a source blockchain and relaying the subsequent transaction to the destination blockchain, as well as Systemic Financial Risks, Early Stage Risks, and Trust-minimization (Counterparty Risk). Additionally, it covers two different methods of cross-chain communication: local verification and native verification. \n",
      "\n",
      "This document can answer questions such as: What is a cross-chain bridge? What are the trade-offs associated with them? What are the risks associated with using them? What are the four general interoperability solutions for validating the state of a source blockchain and relaying the subsequent transaction to the destination blockchain? What are Systemic Financial Risks, Early Stage Risks, and Trust-minimization (Counterparty Risk)? What are the two different methods of cross-chain communication, the trust models associated with each, and the advantages and disadvantages of each? What is an example of a natively verified bridge?\n",
      "2023-05-30 11:48:24,729 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: cf61e011-418d-44bc-9852-d074ad9d90cb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:48:25,293 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:48:25,294 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:49:02,135 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:49:02,136 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:49:28,410 - INFO - > [get_response] Total LLM token usage: 619 tokens\n",
      "2023-05-30 11:49:28,411 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:49:28,411 - INFO - > [get_response] Total LLM token usage: 4980 tokens\n",
      "2023-05-30 11:49:28,412 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:49:28,412 - INFO - > Generated summary for doc cf61e011-418d-44bc-9852-d074ad9d90cb: \n",
      "This document provides an overview of the ways in which developers and community members can contribute to the Chainlink project. It covers topics such as contributing to software and tooling, raising an issue, requesting a new feature, submitting a pull request, contributing to the documentation, creating community content, becoming a developer expert, joining the Chainlink Community Advocate program, running a Chainlink focused developer bootcamp, running an in-person meetup or watch party, participating in a hackathon, and applying for a grant. Questions that this document can answer include: What does it mean to contribute to the Chainlink project? What are the ways to contribute to the Chainlink project? How can I contribute to software and tooling? How can I request a new feature? How can I contribute to the documentation? How can I create community content? How can I become a developer expert? How can I join the Chainlink Community Advocate program? How can I run a Chainlink focused developer bootcamp? How can I run an in-person meetup or watch party? How can I participate in a hackathon? How can I apply for a grant?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c470f717-86ac-4c91-b6b4-cd49369c7fcc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:49:28,865 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:49:28,866 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:49:47,056 - INFO - > [get_response] Total LLM token usage: 1908 tokens\n",
      "2023-05-30 11:49:47,058 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:49:47,058 - INFO - > [get_response] Total LLM token usage: 1908 tokens\n",
      "2023-05-30 11:49:47,059 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:49:47,060 - INFO - > Generated summary for doc c470f717-86ac-4c91-b6b4-cd49369c7fcc: \n",
      "This document provides instructions on how to install and use Chainlink in existing projects, as well as how to create a new project using Chainlink Starter Kits. It covers the installation of the Chainlink library using NPM or Yarn, as well as instructions for using Hardhat, Brownie, Truffle, Foundry, Apeworx, and Anchor Starter Kits. It also provides instructions on how to test Chainlink contracts. \n",
      "\n",
      "This document can answer questions such as: How do I install and use Chainlink in my project? How do I create a new project using Chainlink Starter Kits? How do I test Chainlink contracts?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 4c28ae23-9ee5-42b8-8e2d-3eb2feffbf6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:49:47,355 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:49:47,355 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:50:01,493 - INFO - > [get_response] Total LLM token usage: 397 tokens\n",
      "2023-05-30 11:50:01,494 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:50:01,494 - INFO - > [get_response] Total LLM token usage: 397 tokens\n",
      "2023-05-30 11:50:01,495 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:50:01,495 - INFO - > Generated summary for doc 4c28ae23-9ee5-42b8-8e2d-3eb2feffbf6b: \n",
      "This document provides information about the Chainlink Developer mailing list, which is the best place to stay up to date on releases, package updates, new features, breaking changes, events, and connecting with other developers. It also provides a link to the WeChat Chinese Developer Community. This document can answer questions about the Chainlink Developer mailing list, how to stay up to date on releases, package updates, new features, breaking changes, events, and how to connect with other developers.\n",
      "2023-05-30 11:50:01,525 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 417d4941-b3c4-4dcc-8496-5bdcd3f1d748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:50:01,857 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:50:01,858 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:50:40,078 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:50:40,079 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:51:07,912 - INFO - > [get_response] Total LLM token usage: 613 tokens\n",
      "2023-05-30 11:51:07,913 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:07,914 - INFO - > [get_response] Total LLM token usage: 6135 tokens\n",
      "2023-05-30 11:51:07,914 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:07,915 - INFO - > Generated summary for doc 417d4941-b3c4-4dcc-8496-5bdcd3f1d748: \n",
      "This document provides an overview of example projects that have utilized Chainlink contracts in hackathons. It includes a list of hackathon projects, tutorials, and winners from the Chainlink Spring 2021 Hackathon, ETHGlobal's ETHOnline 2020, Chainlink Hackathon 2020, ETHGlobal Hack Money, ETHDenver 2020, Chainlink Virtual Hackathon 2019, ETHBerlin 2019, ETHNewYork 2019, and ETHParis 2019. It provides a summary of the winning projects from four Chainlink-sponsored hackathons, including their name, description, and GitHub link. This document can answer questions such as: What projects have been created using Chainlink contracts? What hackathons have Chainlink contracts been used in? Who are the winners of the Chainlink Spring 2021 Hackathon? What projects were created in ETHGlobal's ETHOnline 2020? What projects were created in the Chainlink Hackathon 2020? Who are the winners of ETHGlobal Hack Money? Who are the winners of ETHDenver 2020? Who are the winners of the Chainlink Virtual Hackathon 2019? Who are the winners of ETHBerlin 2019? Who are the winners of ETHNewYork 2019? Who are the winners of ETHParis 2019?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 2405e68f-6397-47e4-9b13-731a6a7a5de7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:51:19,918 - INFO - > [get_response] Total LLM token usage: 655 tokens\n",
      "2023-05-30 11:51:19,918 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:19,919 - INFO - > [get_response] Total LLM token usage: 655 tokens\n",
      "2023-05-30 11:51:19,919 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:19,920 - INFO - > Generated summary for doc 2405e68f-6397-47e4-9b13-731a6a7a5de7: \n",
      "This document provides a guide on how to fund Solidity contracts with LINK or ETH. It explains how to retrieve the contract address and how to send funds to the contract using MetaMask. It also provides instructions on how to acquire testnet LINK if it is not listed in the Asset drop down menu. Additionally, it provides instructions on how to update the gas limit for the token transfer to be successful. This document can answer questions about how to fund Solidity contracts, how to retrieve the contract address, how to send funds to the contract, and how to acquire testnet LINK.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 5d682edd-14db-4a7f-81ab-e57fa5d50c34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:51:31,200 - INFO - > [get_response] Total LLM token usage: 1939 tokens\n",
      "2023-05-30 11:51:31,202 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:31,202 - INFO - > [get_response] Total LLM token usage: 1939 tokens\n",
      "2023-05-30 11:51:31,203 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:31,204 - INFO - > Generated summary for doc 5d682edd-14db-4a7f-81ab-e57fa5d50c34: \n",
      "This document provides a guide for users to get help and support when using Chainlink. It outlines five steps to follow when looking for help, including double checking the documentation, doing a web search, opening an issue on GitHub or the code repository, asking a question on Stack Overflow or Stack Exchange Ethereum, and asking the community. This document can answer questions about how to get help and support when using Chainlink, how to format an issue on GitHub, and how to ask a question on Stack Overflow or Stack Exchange Ethereum.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: b16cfceb-3015-4224-bac2-23046a3d40c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:51:31,572 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:51:31,572 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:51:35,977 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:51:35,977 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:51:46,473 - INFO - > [get_response] Total LLM token usage: 1021 tokens\n",
      "2023-05-30 11:51:46,473 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:46,474 - INFO - > [get_response] Total LLM token usage: 1021 tokens\n",
      "2023-05-30 11:51:46,474 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:51:46,475 - INFO - > Generated summary for doc b16cfceb-3015-4224-bac2-23046a3d40c9: \n",
      "This document provides resources for hackathon projects, including starter kits, support communications, tutorials, and inspiration. It also provides a note on resources, which directs readers to the Learning Resources page for a comprehensive list of resources. This document can answer questions about what resources are available for hackathon projects, where to find starter kits, how to get support, and what tutorials and inspiration are available.\n",
      "2023-05-30 11:51:46,559 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c4985a7a-da66-4839-b624-651643ac5ad6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:52:46,825 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:52:46,828 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:52:51,134 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:52:51,136 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:52:55,465 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:52:55,466 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:53:26,622 - INFO - > [get_response] Total LLM token usage: 855 tokens\n",
      "2023-05-30 11:53:26,623 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:53:26,624 - INFO - > [get_response] Total LLM token usage: 14105 tokens\n",
      "2023-05-30 11:53:26,625 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:53:26,626 - INFO - > Generated summary for doc c4985a7a-da66-4839-b624-651643ac5ad6: \n",
      "This document provides an overview of the Chainlink Token (LINK) contracts, including information about the different versions of the token available on different networks, such as Ethereum Mainnet, Sepolia testnet, Goerli testnet, BNB Chain mainnet, BNB Chain testnet, Polygon (Matic) mainnet, Mumbai testnet, RSK mainnet, Gnosis Chain (xDai) mainnet, Avalanche mainnet, Fuji testnet, Fantom mainnet, Fantom testnet, Arbitrum mainnet, Arbitrum Goerli testnet, Optimism mainnet, Optimism Goerli testnet, Harmony mainnet, Moonriver mainnet, Moonbeam mainnet, Metis, Andromeda mainnet, BASE, and BASE Goerli testnet. It also provides information about the ERC677 transferAndCall token standard, the smallest denomination of LINK (Juel), and the risks associated with using cross-chain bridges. This document can answer questions about the different versions of the Chainlink Token, such as what network it is available on, what the address is, and what the symbol is, as well as questions about how to transfer LINK from Ethereum Mainnet to other networks.\n",
      "2023-05-30 11:53:26,665 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 540e58bc-81b8-455a-b4bb-42abaa3db3ea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:53:27,030 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:53:27,033 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:53:31,352 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:53:31,354 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:54:15,035 - INFO - > [get_response] Total LLM token usage: 425 tokens\n",
      "2023-05-30 11:54:15,035 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:15,036 - INFO - > [get_response] Total LLM token usage: 5360 tokens\n",
      "2023-05-30 11:54:15,036 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:15,037 - INFO - > Generated summary for doc 540e58bc-81b8-455a-b4bb-42abaa3db3ea: \n",
      "This document provides an introduction to Chainlink VRF, a provably fair and verifiable random number generator (RNG) that enables smart contracts to access random values without compromising security or usability. It explains two methods for requesting randomness: subscription and direct funding, and provides guidance on which method is best suited for different use cases. It also provides security considerations and examples of applications that can benefit from Chainlink VRF. This document can answer questions about the two methods for requesting randomness, the benefits of Chainlink VRF, the differences between the two methods, how to stay up to date on when VRF v2 becomes available on more networks, and security considerations when using Chainlink VRF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 0675339e-d20f-4785-921e-7404237973db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:54:23,118 - INFO - > [get_response] Total LLM token usage: 1063 tokens\n",
      "2023-05-30 11:54:23,120 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:23,121 - INFO - > [get_response] Total LLM token usage: 1063 tokens\n",
      "2023-05-30 11:54:23,121 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:23,122 - INFO - > Generated summary for doc 0675339e-d20f-4785-921e-7404237973db: \n",
      "This document is an API reference for Chainlink VRF v1. It provides an overview, index, constructors, functions, and reference for the VRFConsumerBase contract. It also provides information on maximizing security when using Chainlink VRF. This document can answer questions about the constructors and functions of the VRFConsumerBase contract, as well as provide information on maximizing security when using Chainlink VRF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f3a39b2e-80b2-4bb8-a900-59f594eeec52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:54:34,042 - INFO - > [get_response] Total LLM token usage: 1605 tokens\n",
      "2023-05-30 11:54:34,043 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:34,043 - INFO - > [get_response] Total LLM token usage: 1605 tokens\n",
      "2023-05-30 11:54:34,044 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:34,045 - INFO - > Generated summary for doc f3a39b2e-80b2-4bb8-a900-59f594eeec52: \n",
      "This document provides best practices for using Chainlink VRF v1. It covers topics such as getting a random number within a range, getting multiple random numbers, and having multiple VRF requests in flight. It also provides examples of how to use mappings to store data related to the requests. This document can answer questions such as how to generate a random number within a given range, how to get multiple random numbers from a single VRF response, and how to store data related to VRF requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 2cc14ae2-6ddf-4bbf-903c-7f04258a9c68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:54:45,220 - INFO - > [get_response] Total LLM token usage: 2688 tokens\n",
      "2023-05-30 11:54:45,221 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:45,222 - INFO - > [get_response] Total LLM token usage: 2688 tokens\n",
      "2023-05-30 11:54:45,223 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:54:45,223 - INFO - > Generated summary for doc 2cc14ae2-6ddf-4bbf-903c-7f04258a9c68: \n",
      "This document explains how to use Chainlink VRF to get a random number inside a smart contract. It provides an example contract that inherits from VRFConsumerBase and defines two required functions: requestRandomness and fulfillRandomness. It also provides information on how to acquire testnet LINK and ETH, as well as security considerations and network congestion and responsiveness. This document can answer questions about how to use Chainlink VRF to get a random number, how to fund a contract, and how to handle network congestion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ec5747f1-eb04-441b-a1ff-fc963f59d533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:54:45,609 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:54:45,610 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:54:49,943 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:54:49,945 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:55:05,713 - INFO - > [get_response] Total LLM token usage: 755 tokens\n",
      "2023-05-30 11:55:05,714 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:55:05,714 - INFO - > [get_response] Total LLM token usage: 755 tokens\n",
      "2023-05-30 11:55:05,715 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:55:05,716 - INFO - > Generated summary for doc ec5747f1-eb04-441b-a1ff-fc963f59d533: \n",
      "This document provides an introduction to Chainlink VRF v1, a provably-fair and verifiable source of randomness designed for smart contracts. It explains how developers can use Chainlink VRF as a tamper-proof random number generator (RNG) to build reliable smart contracts for applications that rely on unpredictable outcomes. It also explains how Chainlink VRF enables smart contracts to access randomness without compromising on security or usability. This document can answer questions about how to generate random numbers in smart contracts, how to verify randomness on-chain, and what networks are supported by Chainlink VRF.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 47d5d874-b074-4ea1-b241-59af2a2af375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:55:19,842 - INFO - > [get_response] Total LLM token usage: 2002 tokens\n",
      "2023-05-30 11:55:19,845 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:55:19,846 - INFO - > [get_response] Total LLM token usage: 2002 tokens\n",
      "2023-05-30 11:55:19,847 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:55:19,848 - INFO - > Generated summary for doc 47d5d874-b074-4ea1-b241-59af2a2af375: \n",
      "This document provides an overview of security considerations for using Chainlink's VRF v1 service. It explains the importance of using the requestId to match randomness requests with their fulfillment, choosing a safe block confirmation time, not re-requesting randomness, not accepting bids/bets/inputs after making a randomness request, and using VRFConsumerBase in the contract. It also provides links to the consensus documentation for Ethereum, BNB Chain, and Polygon. This document can answer questions about how to securely use Chainlink's VRF v1 service, how to match randomness requests with their fulfillment, and how to choose a safe block confirmation time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f6ae8498-ecd3-4766-b5a9-31889da6514d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:55:20,279 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:55:20,280 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:55:24,606 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 11:55:24,607 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 11:55:45,545 - INFO - > [get_response] Total LLM token usage: 3824 tokens\n",
      "2023-05-30 11:55:45,546 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:55:45,546 - INFO - > [get_response] Total LLM token usage: 3824 tokens\n",
      "2023-05-30 11:55:45,547 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:55:45,547 - INFO - > Generated summary for doc f6ae8498-ecd3-4766-b5a9-31889da6514d: \n",
      "This document provides an overview of the networks supported by Chainlink VRF, a service that allows users to integrate provably-fair and verifiably random data into their smart contracts. It provides information on the LINK token, VRF coordinator, key hash, and fee associated with each network. It also provides information on the response times and faucets for the Polygon (Matic) Mainnet, Polygon (Matic) Mumbai Testnet, BNB Chain Mainnet, BNB Chain Testnet, Ethereum Mainnet, and Goerli networks. \n",
      "\n",
      "This document can answer questions about the LINK token, VRF coordinator, key hash, and fee associated with each network supported by Chainlink VRF, as well as the response times and faucets for each network.\n",
      "2023-05-30 11:55:45,595 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 32f3482b-495a-4208-8eb7-4e2bab1df1a6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:56:27,284 - INFO - > [get_response] Total LLM token usage: 412 tokens\n",
      "2023-05-30 11:56:27,285 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:56:27,286 - INFO - > [get_response] Total LLM token usage: 4821 tokens\n",
      "2023-05-30 11:56:27,286 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:56:27,287 - INFO - > Generated summary for doc 32f3482b-495a-4208-8eb7-4e2bab1df1a6: \n",
      "This document provides best practices for using Chainlink VRF, a service that provides randomness for smart contracts. It outlines the steps for setting up a VRF, including generating a random number, verifying the random number, and emitting an event. It also provides security considerations to review when using VRF. Questions that this document can answer include: How do I set up a VRF? How do I generate a random number? How do I verify a random number? How do I use the VRF in different scenarios? How to generate a random number within a given range? How to get multiple random values from a single VRF request? How to process VRF responses depending on predetermined conditions?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: bda48270-f3c3-48ce-b8ee-0e7982aa7fb1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:56:37,899 - INFO - > [get_response] Total LLM token usage: 2547 tokens\n",
      "2023-05-30 11:56:37,900 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:56:37,900 - INFO - > [get_response] Total LLM token usage: 2547 tokens\n",
      "2023-05-30 11:56:37,901 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:56:37,902 - INFO - > Generated summary for doc bda48270-f3c3-48ce-b8ee-0e7982aa7fb1: \n",
      "This document explains how to generate random numbers using the direct funding method in Chainlink VRF v2. It outlines the steps for setting up a consuming contract, submitting a VRF request, and how the request is processed. It also provides information on the limits of the request, such as the maximum number of random values per request and the maximum allowed callback gas limit. This document can answer questions about how to set up a consuming contract, how to submit a VRF request, and what the limits are for the request.\n",
      "2023-05-30 11:56:37,940 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: c2320868-cbd3-4910-a67c-e511784c6115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:57:22,826 - INFO - > [get_response] Total LLM token usage: 461 tokens\n",
      "2023-05-30 11:57:22,827 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:57:22,827 - INFO - > [get_response] Total LLM token usage: 6565 tokens\n",
      "2023-05-30 11:57:22,828 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:57:22,829 - INFO - > Generated summary for doc c2320868-cbd3-4910-a67c-e511784c6115: \n",
      "This document provides a guide on how to get random values using Chainlink VRF v2 without managing a subscription. It explains the requirements, how to create and deploy a VRF v2 compatible contract, how to fund the contract, how to request random values, how to analyze the contract, and how to clean up. This document can answer questions about how to get random values using Chainlink VRF v2, how to create and deploy a VRF v2 compatible contract, how to fund the contract, how to request random values, how to analyze the contract, and how to clean up. It also provides instructions on how to use the VRF v2 Direct Funding feature, what parameters are needed to submit a request, what functions are available in the contract, and how to withdraw any remaining LINK tokens from the contract.\n",
      "2023-05-30 11:57:22,942 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 65a6de2a-6c0b-498c-a798-06ab973c6eac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:58:25,698 - INFO - > [get_response] Total LLM token usage: 699 tokens\n",
      "2023-05-30 11:58:25,698 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:58:25,699 - INFO - > [get_response] Total LLM token usage: 10878 tokens\n",
      "2023-05-30 11:58:25,699 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:58:25,700 - INFO - > Generated summary for doc 65a6de2a-6c0b-498c-a798-06ab973c6eac: \n",
      "This document provides a guide on how to test Chainlink VRF v2 on a Remix IDE sandbox blockchain environment. It explains the benefits of local testing, the testing logic, and the prerequisites. It also provides instructions on how to deploy the necessary contracts, configure the VRFV2Wrapper, fund the VRFV2Wrapper subscription, deploy the VRF consumer contract, fund the consumer contract, request random words, fulfill the VRF request, and check the results. This document can answer questions such as what are the benefits of local testing, what is the testing logic, what are the prerequisites for testing, how to deploy the necessary contracts, how to configure the VRFV2Wrapper, how to fund the VRFV2Wrapper subscription, how to deploy the VRF consumer contract, how to fund the consumer contract, how to request random words, how to fulfill the VRF request, and how to check the results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: a49b6454-6a49-4575-a4fe-f34e5bcf7e91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 11:58:36,295 - INFO - > [get_response] Total LLM token usage: 1390 tokens\n",
      "2023-05-30 11:58:36,296 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:58:36,296 - INFO - > [get_response] Total LLM token usage: 1390 tokens\n",
      "2023-05-30 11:58:36,297 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 11:58:36,298 - INFO - > Generated summary for doc a49b6454-6a49-4575-a4fe-f34e5bcf7e91: \n",
      "This document provides a guide for migrating from VRF v1 to VRF v2 (Direct funding method). It outlines the similarities and differences between the two versions, and provides instructions for updating applications to use VRF v2. It also includes security considerations and a comparison between VRF v1 and VRF v2. This document can answer questions about the differences between VRF v1 and VRF v2, how to update applications to use VRF v2, and security considerations when using VRF v2.\n",
      "2023-05-30 11:58:36,363 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 267c43c9-1843-4a7c-bf48-52fcd8ceee37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:00:01,166 - INFO - > [get_response] Total LLM token usage: 891 tokens\n",
      "2023-05-30 12:00:01,168 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:00:01,170 - INFO - > [get_response] Total LLM token usage: 12421 tokens\n",
      "2023-05-30 12:00:01,170 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:00:01,171 - INFO - > Generated summary for doc 267c43c9-1843-4a7c-bf48-52fcd8ceee37: \n",
      "This document provides information about the VRF v2 Direct Funding Method, which allows users to integrate provably fair and verifiably random data into their smart contracts. It includes information about the parameters configured in the VRF v2 Wrapper and Coordinator contracts, as well as the fees associated with using the service. It also provides information about the supported networks, including Ethereum mainnet, Sepolia testnet, Goerli testnet, BNB Chain, BNB Chain testnet, Polygon (Matic) mainnet, Polygon (Matic) Mumbai testnet, Avalanche mainnet, Avalanche Fuji testnet, Fantom mainnet, and Fantom testnet. This document can answer questions about the parameters configured in the VRF v2 Wrapper and Coordinator contracts, the fees associated with using the service, the security considerations associated with using the service, the details for calculating the total transaction cost, the LINK token and its compatibility with Chainlink oracles on different networks, and the LINK token, VRF Wrapper, VRF Coordinator, Wrapper Premium Percentage, Coordinator Flat Fee, Minimum Confirmations, Maximum Confirmations, Maximum Random Values, Wrapper Gas overhead, and Coordinator Gas Overhead for each network.\n",
      "2023-05-30 12:00:01,228 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 7dcfc16c-feea-4b43-8a12-3f127ef119bf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:00:01,675 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:00:01,676 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:00:46,847 - INFO - > [get_response] Total LLM token usage: 428 tokens\n",
      "2023-05-30 12:00:46,848 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:00:46,849 - INFO - > [get_response] Total LLM token usage: 6630 tokens\n",
      "2023-05-30 12:00:46,850 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:00:46,851 - INFO - > Generated summary for doc 7dcfc16c-feea-4b43-8a12-3f127ef119bf: \n",
      "This document provides an overview of how to estimate the cost of a Chainlink VRF request on supported networks, including Ethereum and Arbitrum. It explains the variables used to calculate the cost of a request, such as the gas price, callback gas, verification gas, and wrapper overhead gas. It also explains how to estimate the minimum subscription balance needed for a request, and how to calculate the total cost of a request, including the LINK premium. This document can answer questions about how to calculate the cost of a VRF request, how to estimate the minimum subscription balance, how to convert gas costs to LINK, and how to calculate the total cost of a request, including the LINK premium.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 55e22917-c95c-42f6-aace-044b68b0f37d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:01:04,322 - INFO - > [get_response] Total LLM token usage: 1874 tokens\n",
      "2023-05-30 12:01:04,324 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:01:04,327 - INFO - > [get_response] Total LLM token usage: 1874 tokens\n",
      "2023-05-30 12:01:04,330 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:01:04,334 - INFO - > Generated summary for doc 55e22917-c95c-42f6-aace-044b68b0f37d: \n",
      "This document provides an overview of security considerations for using Chainlink's VRF service to gain access to high quality randomness on-chain. It explains how to use the requestId to match randomness requests with their fulfillment, how to choose a safe block confirmation time, why re-requesting randomness should be avoided, why bids/bets/inputs should not be accepted after a randomness request, why the fulfillRandomWords function must not revert, and how to use VRFConsumerBaseV2 and VRFv2WrapperConsumer.sol in your contract to interact with the VRF service. This document can answer questions about how to use the VRF service securely and how to protect against potential manipulation of randomness generation.\n",
      "2023-05-30 12:01:04,418 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 9c457722-a247-45a3-84ca-3b7b6cc3db6a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:01:45,746 - INFO - > [get_response] Total LLM token usage: 433 tokens\n",
      "2023-05-30 12:01:45,748 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:01:45,749 - INFO - > [get_response] Total LLM token usage: 4455 tokens\n",
      "2023-05-30 12:01:45,750 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:01:45,752 - INFO - > Generated summary for doc 9c457722-a247-45a3-84ca-3b7b6cc3db6a: \n",
      "This document provides information about the Ethereum Mainnet VRF v2 coordinator contract and how to use the subscription method to generate random numbers. It covers topics such as setting up a subscription account, registering a smart contract with a subscription account, and the request and receive data cycle. It also outlines the limits of the subscription method, such as the minimum subscription balance, the maximum consuming contracts, and the coordinator contract limits. This document can answer questions about how to use the subscription method to generate random numbers, how to set up a subscription account, what the limits of the subscription method are, and the parameters of the Ethereum Mainnet VRF v2 coordinator contract, as well as provide resources for getting a random number and supported networks.\n",
      "2023-05-30 12:01:45,843 - INFO - > Building index from nodes: 2 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: e582b881-daa4-4e6e-b5da-eed505dbdfac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:02:17,108 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:02:17,109 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:02:56,193 - INFO - > [get_response] Total LLM token usage: 635 tokens\n",
      "2023-05-30 12:02:56,195 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:02:56,196 - INFO - > [get_response] Total LLM token usage: 8530 tokens\n",
      "2023-05-30 12:02:56,198 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:02:56,199 - INFO - > Generated summary for doc e582b881-daa4-4e6e-b5da-eed505dbdfac: \n",
      "This document provides a guide on how to use the Chainlink VRF v2 Subscription to request random numbers. It explains the parameters that must be set for the request, the functions available to use in the contract, security considerations, and instructions on how to cancel a subscription. It also provides an example of how to use Chainlink's Verifiable Random Function (VRF) to generate a random number, including instructions on how to use the Subscription Manager UI, security considerations, best practices, and supported networks. This document can answer questions about how to create and fund a subscription, create and deploy a VRF v2 compatible contract, request random values, analyze the contract, and clean up, as well as what parameters must be set for the request, what functions are available to use in the contract, what security considerations to keep in mind, what best practices to follow, what networks are supported, and how to migrate from VRF v1 to v2.\n",
      "2023-05-30 12:02:56,254 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: e38f6d3e-4be9-4931-99b5-1a9e39407c8c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:03:06,900 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:03:06,903 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:03:11,231 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:03:11,233 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:03:15,551 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:03:15,553 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:03:19,886 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:03:19,887 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:03:28,215 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:03:28,216 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:03:47,996 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:03:47,998 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:04:07,450 - INFO - > [get_response] Total LLM token usage: 383 tokens\n",
      "2023-05-30 12:04:07,451 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:04:07,454 - INFO - > [get_response] Total LLM token usage: 6382 tokens\n",
      "2023-05-30 12:04:07,456 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:04:07,458 - INFO - > Generated summary for doc e38f6d3e-4be9-4931-99b5-1a9e39407c8c: \n",
      "This document provides an example of how to use the Chainlink VRF v2 Subscription Manager contract to programmatically manage a subscription. It explains how to compile and deploy the contract, fund the subscription, add and remove consumer contracts, and cancel the subscription. It also explains how to fund and request randomness in a single transaction. This document can answer questions about how to create a contract that can change subscription configurations, how to request random numbers without a subscription, how to manage a subscription, how to use the VRF v2 Subscription Manager contract, how to fund a subscription, how to add and remove consumer contracts, and how to cancel a subscription.\n",
      "2023-05-30 12:04:07,505 - INFO - > Building index from nodes: 1 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: ec9e3be0-29e5-4dbc-9679-e4b31222dfa6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:04:29,857 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:04:29,858 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:04:34,175 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:04:34,177 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:05:05,641 - INFO - > [get_response] Total LLM token usage: 541 tokens\n",
      "2023-05-30 12:05:05,642 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:05:05,643 - INFO - > [get_response] Total LLM token usage: 6602 tokens\n",
      "2023-05-30 12:05:05,643 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:05:05,644 - INFO - > Generated summary for doc ec9e3be0-29e5-4dbc-9679-e4b31222dfa6: \n",
      "This document provides a step-by-step guide on how to test a VRF v2 consumer contract on a local blockchain environment using RemixIDE. It explains the benefits of local testing, the testing logic, and how to open the contracts on RemixIDE. It also provides security considerations and encourages thorough testing on public testnets. The document explains how to deploy the VRFCoordinatorV2Mock, create and fund a subscription, deploy the VRF consumer contract, add the consumer contract to the subscription, request random words, fulfill the VRF request, and check the results. This document can answer questions about how to test a VRF v2 consumer contract on a local blockchain environment, how to deploy the necessary contracts, and how to create and fund a subscription.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 4ed74bcc-8090-44c1-be36-e799ceff22f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:05:20,008 - INFO - > [get_response] Total LLM token usage: 1811 tokens\n",
      "2023-05-30 12:05:20,009 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:05:20,009 - INFO - > [get_response] Total LLM token usage: 1811 tokens\n",
      "2023-05-30 12:05:20,010 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:05:20,010 - INFO - > Generated summary for doc 4ed74bcc-8090-44c1-be36-e799ceff22f4: \n",
      "This document provides an overview of the Chainlink VRF v2 subscription method and how to migrate from VRF v1. It explains the features of VRF v2, such as the Subscription Manager, variable callback gas limit, more configuration capability, multiple random outputs in a single request, and unified billing. It also provides instructions on how to update existing smart contract code to work with VRF v2. This document can answer questions about the features of VRF v2, how to use the Subscription Manager, and how to update existing smart contract code to work with VRF v2.\n",
      "2023-05-30 12:05:20,098 - INFO - > Building index from nodes: 3 chunks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: f4967b9c-cfdf-4279-a52c-d6f7761afa71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:05:20,486 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:05:20,487 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:05:24,853 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:05:24,854 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:06:41,956 - INFO - > [get_response] Total LLM token usage: 754 tokens\n",
      "2023-05-30 12:06:41,957 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:06:41,958 - INFO - > [get_response] Total LLM token usage: 15625 tokens\n",
      "2023-05-30 12:06:41,958 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:06:41,960 - INFO - > Generated summary for doc f4967b9c-cfdf-4279-a52c-d6f7761afa71: \n",
      "This document provides information about Chainlink VRF v2, a service that allows users to integrate provably fair and verifiably random data into their smart contracts. It outlines the coordinator parameters, fee parameters, and configurations for the Ethereum mainnet, Sepolia testnet, Goerli testnet, BNB Chain, BNB Chain testnet, Polygon (Matic) mainnet, Polygon (Matic) Mumbai testnet, Avalanche mainnet, Avalanche Fuji testnet, Fantom mainnet, Fantom testnet, Arbitrum mainnet, and Arbitrum Goerli testnet. It also provides security considerations and links to other resources. This document can answer questions about the LINK token, VRF coordinator, key hashes, premiums, max gas limits, minimum and maximum confirmations, and maximum random values for each of the supported networks, as well as questions about the risks associated with bridges and how to request random numbers without a subscription.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 46229450-3ed5-4fc0-bf0a-081aae98bc6a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:06:42,363 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:06:42,364 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:06:55,308 - INFO - > [get_response] Total LLM token usage: 2082 tokens\n",
      "2023-05-30 12:06:55,309 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:06:55,309 - INFO - > [get_response] Total LLM token usage: 2082 tokens\n",
      "2023-05-30 12:06:55,310 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:06:55,311 - INFO - > Generated summary for doc 46229450-3ed5-4fc0-bf0a-081aae98bc6a: \n",
      "This document provides an overview of the VRF v2 Subscription Manager user interface. It explains the different sections of the UI, such as the Actions menu, Consumers, Pending, History, Recent fulfillments, Events, and Failed requests. It also provides security considerations and troubleshooting tips. This document can answer questions about how to fund a subscription, add or remove consumers, view pending requests, view recent fulfillments, view events, and view failed requests.\n",
      "2023-05-30 12:06:55,317 - INFO - > [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:06:55,318 - INFO - > [build_index_from_nodes] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "response_synthesizer = ResponseSynthesizer.from_args(response_mode=\"tree_summarize\")\n",
    "doc_summary_index = GPTDocumentSummaryIndex.from_documents(\n",
    "    documents=documents, \n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:33:18,181 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:33:18,182 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 12:33:19,541 - INFO - > [get_response] Total LLM token usage: 1198 tokens\n",
      "2023-05-30 12:33:19,541 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:19,542 - INFO - > [get_response] Total LLM token usage: 1198 tokens\n",
      "2023-05-30 12:33:19,542 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:20,584 - INFO - > [get_response] Total LLM token usage: 820 tokens\n",
      "2023-05-30 12:33:20,585 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:20,586 - INFO - > [get_response] Total LLM token usage: 820 tokens\n",
      "2023-05-30 12:33:20,586 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:21,073 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:33:21,074 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:33:50,063 - INFO - > [get_response] Total LLM token usage: 2063 tokens\n",
      "2023-05-30 12:33:50,064 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:50,064 - INFO - > [get_response] Total LLM token usage: 2063 tokens\n",
      "2023-05-30 12:33:50,065 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:50,980 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:33:50,981 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 12:33:51,398 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:33:51,399 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:33:56,572 - INFO - > [get_response] Total LLM token usage: 1168 tokens\n",
      "2023-05-30 12:33:56,574 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:56,576 - INFO - > [get_response] Total LLM token usage: 1168 tokens\n",
      "2023-05-30 12:33:56,579 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:57,689 - INFO - > [get_response] Total LLM token usage: 1182 tokens\n",
      "2023-05-30 12:33:57,692 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:33:57,694 - INFO - > [get_response] Total LLM token usage: 1182 tokens\n",
      "2023-05-30 12:33:57,695 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:34:23,350 - INFO - > [get_response] Total LLM token usage: 2332 tokens\n",
      "2023-05-30 12:34:23,351 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:34:23,351 - INFO - > [get_response] Total LLM token usage: 2332 tokens\n",
      "2023-05-30 12:34:23,351 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:34:24,033 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:34:24,035 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 12:34:25,740 - INFO - > [get_response] Total LLM token usage: 1205 tokens\n",
      "2023-05-30 12:34:25,742 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:34:25,744 - INFO - > [get_response] Total LLM token usage: 1205 tokens\n",
      "2023-05-30 12:34:25,745 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:34:27,021 - INFO - > [get_response] Total LLM token usage: 812 tokens\n",
      "2023-05-30 12:34:27,022 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:34:27,023 - INFO - > [get_response] Total LLM token usage: 812 tokens\n",
      "2023-05-30 12:34:27,023 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:34:27,409 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:34:27,411 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:34:31,754 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:34:31,755 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:34:36,088 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:34:36,090 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:35:08,810 - INFO - > [get_response] Total LLM token usage: 1994 tokens\n",
      "2023-05-30 12:35:08,810 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:08,811 - INFO - > [get_response] Total LLM token usage: 1994 tokens\n",
      "2023-05-30 12:35:08,812 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:09,398 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:35:09,399 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 12:35:10,619 - INFO - > [get_response] Total LLM token usage: 1196 tokens\n",
      "2023-05-30 12:35:10,619 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:10,620 - INFO - > [get_response] Total LLM token usage: 1196 tokens\n",
      "2023-05-30 12:35:10,620 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:10,955 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:35:10,957 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:35:15,944 - INFO - > [get_response] Total LLM token usage: 478 tokens\n",
      "2023-05-30 12:35:15,945 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:15,946 - INFO - > [get_response] Total LLM token usage: 478 tokens\n",
      "2023-05-30 12:35:15,947 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:16,271 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:35:16,272 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:35:46,345 - INFO - > [get_response] Total LLM token usage: 1673 tokens\n",
      "2023-05-30 12:35:46,346 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:46,346 - INFO - > [get_response] Total LLM token usage: 1673 tokens\n",
      "2023-05-30 12:35:46,346 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:46,915 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:35:46,917 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 12:35:48,875 - INFO - > [get_response] Total LLM token usage: 1259 tokens\n",
      "2023-05-30 12:35:48,876 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:48,876 - INFO - > [get_response] Total LLM token usage: 1259 tokens\n",
      "2023-05-30 12:35:48,877 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:50,513 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 12:35:50,514 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:50,514 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 12:35:50,515 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:35:50,850 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:35:50,851 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:36:19,050 - INFO - > [get_response] Total LLM token usage: 2463 tokens\n",
      "2023-05-30 12:36:19,052 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:36:19,053 - INFO - > [get_response] Total LLM token usage: 2463 tokens\n",
      "2023-05-30 12:36:19,054 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:36:19,792 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:36:19,793 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 12:36:21,089 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 12:36:21,091 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:36:21,093 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 12:36:21,103 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:36:21,560 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:36:21,561 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:36:26,870 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 12:36:26,871 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:36:26,872 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 12:36:26,872 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:36:27,304 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:36:27,304 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:36:31,647 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:36:31,649 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:37:02,772 - INFO - > [get_response] Total LLM token usage: 2431 tokens\n",
      "2023-05-30 12:37:02,773 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:02,775 - INFO - > [get_response] Total LLM token usage: 2431 tokens\n",
      "2023-05-30 12:37:02,775 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:03,605 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:37:03,606 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 12:37:03,971 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:37:03,973 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:37:08,359 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:37:08,361 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:37:14,077 - INFO - > [get_response] Total LLM token usage: 816 tokens\n",
      "2023-05-30 12:37:14,078 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:14,080 - INFO - > [get_response] Total LLM token usage: 816 tokens\n",
      "2023-05-30 12:37:14,080 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:15,241 - INFO - > [get_response] Total LLM token usage: 1240 tokens\n",
      "2023-05-30 12:37:15,242 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:15,243 - INFO - > [get_response] Total LLM token usage: 1240 tokens\n",
      "2023-05-30 12:37:15,244 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:42,684 - INFO - > [get_response] Total LLM token usage: 2052 tokens\n",
      "2023-05-30 12:37:42,685 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:42,686 - INFO - > [get_response] Total LLM token usage: 2052 tokens\n",
      "2023-05-30 12:37:42,687 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:43,278 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:37:43,279 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 12:37:44,819 - INFO - > [get_response] Total LLM token usage: 1197 tokens\n",
      "2023-05-30 12:37:44,820 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:44,820 - INFO - > [get_response] Total LLM token usage: 1197 tokens\n",
      "2023-05-30 12:37:44,820 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:45,134 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:37:45,135 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:37:50,001 - INFO - > [get_response] Total LLM token usage: 1120 tokens\n",
      "2023-05-30 12:37:50,002 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:50,002 - INFO - > [get_response] Total LLM token usage: 1120 tokens\n",
      "2023-05-30 12:37:50,002 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:55,918 - INFO - > [get_response] Total LLM token usage: 2035 tokens\n",
      "2023-05-30 12:37:55,919 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:55,919 - INFO - > [get_response] Total LLM token usage: 2035 tokens\n",
      "2023-05-30 12:37:55,920 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:37:56,375 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:37:56,376 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 12:37:56,751 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:37:56,752 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:38:01,092 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:38:01,094 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:38:09,090 - INFO - > [get_response] Total LLM token usage: 1273 tokens\n",
      "2023-05-30 12:38:09,091 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:09,092 - INFO - > [get_response] Total LLM token usage: 1273 tokens\n",
      "2023-05-30 12:38:09,093 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:09,095 - ERROR - Invalid prediction:  0x72AFAECF99C9d9C8215fF44C77B94B99C28741e8\n",
      "2023-05-30 12:38:09,880 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:38:09,881 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 12:38:10,219 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:38:10,220 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:38:17,219 - INFO - > [get_response] Total LLM token usage: 1281 tokens\n",
      "2023-05-30 12:38:17,220 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:17,221 - INFO - > [get_response] Total LLM token usage: 1281 tokens\n",
      "2023-05-30 12:38:17,221 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:17,222 - ERROR - Invalid prediction: \n",
      "0x5586bF404C7A22A4a4077401272cE5945f80189C\n",
      "2023-05-30 12:38:17,962 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:38:17,962 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 12:38:18,327 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:38:18,328 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:38:23,401 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 12:38:23,402 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:23,403 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 12:38:23,404 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:24,558 - INFO - > [get_response] Total LLM token usage: 1075 tokens\n",
      "2023-05-30 12:38:24,560 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:24,562 - INFO - > [get_response] Total LLM token usage: 1075 tokens\n",
      "2023-05-30 12:38:24,563 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:28,845 - INFO - > [get_response] Total LLM token usage: 1891 tokens\n",
      "2023-05-30 12:38:28,846 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:28,846 - INFO - > [get_response] Total LLM token usage: 1891 tokens\n",
      "2023-05-30 12:38:28,847 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:29,406 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:38:29,406 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 12:38:30,370 - INFO - > [get_response] Total LLM token usage: 1255 tokens\n",
      "2023-05-30 12:38:30,371 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:30,372 - INFO - > [get_response] Total LLM token usage: 1255 tokens\n",
      "2023-05-30 12:38:30,372 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:30,709 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:38:30,710 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:38:35,633 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 12:38:35,635 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:35,635 - INFO - > [get_response] Total LLM token usage: 1218 tokens\n",
      "2023-05-30 12:38:35,636 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:36,018 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:38:36,019 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:38:40,340 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:38:40,341 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:38:48,121 - INFO - > [get_response] Total LLM token usage: 2152 tokens\n",
      "2023-05-30 12:38:48,122 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:48,122 - INFO - > [get_response] Total LLM token usage: 2152 tokens\n",
      "2023-05-30 12:38:48,124 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:48,734 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:38:48,735 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 12:38:49,609 - INFO - > [get_response] Total LLM token usage: 567 tokens\n",
      "2023-05-30 12:38:49,610 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:49,610 - INFO - > [get_response] Total LLM token usage: 567 tokens\n",
      "2023-05-30 12:38:49,611 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:50,474 - INFO - > [get_response] Total LLM token usage: 456 tokens\n",
      "2023-05-30 12:38:50,475 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:50,476 - INFO - > [get_response] Total LLM token usage: 456 tokens\n",
      "2023-05-30 12:38:50,476 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:58,344 - INFO - > [get_response] Total LLM token usage: 752 tokens\n",
      "2023-05-30 12:38:58,345 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:58,346 - INFO - > [get_response] Total LLM token usage: 752 tokens\n",
      "2023-05-30 12:38:58,347 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:38:59,187 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:38:59,188 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 12:38:59,521 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:38:59,522 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:39:03,850 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:39:03,851 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:39:08,192 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:39:08,193 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:39:12,519 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:39:12,520 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:39:21,444 - INFO - > [get_response] Total LLM token usage: 708 tokens\n",
      "2023-05-30 12:39:21,445 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:21,446 - INFO - > [get_response] Total LLM token usage: 708 tokens\n",
      "2023-05-30 12:39:21,446 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:22,652 - INFO - > [get_response] Total LLM token usage: 1220 tokens\n",
      "2023-05-30 12:39:22,653 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:22,653 - INFO - > [get_response] Total LLM token usage: 1220 tokens\n",
      "2023-05-30 12:39:22,654 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:22,997 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:39:22,998 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:39:37,884 - INFO - > [get_response] Total LLM token usage: 1676 tokens\n",
      "2023-05-30 12:39:37,885 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:37,886 - INFO - > [get_response] Total LLM token usage: 1676 tokens\n",
      "2023-05-30 12:39:37,886 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:38,444 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:39:38,446 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 12:39:39,706 - INFO - > [get_response] Total LLM token usage: 1100 tokens\n",
      "2023-05-30 12:39:39,707 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:39,707 - INFO - > [get_response] Total LLM token usage: 1100 tokens\n",
      "2023-05-30 12:39:39,708 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:40,918 - INFO - > [get_response] Total LLM token usage: 541 tokens\n",
      "2023-05-30 12:39:40,918 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:40,919 - INFO - > [get_response] Total LLM token usage: 541 tokens\n",
      "2023-05-30 12:39:40,919 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:41,242 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:39:41,243 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:39:52,897 - INFO - > [get_response] Total LLM token usage: 1354 tokens\n",
      "2023-05-30 12:39:52,897 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:52,898 - INFO - > [get_response] Total LLM token usage: 1354 tokens\n",
      "2023-05-30 12:39:52,898 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:53,715 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:39:53,716 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 12:39:54,876 - INFO - > [get_response] Total LLM token usage: 514 tokens\n",
      "2023-05-30 12:39:54,876 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:54,877 - INFO - > [get_response] Total LLM token usage: 514 tokens\n",
      "2023-05-30 12:39:54,878 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:56,951 - INFO - > [get_response] Total LLM token usage: 1193 tokens\n",
      "2023-05-30 12:39:56,952 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:39:56,953 - INFO - > [get_response] Total LLM token usage: 1193 tokens\n",
      "2023-05-30 12:39:56,953 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:40:09,023 - INFO - > [get_response] Total LLM token usage: 1473 tokens\n",
      "2023-05-30 12:40:09,024 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:40:09,025 - INFO - > [get_response] Total LLM token usage: 1473 tokens\n",
      "2023-05-30 12:40:09,025 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:40:09,588 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:40:09,588 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 12:40:09,918 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:09,919 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:40:14,346 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:14,347 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:40:18,665 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:18,666 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:40:24,021 - INFO - > [get_response] Total LLM token usage: 1222 tokens\n",
      "2023-05-30 12:40:24,022 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:40:24,022 - INFO - > [get_response] Total LLM token usage: 1222 tokens\n",
      "2023-05-30 12:40:24,022 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:40:24,340 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:24,341 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:40:28,649 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:28,650 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:40:32,971 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:32,972 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:40:37,647 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:37,648 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:40:46,634 - INFO - > [get_response] Total LLM token usage: 1271 tokens\n",
      "2023-05-30 12:40:46,635 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:40:46,636 - INFO - > [get_response] Total LLM token usage: 1271 tokens\n",
      "2023-05-30 12:40:46,636 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:40:47,000 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:40:47,001 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:41:02,763 - INFO - > [get_response] Total LLM token usage: 2207 tokens\n",
      "2023-05-30 12:41:02,764 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:02,764 - INFO - > [get_response] Total LLM token usage: 2207 tokens\n",
      "2023-05-30 12:41:02,765 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:03,411 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:41:03,412 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 12:41:04,274 - INFO - > [get_response] Total LLM token usage: 643 tokens\n",
      "2023-05-30 12:41:04,275 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:04,276 - INFO - > [get_response] Total LLM token usage: 643 tokens\n",
      "2023-05-30 12:41:04,276 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:04,634 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:41:04,635 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:41:09,471 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 12:41:09,472 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:09,472 - INFO - > [get_response] Total LLM token usage: 1227 tokens\n",
      "2023-05-30 12:41:09,473 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:15,282 - INFO - > [get_response] Total LLM token usage: 1580 tokens\n",
      "2023-05-30 12:41:15,283 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:15,283 - INFO - > [get_response] Total LLM token usage: 1580 tokens\n",
      "2023-05-30 12:41:15,283 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:41:15,874 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:41:15,875 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 12:41:16,475 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:41:16,476 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:51:20,583 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600).\n",
      "2023-05-30 12:51:25,641 - INFO - > [get_response] Total LLM token usage: 1098 tokens\n",
      "2023-05-30 12:51:25,641 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:25,642 - INFO - > [get_response] Total LLM token usage: 1098 tokens\n",
      "2023-05-30 12:51:25,642 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:27,012 - INFO - > [get_response] Total LLM token usage: 1155 tokens\n",
      "2023-05-30 12:51:27,013 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:27,014 - INFO - > [get_response] Total LLM token usage: 1155 tokens\n",
      "2023-05-30 12:51:27,014 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:37,122 - INFO - > [get_response] Total LLM token usage: 2028 tokens\n",
      "2023-05-30 12:51:37,123 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:37,124 - INFO - > [get_response] Total LLM token usage: 2028 tokens\n",
      "2023-05-30 12:51:37,124 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:37,784 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:51:37,785 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 12:51:39,191 - INFO - > [get_response] Total LLM token usage: 1158 tokens\n",
      "2023-05-30 12:51:39,192 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:39,192 - INFO - > [get_response] Total LLM token usage: 1158 tokens\n",
      "2023-05-30 12:51:39,193 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:40,273 - INFO - > [get_response] Total LLM token usage: 1212 tokens\n",
      "2023-05-30 12:51:40,273 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:40,274 - INFO - > [get_response] Total LLM token usage: 1212 tokens\n",
      "2023-05-30 12:51:40,275 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:55,364 - INFO - > [get_response] Total LLM token usage: 2190 tokens\n",
      "2023-05-30 12:51:55,364 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:55,365 - INFO - > [get_response] Total LLM token usage: 2190 tokens\n",
      "2023-05-30 12:51:55,365 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:51:55,912 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:51:55,912 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 12:51:56,241 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:51:56,242 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:52:00,611 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:52:00,612 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:52:04,958 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:52:04,958 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:52:11,619 - INFO - > [get_response] Total LLM token usage: 1147 tokens\n",
      "2023-05-30 12:52:11,620 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:11,621 - INFO - > [get_response] Total LLM token usage: 1147 tokens\n",
      "2023-05-30 12:52:11,621 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:12,584 - INFO - > [get_response] Total LLM token usage: 458 tokens\n",
      "2023-05-30 12:52:12,585 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:12,586 - INFO - > [get_response] Total LLM token usage: 458 tokens\n",
      "2023-05-30 12:52:12,586 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:20,239 - INFO - > [get_response] Total LLM token usage: 1340 tokens\n",
      "2023-05-30 12:52:20,239 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:20,240 - INFO - > [get_response] Total LLM token usage: 1340 tokens\n",
      "2023-05-30 12:52:20,241 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:20,798 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:52:20,798 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 12:52:21,885 - INFO - > [get_response] Total LLM token usage: 842 tokens\n",
      "2023-05-30 12:52:21,886 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:21,887 - INFO - > [get_response] Total LLM token usage: 842 tokens\n",
      "2023-05-30 12:52:21,887 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:22,973 - INFO - > [get_response] Total LLM token usage: 1228 tokens\n",
      "2023-05-30 12:52:22,974 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:22,975 - INFO - > [get_response] Total LLM token usage: 1228 tokens\n",
      "2023-05-30 12:52:22,976 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:23,028 - INFO - > Building index from nodes: 1 chunks\n",
      "2023-05-30 12:52:35,616 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:52:35,617 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:52:54,107 - INFO - > [get_response] Total LLM token usage: 260 tokens\n",
      "2023-05-30 12:52:54,108 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:54,109 - INFO - > [get_response] Total LLM token usage: 6365 tokens\n",
      "2023-05-30 12:52:54,110 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:54,694 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:52:54,694 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 12:52:55,921 - INFO - > [get_response] Total LLM token usage: 808 tokens\n",
      "2023-05-30 12:52:55,922 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:55,922 - INFO - > [get_response] Total LLM token usage: 808 tokens\n",
      "2023-05-30 12:52:55,923 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:56,908 - INFO - > [get_response] Total LLM token usage: 1159 tokens\n",
      "2023-05-30 12:52:56,909 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:56,910 - INFO - > [get_response] Total LLM token usage: 1159 tokens\n",
      "2023-05-30 12:52:56,910 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:52:57,240 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:52:57,241 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:53:01,542 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:53:01,543 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:53:15,906 - INFO - > [get_response] Total LLM token usage: 1743 tokens\n",
      "2023-05-30 12:53:15,907 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:15,908 - INFO - > [get_response] Total LLM token usage: 1743 tokens\n",
      "2023-05-30 12:53:15,908 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:16,529 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:53:16,530 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 12:53:17,564 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 12:53:17,564 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:17,565 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 12:53:17,565 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:17,882 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:53:17,885 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:53:22,931 - INFO - > [get_response] Total LLM token usage: 677 tokens\n",
      "2023-05-30 12:53:22,932 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:22,932 - INFO - > [get_response] Total LLM token usage: 677 tokens\n",
      "2023-05-30 12:53:22,933 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:23,279 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:53:23,280 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:53:32,337 - INFO - > [get_response] Total LLM token usage: 1501 tokens\n",
      "2023-05-30 12:53:32,338 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:32,339 - INFO - > [get_response] Total LLM token usage: 1501 tokens\n",
      "2023-05-30 12:53:32,340 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:32,915 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:53:32,916 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 12:53:34,296 - INFO - > [get_response] Total LLM token usage: 1201 tokens\n",
      "2023-05-30 12:53:34,296 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:34,297 - INFO - > [get_response] Total LLM token usage: 1201 tokens\n",
      "2023-05-30 12:53:34,297 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:34,645 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:53:34,646 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:53:39,611 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 12:53:39,612 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:39,612 - INFO - > [get_response] Total LLM token usage: 1203 tokens\n",
      "2023-05-30 12:53:39,613 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:51,798 - INFO - > [get_response] Total LLM token usage: 2173 tokens\n",
      "2023-05-30 12:53:51,799 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:51,800 - INFO - > [get_response] Total LLM token usage: 2173 tokens\n",
      "2023-05-30 12:53:51,800 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:52,389 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:53:52,391 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 12:53:53,668 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 12:53:53,668 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:53,669 - INFO - > [get_response] Total LLM token usage: 1135 tokens\n",
      "2023-05-30 12:53:53,669 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:53:54,013 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:53:54,014 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:53:58,316 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:53:58,317 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:54:02,632 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:54:02,633 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:54:07,685 - INFO - > [get_response] Total LLM token usage: 547 tokens\n",
      "2023-05-30 12:54:07,686 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:07,687 - INFO - > [get_response] Total LLM token usage: 547 tokens\n",
      "2023-05-30 12:54:07,687 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:21,696 - INFO - > [get_response] Total LLM token usage: 1467 tokens\n",
      "2023-05-30 12:54:21,697 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:21,698 - INFO - > [get_response] Total LLM token usage: 1467 tokens\n",
      "2023-05-30 12:54:21,698 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:22,342 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 12:54:22,343 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 12:54:23,535 - INFO - > [get_response] Total LLM token usage: 1131 tokens\n",
      "2023-05-30 12:54:23,536 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:23,536 - INFO - > [get_response] Total LLM token usage: 1131 tokens\n",
      "2023-05-30 12:54:23,537 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:24,828 - INFO - > [get_response] Total LLM token usage: 1101 tokens\n",
      "2023-05-30 12:54:24,829 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:24,829 - INFO - > [get_response] Total LLM token usage: 1101 tokens\n",
      "2023-05-30 12:54:24,830 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:25,612 - INFO - error_code=None error_message='The server had an error while processing your request. Sorry about that!' error_param=None error_type=server_error message='OpenAI API error received' stream_error=False\n",
      "2023-05-30 12:54:25,612 - WARNING - Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "2023-05-30 12:54:39,549 - INFO - > [get_response] Total LLM token usage: 2001 tokens\n",
      "2023-05-30 12:54:39,549 - INFO - > [get_response] Total embedding token usage: 0 tokens\n",
      "2023-05-30 12:54:39,550 - INFO - > [get_response] Total LLM token usage: 2001 tokens\n",
      "2023-05-30 12:54:39,550 - INFO - > [get_response] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "df12 = df.copy()\n",
    "\n",
    "for i, gt in df12.iterrows():\n",
    "    try:\n",
    "        df12.loc[i, \"result\"] = query_engine.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df12.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd71bc6a832545aa8cf091e0ed642129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7ff18de53c423cb2be95ff19c6f547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e96d9aaa3840e8928a677d2d350739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e797824ca145f1bc40c165598cbc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_answer(df12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 13:03:52,761 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:52,762 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 13:03:53,533 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:53,534 - INFO - > [retrieve] Total embedding token usage: 14 tokens\n",
      "2023-05-30 13:03:54,231 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:54,232 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 13:03:54,797 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:54,798 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 13:03:55,359 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:55,360 - INFO - > [retrieve] Total embedding token usage: 27 tokens\n",
      "2023-05-30 13:03:55,887 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:55,889 - INFO - > [retrieve] Total embedding token usage: 24 tokens\n",
      "2023-05-30 13:03:56,355 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:56,356 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 13:03:57,989 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:57,990 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 13:03:58,560 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:58,561 - INFO - > [retrieve] Total embedding token usage: 15 tokens\n",
      "2023-05-30 13:03:59,249 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:03:59,250 - INFO - > [retrieve] Total embedding token usage: 20 tokens\n",
      "2023-05-30 13:04:00,063 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:00,064 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 13:04:00,684 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:00,685 - INFO - > [retrieve] Total embedding token usage: 26 tokens\n",
      "2023-05-30 13:04:01,204 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:01,205 - INFO - > [retrieve] Total embedding token usage: 5 tokens\n",
      "2023-05-30 13:04:01,843 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:01,844 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 13:04:03,095 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:03,096 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 13:04:03,695 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:03,696 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 13:04:04,413 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:04,414 - INFO - > [retrieve] Total embedding token usage: 17 tokens\n",
      "2023-05-30 13:04:04,885 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:04,886 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 13:04:05,586 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:05,587 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n",
      "2023-05-30 13:04:06,247 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:06,248 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 13:04:06,848 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:06,850 - INFO - > [retrieve] Total embedding token usage: 7 tokens\n",
      "2023-05-30 13:04:07,657 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:07,657 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 13:04:08,431 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:08,432 - INFO - > [retrieve] Total embedding token usage: 10 tokens\n",
      "2023-05-30 13:04:09,026 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:09,027 - INFO - > [retrieve] Total embedding token usage: 16 tokens\n",
      "2023-05-30 13:04:09,602 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:09,603 - INFO - > [retrieve] Total embedding token usage: 11 tokens\n",
      "2023-05-30 13:04:10,198 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:10,199 - INFO - > [retrieve] Total embedding token usage: 12 tokens\n",
      "2023-05-30 13:04:10,778 - INFO - > [retrieve] Total LLM token usage: 0 tokens\n",
      "2023-05-30 13:04:10,780 - INFO - > [retrieve] Total embedding token usage: 8 tokens\n"
     ]
    }
   ],
   "source": [
    "retrieve_docs(df12, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba21632f87694267b23b3f8753ead6c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df884bada663471e87d917ea9784c66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Right', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4296896f2d714f9ca49746acc902a864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Wrong', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df26192ba22e4bf8bbb4bd50f7cd0668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Partial', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_dataframe_retrieval(df12, query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df12.to_csv(\"exp_12.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
