{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/marshath/play/chainlink/algovate/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import GPTVectorStoreIndex, ResponseSynthesizer\n",
    "from llama_index.data_structs.node import Node, DocumentRelationship\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index import LLMPredictor, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
    "from llama_index import GPTListIndex\n",
    "from llama_index.indices.list.retrievers import ListIndexLLMRetriever\n",
    "from llama_index.indices.postprocessor import (\n",
    "    LLMRerank\n",
    ")\n",
    "from llama_index.indices.postprocessor.node import AutoPrevNextNodePostprocessor\n",
    "\n",
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.indices.postprocessor.node import PrevNextNodePostprocessor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/marshath/play/chainlink/algovate/algovate/data/combined_documents.pkl', 'rb') as f:\n",
    "    cl_combineddoc = pickle.load(f)\n",
    "\n",
    "# convert to llama documents\n",
    "llama_cl_combineddoc = []\n",
    "for doc in cl_combineddoc:\n",
    "    llama_cl_combineddoc.append(Document(text=doc.page_content, extra_info=doc.metadata))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/marshath/play/chainlink/algovate/algovate/data/more-chainlink-tests.csv\", header=None)\n",
    "df.columns = [\"question\", \"source\"]\n",
    "df[\"answer\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_question_index = 0\n",
    "\n",
    "# # Create text box and display the first question\n",
    "# answer_box = widgets.Textarea(value=df['answer'][current_question_index], placeholder='Type your answer here', rows=3)\n",
    "# question_display = widgets.HTML(value=f\"<h4>Question {current_question_index + 1}: {df['question'][current_question_index]}</h4>\")\n",
    "# display(question_display)\n",
    "# display(answer_box)\n",
    "\n",
    "# # Function to update the DataFrame with the answer and display the next question\n",
    "# def update_answer(btn):\n",
    "#     global current_question_index\n",
    "#     df.at[current_question_index, 'answer'] = answer_box.value  # Update the answer in the DataFrame\n",
    "#     current_question_index += 1  # Move to the next question\n",
    "\n",
    "#     if current_question_index < len(df):\n",
    "#         question_display.value = f\"<h4>Question {current_question_index + 1}: {df['question'][current_question_index]}</h4>\"\n",
    "#         answer_box.value = df['answer'][current_question_index]\n",
    "#     else:\n",
    "#         next_button.disabled = True\n",
    "#         next_button.description = 'Answers saved!'\n",
    "\n",
    "# # Create a button to save the answers or move to the next question\n",
    "# next_button = widgets.Button(description='Next Question')\n",
    "# next_button.on_click(update_answer)\n",
    "\n",
    "# # Display the next button\n",
    "# display(next_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/home/marshath/play/chainlink/algovate/algovate/data/more-chainlink-tests.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0,))\n",
    "\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_base_combineddoc = GPTVectorStoreIndex.from_documents(\n",
    "    llama_cl_combineddoc, \n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "query_engine_base_combineddoc = index_base_combineddoc.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 3642e33aaa1ed329a618814d8ed04776 in your message.).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     df2\u001b[39m.\u001b[39mloc[i, \u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m query_engine_base_combineddoc\u001b[39m.\u001b[39;49mquery(gt\u001b[39m.\u001b[39;49mquestion)\n\u001b[1;32m      5\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/chainlink/lib/python3.10/site-packages/llama_index/indices/query/base.py:18\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     17\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_query(str_or_query_bundle)\n",
      "File \u001b[0;32m~/anaconda3/envs/chainlink/lib/python3.10/site-packages/llama_index/query_engine/retriever_query_engine.py:139\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    138\u001b[0m retrieve_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_start(CBEventType\u001b[39m.\u001b[39mRETRIEVE)\n\u001b[0;32m--> 139\u001b[0m nodes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retriever\u001b[39m.\u001b[39;49mretrieve(query_bundle)\n\u001b[1;32m    140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_event_end(\n\u001b[1;32m    141\u001b[0m     CBEventType\u001b[39m.\u001b[39mRETRIEVE, payload\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mnodes\u001b[39m\u001b[39m\"\u001b[39m: nodes}, event_id\u001b[39m=\u001b[39mretrieve_id\n\u001b[1;32m    142\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/chainlink/lib/python3.10/site-packages/llama_index/indices/base_retriever.py:21\u001b[0m, in \u001b[0;36mBaseRetriever.retrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     20\u001b[0m     str_or_query_bundle \u001b[39m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 21\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve(str_or_query_bundle)\n",
      "File \u001b[0;32m~/anaconda3/envs/chainlink/lib/python3.10/site-packages/llama_index/token_counter/token_counter.py:78\u001b[0m, in \u001b[0;36mllm_token_counter.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self):\n\u001b[0;32m---> 78\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[39mreturn\u001b[39;00m f_return_val\n",
      "File \u001b[0;32m~/anaconda3/envs/chainlink/lib/python3.10/site-packages/llama_index/indices/vector_store/retrievers/retriever.py:68\u001b[0m, in \u001b[0;36mVectorIndexRetriever._retrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_store\u001b[39m.\u001b[39mis_embedding_query:\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mif\u001b[39;00m query_bundle\u001b[39m.\u001b[39;49membedding \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m         query_bundle\u001b[39m.\u001b[39membedding \u001b[39m=\u001b[39m (\n\u001b[1;32m     70\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_service_context\u001b[39m.\u001b[39membed_model\u001b[39m.\u001b[39mget_agg_embedding_from_queries(\n\u001b[1;32m     71\u001b[0m                 query_bundle\u001b[39m.\u001b[39membedding_strs\n\u001b[1;32m     72\u001b[0m             )\n\u001b[1;32m     73\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'embedding'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     df2\u001b[39m.\u001b[39mloc[i, \u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m query_engine_base_combineddoc\u001b[39m.\u001b[39mquery(gt\u001b[39m.\u001b[39mquestion)\n\u001b[1;32m      5\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m----> 6\u001b[0m     logger\u001b[39m.\u001b[39merror(e)\n\u001b[1;32m      7\u001b[0m     df2\u001b[39m.\u001b[39mloc[i, \u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "df2 = df.copy()\n",
    "for i, gt in df2.iterrows():\n",
    "    try:\n",
    "        df2.loc[i, \"result\"] = query_engine_base_combineddoc.query(gt.question)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        df2.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
