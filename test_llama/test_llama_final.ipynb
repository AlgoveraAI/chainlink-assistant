{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python-dotenv could not parse statement starting at line 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/home/marshath/play/chainlink/algovate/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index import GPTVectorStoreIndex, ResponseSynthesizer\n",
    "from llama_index.data_structs.node import Node, DocumentRelationship\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
    "from llama_index import LLMPredictor, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
    "from llama_index import GPTListIndex\n",
    "from llama_index.indices.list.retrievers import ListIndexLLMRetriever\n",
    "from llama_index.indices.postprocessor import (\n",
    "    LLMRerank\n",
    ")\n",
    "from llama_index.indices.postprocessor.node import AutoPrevNextNodePostprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from llama_index.indices.postprocessor.node import PrevNextNodePostprocessor\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.storage.storage_context import StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open('/home/marshath/play/chainlink/algovate/algovate/data/documents.pkl', 'rb') as f:\n",
    "    cl_techdoc = pickle.load(f)\n",
    "\n",
    "# convert to llama documents\n",
    "llama_cl_techdoc = []\n",
    "for doc in cl_techdoc:\n",
    "    llama_cl_techdoc.append(Document(text=doc.page_content, extra_info=doc.metadata))\n",
    "\n",
    "with open('/home/marshath/play/chainlink/algovate/algovate/data/combined_documents.pkl', 'rb') as f:\n",
    "    cl_combineddoc = pickle.load(f)\n",
    "\n",
    "# convert to llama documents\n",
    "llama_cl_combineddoc = []\n",
    "for doc in cl_combineddoc:\n",
    "    llama_cl_combineddoc.append(Document(text=doc.page_content, extra_info=doc.metadata))\n",
    "\n",
    "# load the ground truths\n",
    "with open(\"/home/marshath/play/chainlink/algovate/algovate/data/ground_truths.pkl\", \"rb\") as f:\n",
    "    ground_truths = pickle.load(f)\n",
    "\n",
    "df = pd.DataFrame(ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to nodes\n",
    "parser = SimpleNodeParser()\n",
    "cl_techdoc_nodes = parser.get_nodes_from_documents(llama_cl_techdoc)\n",
    "cl_combineddoc_nodes = parser.get_nodes_from_documents(llama_cl_combineddoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(df, query_engine):\n",
    "    # Add retrieved_docs column if it doesn't exist\n",
    "    df['retrieved_docs'] = df.get('retrieved_docs', '')\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        retrieved_docs = query_engine.retrieve(row['question'])\n",
    "\n",
    "        docs = \"\"\n",
    "        for d in retrieved_docs:\n",
    "            if docs == \"\":\n",
    "                docs = d.node.text\n",
    "            else:\n",
    "                docs = docs + \"\\n\\n:::NEXT DOC:::\\n\\n\" + d.node.text\n",
    "\n",
    "        # Store retrieved docs in the DataFrame\n",
    "        df.at[index, 'retrieved_docs'] = docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0,))\n",
    "\n",
    "max_input_size = 4096\n",
    "num_output = 256\n",
    "max_chunk_overlap = 20\n",
    "\n",
    "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### techdoc only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_base_techdoc = GPTVectorStoreIndex.from_documents(\n",
    "    llama_cl_techdoc, \n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "query_engine_base_techdoc = index_base_techdoc.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "for i, gt in df1.iterrows():\n",
    "    try:\n",
    "        df1.loc[i, \"result\"] = query_engine_base_techdoc.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df1.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df1, query_engine_base_techdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"/home/marshath/play/chainlink/algovate/test_llama/techdoc_base.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combineddocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_base_combineddoc = GPTVectorStoreIndex.from_documents(\n",
    "    llama_cl_combineddoc, \n",
    "    service_context=service_context\n",
    ")\n",
    "\n",
    "query_engine_base_combineddoc = index_base_combineddoc.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.copy()\n",
    "for i, gt in df2.iterrows():\n",
    "    try:\n",
    "        df2.loc[i, \"result\"] = query_engine_base_combineddoc.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df2.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df2, query_engine_base_combineddoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"/home/marshath/play/chainlink/algovate/test_llama/combineddoc_base.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://medium.com/llamaindex-blog/using-llms-for-retrieval-and-reranking-23cf2d3a14b6\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, chunk_size_limit=512)\n",
    "reranker = LLMRerank(choice_batch_size=5, top_n=3, service_context=service_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### techdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_llmreranker_techdoc = GPTVectorStoreIndex.from_documents(llama_cl_techdoc, service_context=service_context)\n",
    "\n",
    "query_engine_llmreranker_techdoc = index_llmreranker_techdoc.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 1a41b5f75d35c8bb9e19a7c96ef923bf in your message.).\n"
     ]
    }
   ],
   "source": [
    "df3 = df.copy()\n",
    "for i, gt in df3.iterrows():\n",
    "    try:\n",
    "        df3.loc[i, \"result\"] = query_engine_llmreranker_techdoc.query(gt.question)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        df3.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df3, query_engine_llmreranker_techdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv(\"/home/marshath/play/chainlink/algovate/test_llama/techdoc_llmreranker.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combineddoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_llmreranker_combineddoc = GPTVectorStoreIndex.from_documents(llama_cl_combineddoc, service_context=service_context)\n",
    "\n",
    "query_engine_llmreranker_combineddoc = index_llmreranker_combineddoc.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[reranker],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.copy()\n",
    "for i, gt in df4.iterrows():\n",
    "    try:\n",
    "        df4.loc[i, \"result\"] = query_engine_llmreranker_combineddoc.query(gt.question)\n",
    "    except Exception as e:\n",
    "        # logger.error(e)\n",
    "        df4.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df4, query_engine_llmreranker_combineddoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv(\"/home/marshath/play/chainlink/algovate/test_llama/combineddoc_llmreranker.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoPrevNext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### techdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults()\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(llama_cl_techdoc)\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "index_autopn_techdoc = GPTVectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "node_postprocessor = AutoPrevNextNodePostprocessor(docstore=docstore, num_nodes=4, service_context=service_context)\n",
    "\n",
    "query_engine_autopn_techdoc = index_autopn_techdoc.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[node_postprocessor],\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "df5 = df.copy()\n",
    "for i, gt in df5.iterrows():\n",
    "    try:\n",
    "        df5.loc[i, \"result\"] = query_engine_autopn_techdoc.query(gt.question)\n",
    "    except Exception as e:\n",
    "        df5.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df5, query_engine_autopn_techdoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.to_csv(\"/home/marshath/play/chainlink/algovate/test_llama/techdoc_autopn.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### combineddoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults()\n",
    "\n",
    "nodes = service_context.node_parser.get_nodes_from_documents(llama_cl_combineddoc)\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "index_autopn_combineddoc = GPTVectorStoreIndex(nodes, storage_context=storage_context)\n",
    "\n",
    "node_postprocessor = AutoPrevNextNodePostprocessor(docstore=docstore, num_nodes=4, service_context=service_context)\n",
    "\n",
    "query_engine_autopn_combineddoc = index_autopn_combineddoc.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    node_postprocessors=[node_postprocessor],\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "df6 = df.copy()\n",
    "for i, gt in df6.iterrows():\n",
    "    try:\n",
    "        df6.loc[i, \"result\"] = query_engine_autopn_combineddoc.query(gt.question)\n",
    "    except Exception as e:\n",
    "        df6.loc[i, \"result\"] = \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_docs(df6, query_engine_autopn_combineddoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.to_csv(\"/home/marshath/play/chainlink/algovate/test_llama/combineddoc_autopn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
