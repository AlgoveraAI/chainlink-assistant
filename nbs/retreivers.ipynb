{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import tiktoken\n",
    "from pydantic import BaseModel\n",
    "from typing import Any, Dict, List\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# Formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# stream handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data docs in retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/marshath/play/chainlink/chainlink-assistant/data/datadocs_2023-08-16.pkl\", \"rb\") as f:\n",
    "    data_docs = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(data_docs)\n",
    "vectorstrore = FAISS.from_documents(split_docs, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = vectorstrore.as_retriever(search_kwargs={\"k\":4})\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.)\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The contract address for ATOM / USD on the Moonbeam network is \"0x4f152d143c97b5e8d2293bc5b2380600f274a5dd\".\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques = \"what is the contract address for ATOM / USD on moonbeam network?\"\n",
    "r_docs = ret.get_relevant_documents(ques)\n",
    "answer = chain(ques)\n",
    "Markdown(answer[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The following is the details for the pair AAVE / USD which operates on the Metis Mainnet. This asset is named \"Aave\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.5%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0x54389e89a5ec1d4312d5b5c48055d6e56a177bf9"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(r_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three point router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomeSplitter:\n",
    "    def __init__(self, chunk_threshold=6000, chunk_size=6000, chunk_overlap=50):\n",
    "        self.chunk_threshold = chunk_threshold\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.splitter = TokenTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def token_counter(self, document):\n",
    "        tokens = self.enc.encode(document.page_content)\n",
    "        return len(tokens)\n",
    "\n",
    "    def split(self, documents):\n",
    "        chunked_documents = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            try:\n",
    "                if self.token_counter(doc) > self.chunk_threshold:\n",
    "                    chunks = self.splitter.split_documents([doc])\n",
    "                    chunks = [\n",
    "                        Document(\n",
    "                            page_content=chunk.page_content,\n",
    "                            metadata={\n",
    "                                \"source\": f\"{chunk.metadata['source']} chunk {i}\"\n",
    "                            },\n",
    "                        )\n",
    "                        for i, chunk in enumerate(chunks)\n",
    "                    ]\n",
    "                    chunked_documents.extend(chunks)\n",
    "                else:\n",
    "                    chunked_documents.append(doc)\n",
    "            except Exception as e:\n",
    "                chunked_documents.append(doc)\n",
    "                print(f\"Error on document {i}\")\n",
    "                print(e)\n",
    "                print(doc.metadata[\"source\"])\n",
    "\n",
    "        return chunked_documents\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    full_docs: List[Document]\n",
    "    base_retriever_all: BaseRetriever = None\n",
    "    base_retriever_data: BaseRetriever = None\n",
    "    k_initial: int = 10\n",
    "    k_final: int = 4\n",
    "\n",
    "    logger: Any = None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        full_docs: List[Document],\n",
    "        vectorstore_all: FAISS,\n",
    "        vectorstore_data: FAISS,\n",
    "        search_kwargs: Dict[str, Any] = {},\n",
    "        k_initial: int = 10,\n",
    "        k_final: int = 4,\n",
    "        logger: Any = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        # splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "        # split_docs = splitter.split_documents(full_docs)\n",
    "        # vector_store = FAISS.from_documents(split_docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "        return cls(\n",
    "            full_docs=full_docs,\n",
    "            base_retriever_all=vectorstore_all.as_retriever(search_kwargs={\"k\": k_initial}),\n",
    "            base_retriever_data=vectorstore_data.as_retriever(search_kwargs={\"k\": k_initial}),\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "    def get_relevant_documents(self, query: str, workflow:int=1) -> List[Document]:\n",
    "        self.logger.info(f\"Worflow: {workflow}\")\n",
    "\n",
    "        if workflow == 2:\n",
    "            results = self.base_retriever_data.get_relevant_documents(query=query)\n",
    "            self.logger.info(f\"Retrieved {len(results)} documents\")\n",
    "            return results[:self.k_final]\n",
    "\n",
    "        else:\n",
    "            results =  self.base_retriever_all.get_relevant_documents(query=query)\n",
    "            self.logger.info(f\"Retrieved {len(results)} documents\")\n",
    "            if workflow == 1:\n",
    "                doc_ids = [doc.metadata[\"source\"] for doc in results]\n",
    "\n",
    "                # make it a set but keep the order\n",
    "                doc_ids = list(dict.fromkeys(doc_ids))[:self.k_final]\n",
    "\n",
    "                # log to the logger\n",
    "                self.logger.info(f\"Retrieved {len(doc_ids)} unique documents\")\n",
    "\n",
    "                # get upto 4 documents\n",
    "                full_retrieved_docs = [d for d in self.full_docs if d.metadata[\"source\"] in doc_ids]\n",
    "\n",
    "                return self.prepare_source(full_retrieved_docs)\n",
    "\n",
    "            full_retrieved_docs = results[:self.k_final]\n",
    "            return self.prepare_source(full_retrieved_docs)\n",
    "        \n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_source(self, documents: List[Document]) -> List[Document]:\n",
    "        \n",
    "        for doc in documents:\n",
    "            source = doc.metadata[\"source\"]\n",
    "            if \"chunk\" in source:\n",
    "                source = source.split(\"chunk\")[0].strip()\n",
    "                doc.metadata[\"source\"] = source\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-apfNELnY4pAbHrx6LItJCss8 on tokens per min. Limit: 1000000 / min. Current: 849774 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-apfNELnY4pAbHrx6LItJCss8 on tokens per min. Limit: 1000000 / min. Current: 874278 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/marshath/play/chainlink/chainlink-assistant/data/documents.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "with open(\"/home/marshath/play/chainlink/chainlink-assistant/data/datadocs_2023-08-18.pkl\", \"rb\") as f:\n",
    "    datadocs = pickle.load(f)\n",
    "\n",
    "# Split documents into chunks for 16k model\n",
    "full_doc_splitter = CustomeSplitter()\n",
    "chunked_full_documents = full_doc_splitter.split(documents)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(documents)\n",
    "\n",
    "# Create vectorstore for all documents\n",
    "vectorstore_all = FAISS.from_documents(split_docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Split documents into chunks using datadocs\n",
    "split_docs_data = splitter.split_documents(datadocs)\n",
    "\n",
    "# Create vectorstore for datadocs\n",
    "vectorstore_data = FAISS.from_documents(split_docs_data, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore_all\n",
    "faiss.write_index(vectorstore_all.index, \"docs_all.index\")\n",
    "vectorstore_all.index = None\n",
    "with open(\"faiss_store_all.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore_all, f)\n",
    "\n",
    "# Save vectorstore_data\n",
    "faiss.write_index(vectorstore_data.index, \"docs_data.index\")\n",
    "vectorstore_data.index = None\n",
    "with open(\"faiss_store_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open vectorstore all\n",
    "index_all = faiss.read_index('/home/marshath/play/chainlink/chainlink-assistant/nbs/docs_all.index')\n",
    "index_data = faiss.read_index('/home/marshath/play/chainlink/chainlink-assistant/nbs/docs_data.index')\n",
    "\n",
    "with open('/home/marshath/play/chainlink/chainlink-assistant/nbs/faiss_store_all.pkl', 'rb') as f:\n",
    "    vectorstore_all = pickle.load(f)\n",
    "\n",
    "with open('/home/marshath/play/chainlink/chainlink-assistant/nbs/faiss_store_data.pkl', 'rb') as f:\n",
    "    vectorstore_data = pickle.load(f)\n",
    "\n",
    "vectorstore_all.index = index_all\n",
    "vectorstore_data.index = index_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "retriever = CustomRetriever.from_documents(\n",
    "    chunked_full_documents, \n",
    "    vectorstore_all=vectorstore_all,\n",
    "    vectorstore_data=vectorstore_data,\n",
    "    k_initial=10, \n",
    "    k_final=4, \n",
    "    logger=logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "final_answer_system_template = \"\"\"\n",
    "As an AI assistant helping answer a user's question about Chainlink, your task is to provide the answer to the user's question based on the collection of documents provided. Each document is demarcated by the 'Source:' tag. \n",
    "\n",
    "In most cases, the answer to the user's question can be found in one of the documents.\n",
    "\n",
    "If the documents do not contain the required information to answer user's question, respond with 'I don't know'. In this case, you can provide a link to the Chainlink documentation.\n",
    "\n",
    "Each point in your answer should be formatted with corresponding reference(s) using markdown. Conclude your response with a footnote that enumerates all the references involved. Please make sure to use only the references provided in the documents and not to use any external references. \n",
    "\n",
    "The footnote should be formatted as follows: \n",
    "```\n",
    "References:\n",
    "[^1^]: <reference 1> \n",
    "[^2^]: <reference 2> \n",
    "[^3^]: <reference 3>\n",
    "```\n",
    "Please avoid duplicating references. For example, if the same reference is used twice in the answer, please only include it once in the footnote.\n",
    "\"\"\"\n",
    "\n",
    "final_answer_human_template = \"\"\"\n",
    "User's question: {question}\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "FINAL_ANSWER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(final_answer_system_template),\n",
    "        HumanMessagePromptTemplate.from_template(final_answer_human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "final_answer_2_system_template = \"\"\"\n",
    "As an AI assistant helping answer a user's question about Chainlink, your task is to provide the answer to the user's question based on the potential answers derived from previous LLM call(s). \n",
    "If the document doesn't contain the required information, respond with 'I don't know'.\n",
    "Each point in your answer should be formatted with corresponding reference(s) using markdown. Conclude your response with a footnote that enumerates all the references involved. \n",
    "\n",
    "The footnote should be formatted as follows: \n",
    "```\n",
    "References:\n",
    "[^1^]: <reference 1> \n",
    "[^2^]: <reference 2> \n",
    "[^3^]: <reference 3>\n",
    "```\n",
    "Please avoid duplicating references. For example, if the same reference is used twice in the answer, please only include it once in the footnote.\n",
    "\"\"\"\n",
    "\n",
    "final_answer_2_human_template = \"\"\"\n",
    "User's question: {question}\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "FINAL_ANSWER_2_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(final_answer_2_system_template),\n",
    "        HumanMessagePromptTemplate.from_template(final_answer_2_human_template),\n",
    "    ]\n",
    ")\n",
    "\n",
    "router_system_prompt = \"\"\"\n",
    "As an AI assistant helping ansswer a user's question about Chainlink, your first task is to route the question to the proper workflow. \n",
    "There are three workflows:\n",
    "    1. short-form which is suitable for simple questions. It is bad at answering questions requiring code output.\n",
    "    2. long-form which is suitable for complex questions. It is good at answering questions requiring code output.\n",
    "    3. is a specialized workflow for answering questions about Chainlink's price feeds. It is good at answering questions about Chainlink's price feeds.\n",
    "\n",
    "Sample questions for each workflow:\n",
    "\n",
    "Workflow 1: short-form\n",
    "- What is Chainlink?\n",
    "- What is a Chainlink node?\n",
    "- What is a Chainlink oracle?\n",
    "\n",
    "Workflow 2: long-form\n",
    "- give me a sample solidity contract to use Chainlink price feeds?\n",
    "- give me a sample solidity contract to use Chainlink VRF?\n",
    "- give me a sample solidity contract to use NFT Floor Price Feeds?\n",
    "- Give code examples to demonstrate how to deploy a consumer contract on-chain that reads a data feed and stores the value?\n",
    "\n",
    "Workflow 3: specialized\n",
    "- check if a feed is verified, ex: Is eth/usd a verified feed?\n",
    "- is eth/usd feed backed by staking?\n",
    "- under what asset class does eth/usd fall?\n",
    "- what is the tier of the eth/usd feed on binance?\n",
    "- what is the deviation threshold of eth/usd on binance?\n",
    "- how many oracles carry eth/usd on binance?\n",
    "\n",
    "Your output should be a number between 0 and 2. 0 means the question should be routed to the short-form workflow. 1 means the question should be routed to the long-form workflow. 2 means the question should be routed to the specialized workflow.\n",
    "\"\"\"\n",
    "\n",
    "router_human_prompt = \"\"\"\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "ROUTER_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate.from_template(router_system_prompt),\n",
    "        HumanMessagePromptTemplate.from_template(router_human_prompt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streaming_chain(chain, workflow):\n",
    "    \"\"\"Return a new streaming chain.\"\"\"\n",
    "\n",
    "    if workflow == 1:\n",
    "        llm_stream = ChatOpenAI(\n",
    "            temperature=0.0,\n",
    "            model=\"gpt-3.5-turbo-16k\",\n",
    "            streaming=True,\n",
    "            callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        )\n",
    "        logger.info(\"Using long-form workflow\")\n",
    "        chain.llm = llm_stream\n",
    "    else:\n",
    "        llm_stream = ChatOpenAI(\n",
    "            temperature=0.0,\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            streaming=True,\n",
    "            # max_tokens=256,\n",
    "            callbacks=[StreamingStdOutCallbackHandler()],\n",
    "        )\n",
    "        chain.llm = llm_stream\n",
    "        logger.info(\"Using short-form workflow\")\n",
    "\n",
    "    return chain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.)\n",
    "base_chain = LLMChain(llm=llm, prompt=FINAL_ANSWER_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "try:\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "except KeyError:\n",
    "    logger.info(f\"Encoding for model {model} not found. Using default encoding.\")\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "def calculate_tokens(document, encoding):\n",
    "    \"\"\"Calculate the number of tokens in a list of documents.\"\"\"\n",
    "    return len(encoding.encode(document))\n",
    "\n",
    "\n",
    "def concatenate_documents(documents, max_tokens):\n",
    "    \"\"\"Combine documents up to a certain token limit.\"\"\"\n",
    "    combined_docs = \"\"\n",
    "    token_count = 0\n",
    "    used_docs = []\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_tokens = calculate_tokens(doc.page_content, encoding)\n",
    "        if (token_count + doc_tokens) <= max_tokens:\n",
    "            combined_docs += f\"\\n\\n{doc.page_content}\\nSource: {doc.metadata['source']}\"\n",
    "            token_count += doc_tokens\n",
    "            used_docs.append(doc)\n",
    "\n",
    "    return combined_docs, used_docs\n",
    "\n",
    "\n",
    "def call_llm_final_answer(question, document, chain, stream=False):\n",
    "    \"\"\"Call LLM with a question and a single document.\"\"\"\n",
    "    chain.prompt = FINAL_ANSWER_PROMPT\n",
    "    if stream:\n",
    "        return chain.apredict(question=question, document=document)\n",
    "    else:\n",
    "        return chain.predict(question=question, document=document)\n",
    "\n",
    "\n",
    "def call_llm_final_2_answer(question, document, chain):\n",
    "    \"\"\"Call LLM with a question and a single document.\"\"\"\n",
    "    chain.prompt = FINAL_ANSWER_2_PROMPT\n",
    "    return chain.apredict(question=question, document=document)\n",
    "\n",
    "\n",
    "def process_documents(question, chain, max_tokens=14_000):\n",
    "    \"\"\"Process a list of documents with LLM calls.\"\"\"\n",
    "    \n",
    "    # Use router to decide which workflow to use\n",
    "    chain.prompt = ROUTER_PROMPT\n",
    "    try:\n",
    "        workflow = int(chain.predict(question=question))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in router: {e}\")\n",
    "        workflow = 0\n",
    "\n",
    "    logger.info(f\"Using workflow {workflow}\")\n",
    "    print(f\"Using workflow {workflow}\")\n",
    "\n",
    "    documents = retriever.get_relevant_documents(question, workflow=workflow)\n",
    "    batches = []\n",
    "    num_llm_calls = 0\n",
    "    while documents:\n",
    "        batch, used_docs = concatenate_documents(documents, max_tokens)\n",
    "        batches.append(batch)\n",
    "        # logger.info(f\"Calling LLM with {batch}\")\n",
    "        documents = [doc for doc in documents if doc not in used_docs]\n",
    "        num_llm_calls += 1\n",
    "        logger.info(\n",
    "            f\"LLM call {num_llm_calls} complete. {len(documents)} documents remaining.\"\n",
    "        )\n",
    "\n",
    "    return batches, num_llm_calls, workflow\n",
    "\n",
    "\n",
    "async def get_answer(question,max_tokens=14_000):\n",
    "    \"\"\"Get an answer to a question.\"\"\"\n",
    "\n",
    "    # Main code that calls process_documents\n",
    "    batches, num_llm_calls, workflow = process_documents(\n",
    "        question=question, \n",
    "        max_tokens=max_tokens,\n",
    "        chain=base_chain\n",
    "    )\n",
    "\n",
    "    # Get the streaming chain\n",
    "    chain_stream = get_streaming_chain(\n",
    "        chain=base_chain,\n",
    "        workflow=workflow\n",
    "    )\n",
    "\n",
    "    if num_llm_calls == 1:\n",
    "        result = await call_llm_final_answer(\n",
    "            question=question, \n",
    "            document=batches[0], \n",
    "            chain=chain_stream, \n",
    "            stream=True\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        # Handle the list of batches\n",
    "        results = []\n",
    "        for batch in batches:\n",
    "            result = call_llm_final_answer(\n",
    "                question=question, \n",
    "                document=batch, \n",
    "                chain=base_chain\n",
    "            )\n",
    "            results.append(result)\n",
    "\n",
    "        combined_result = \" \".join(results)\n",
    "\n",
    "        logger.info(f\"Final LLM call with {len(results)} results.\")\n",
    "        combined_result = await call_llm_final_2_answer(\n",
    "            question=question, \n",
    "            document=combined_result, \n",
    "            chain=chain_stream\n",
    "        )\n",
    "\n",
    "        return combined_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using workflow 1\n",
      "To retrieve feed data from a price feed using web3.js, you can follow these steps:\n",
      "\n",
      "1. Import the necessary dependencies:\n",
      "```javascript\n",
      "const Web3 = require('web3');\n",
      "const contractABI = require('<path_to_contract_ABI>');\n",
      "```\n",
      "\n",
      "2. Create a new instance of the web3 object:\n",
      "```javascript\n",
      "const web3 = new Web3('<provider_url>');\n",
      "```\n",
      "\n",
      "3. Get the contract instance using the contract ABI and contract address:\n",
      "```javascript\n",
      "const contract = new web3.eth.Contract(contractABI, '<contract_address>');\n",
      "```\n",
      "\n",
      "4. Call the contract function to retrieve the feed data:\n",
      "```javascript\n",
      "contract.methods.getLatestPrice().call((error, result) => {\n",
      "  if (error) {\n",
      "    console.error(error);\n",
      "  } else {\n",
      "    console.log(result);\n",
      "  }\n",
      "});\n",
      "```\n",
      "\n",
      "Replace `<path_to_contract_ABI>`, `<provider_url>`, and `<contract_address>` with the actual values for your contract.\n",
      "\n",
      "This code assumes that you have the contract ABI and the contract address. If you don't have them, you can refer to the Chainlink documentation or the contract source code to obtain them.\n",
      "\n",
      "Note: Make sure you have a web3 provider set up and connected to the Ethereum network.\n",
      "\n",
      "References:\n",
      "[^1^]: [Fetch Commodity Prices in Solidity Smart Contracts](https://blog.chain.link/fetch-commodity-prices-in-solidity-smart-contracts/)\n",
      "[^2^]: [Craft Whiskey Crypto Payments With Chainlink Oracles](https://blog.chain.link/craft-whiskey-crypto-payments-with-chainlink-oracles/)\n",
      "[^3^]: [Get Index Prices in Solidity Smart Contracts](https://blog.chain.link/get-index-prices-in-solidity-smart-contracts/)\n",
      "[^4^]: [Using Data Feeds Off-Chain (Solana)](https://docs.chain.link/data-feeds/solana/using-data-feeds-off-chain/)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To retrieve feed data from a price feed using web3.js, you can follow these steps:\\n\\n1. Import the necessary dependencies:\\n```javascript\\nconst Web3 = require('web3');\\nconst contractABI = require('<path_to_contract_ABI>');\\n```\\n\\n2. Create a new instance of the web3 object:\\n```javascript\\nconst web3 = new Web3('<provider_url>');\\n```\\n\\n3. Get the contract instance using the contract ABI and contract address:\\n```javascript\\nconst contract = new web3.eth.Contract(contractABI, '<contract_address>');\\n```\\n\\n4. Call the contract function to retrieve the feed data:\\n```javascript\\ncontract.methods.getLatestPrice().call((error, result) => {\\n  if (error) {\\n    console.error(error);\\n  } else {\\n    console.log(result);\\n  }\\n});\\n```\\n\\nReplace `<path_to_contract_ABI>`, `<provider_url>`, and `<contract_address>` with the actual values for your contract.\\n\\nThis code assumes that you have the contract ABI and the contract address. If you don't have them, you can refer to the Chainlink documentation or the contract source code to obtain them.\\n\\nNote: Make sure you have a web3 provider set up and connected to the Ethereum network.\\n\\nReferences:\\n[^1^]: [Fetch Commodity Prices in Solidity Smart Contracts](https://blog.chain.link/fetch-commodity-prices-in-solidity-smart-contracts/)\\n[^2^]: [Craft Whiskey Crypto Payments With Chainlink Oracles](https://blog.chain.link/craft-whiskey-crypto-payments-with-chainlink-oracles/)\\n[^3^]: [Get Index Prices in Solidity Smart Contracts](https://blog.chain.link/get-index-prices-in-solidity-smart-contracts/)\\n[^4^]: [Using Data Feeds Off-Chain (Solana)](https://docs.chain.link/data-feeds/solana/using-data-feeds-off-chain/)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await get_answer(\"Write code to show how to use web3.js to retrieve feed data from a price feed?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
