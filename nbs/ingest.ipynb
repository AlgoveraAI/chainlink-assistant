{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import bs4\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import html2text\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from typing import List, Optional, Set, Dict, Tuple, Union, Any\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from youtube_transcript_api import (\n",
    "    NoTranscriptFound,\n",
    "    TranscriptsDisabled,\n",
    "    YouTubeTranscriptApi,\n",
    ")\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "# Settings for requests\n",
    "MAX_THREADS = 10\n",
    "REQUEST_DELAY = 0.1\n",
    "SESSION = requests.Session()\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Ensure GUI is off\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Set up the webdriver\n",
    "s=Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=s, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_links(soup, filter_str='/polygon/mainnet/'):\n",
    "    # Get all links\n",
    "    links = soup.find_all('a')\n",
    "    \n",
    "    # Filter links to only those that are for polygon mainnet\n",
    "    hrefs = [link.get('href') for link in links]\n",
    "    filtered_hrefs = [href for href in hrefs if href is not None and filter_str in href and href.count('/') == 4]\n",
    "\n",
    "    return filtered_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    \"\"\"\n",
    "    Get all links from a given url\n",
    "    \"\"\"\n",
    "\n",
    "    filter_sub_url = url.split(\"link\")[1]\n",
    "\n",
    "    all_links = []\n",
    "    for i in range(10):\n",
    "        if i == 0:\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(7)\n",
    "            time.sleep(7)\n",
    "\n",
    "        else:\n",
    "            driver.find_element(by=\"xpath\", value=\"/html/body/div[1]/main/section[2]/div/div[2]/button[2]\").click()\n",
    "            driver.implicitly_wait(7)\n",
    "            time.sleep(7)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        hrefs = filter_links(soup, filter_sub_url)\n",
    "        all_links.extend(hrefs)\n",
    "        logger.info(f\"Page {i+1} scraped\")\n",
    "    \n",
    "    # Add base url\n",
    "    all_links = [f\"https://data.chain.link{link}\" for link in all_links]\n",
    "    \n",
    "    # Remove duplicates\n",
    "    all_links = list(set(all_links))\n",
    "\n",
    "    return all_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(url):\n",
    "    driver.get(u)\n",
    "    driver.implicitly_wait(3)\n",
    "    time.sleep(3)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    details = {}\n",
    "\n",
    "    # Get the title\n",
    "    details[\"pair\"] = soup.find(\"h1\").text\n",
    "\n",
    "    # Get the details\n",
    "    infos = soup.findAll(\"p\")\n",
    "\n",
    "    # Match pattern\n",
    "    match = r'Minimum of (\\d+)'\n",
    "\n",
    "    prev_word = \"\"\n",
    "    for info in infos:\n",
    "        if prev_word == \"Asset Name\":\n",
    "            details[\"asset_name\"] = info.text\n",
    "        elif prev_word == \"Asset Class\":\n",
    "            details[\"asset_class\"] = info.text\n",
    "        elif prev_word == \"Tier\":\n",
    "            details[\"tier\"] = info.text\n",
    "        elif prev_word == \"Network\":\n",
    "            details[\"network\"] = info.text\n",
    "        elif prev_word == \"Deviation threshold\":\n",
    "            details[\"deviation\"] = info.text\n",
    "        if re.search(match, prev_word):\n",
    "            details[\"num_oracles\"] = info.text\n",
    "        prev_word = info.text\n",
    "\n",
    "    try:\n",
    "        for each in soup.find(\"div\", class_=\"sc-d6e7e954-0 sc-e3a5e58-0 teTjm\"):\n",
    "            if each.name != \"div\":\n",
    "                details[\"contract_address\"] = each.a.text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        for each in soup.find(\"div\", class_=\"sc-d6e7e954-0 sc-3ba96657-0 sc-b8182c9f-1 hRpMsk iSLEhf\"):\n",
    "                details[\"ens_address\"] = each.div.next_sibling.text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_base_urls = [\n",
    "    \"https://data.chain.link/ethereum/mainnet\",\n",
    "    \"https://data.chain.link/polygon/mainnet\",\n",
    "    \"https://data.chain.link/optimism/mainnet\",\n",
    "    \"https://data.chain.link/fantom/mainnet\",\n",
    "    \"https://data.chain.link/moonriver/mainnet\",\n",
    "    \"https://data.chain.link/metis/mainnet\",\n",
    "    \"https://data.chain.link/bsc/mainnet\",\n",
    "    \"https://data.chain.link/arbitrum/mainnet\",\n",
    "    \"https://data.chain.link/avalanche/mainnet\",\n",
    "    \"https://data.chain.link/harmony/mainnet\",\n",
    "    \"https://data.chain.link/moonbeam/mainnet\",\n",
    "]\n",
    "\n",
    "eth_urls = get_links(all_base_urls[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "eth_details = []\n",
    "\n",
    "for u in tqdm(eth_urls[:10], total=len(eth_urls[:10])):\n",
    "    try:\n",
    "        eth_details.append(get_details(u))\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to get details for {u}')\n",
    "        logger.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(details):\n",
    "    \"\"\"Make a sentence from the details\"\"\"\n",
    "\n",
    "    first_sentence = \"\"\"The following is the details for the pair {pair} which operates on the {network}.\"\"\"\n",
    "    second_sentence = \"\"\"This asset is named \"{asset_name}\".\"\"\" \n",
    "    third_sentence = \"\"\"and falls under the \"{asset_class}\" asset class.\"\"\"\n",
    "    fourth_sentence = \"\"\"It has a tier status of \"{tier}\".\"\"\" \n",
    "    fifth_sentence = \"\"\"The deviation threshold for this asset is set at {deviation}.\"\"\" \n",
    "    sixth_sentence = \"\"\"{num_oracles} oracles carries and support this asset.\"\"\" \n",
    "    seventh_sentence = \"\"\"You can find its contract at the address \"{contract_address}\"\"\"\n",
    "    eigth_sentence = \"\"\", and its ENS address is \"{ens_address}\".\"\"\"\n",
    "\n",
    "    sentence = first_sentence.format(pair=details[\"pair\"], network=details[\"network\"])\n",
    "\n",
    "    if \"asset_name\" in details.keys():\n",
    "        sentence += f\" {second_sentence.format(asset_name=details['asset_name'])}\"\n",
    "\n",
    "    if \"asset_class\" in details.keys():\n",
    "        sentence += f\" {third_sentence.format(asset_class=details['asset_class'])}\"\n",
    "\n",
    "    if \"tier\" in details.keys():\n",
    "        sentence += f\" {fourth_sentence.format(tier=details['tier'])}\"\n",
    "\n",
    "    if \"deviation\" in details.keys():\n",
    "        sentence += f\" {fifth_sentence.format(deviation=details['deviation'])}\"\n",
    "\n",
    "    if \"num_oracles\" in details.keys():\n",
    "        sentence += f\" {sixth_sentence.format(num_oracles=details['num_oracles'])}\"\n",
    "\n",
    "    if \"contract_address\" in details.keys():\n",
    "        sentence += f\" {seventh_sentence.format(contract_address=details['contract_address'])}\"\n",
    "\n",
    "    if \"ens_address\" in details.keys():\n",
    "        sentence += f\" {eigth_sentence.format(ens_address=details['ens_address'])}\"\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for det in eth_details:\n",
    "    sentences.append(make_sentence(det))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The following is the details for the pair FXS / USD which operates on the BNB Chain Mainnet. This asset is named \"Frax Share\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.5%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0x0e9d55932893fb1308882c7857285b2b0bcc4f4a',\n",
       " 'The following is the details for the pair BAND / BNB which operates on the BNB Chain Mainnet. This asset is named \"Band Protocol\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.5%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0x3334bf7ec892ca03d1378b51769b7782eaf318c4',\n",
       " 'The following is the details for the pair MASK / USD which operates on the BNB Chain Mainnet. This asset is named \"Mask Network\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.5%. 16 / 16 oracles carries and support this asset. You can find its contract at the address \"0x4978c0abe6899178c1a74838ee0062280888e2cf',\n",
       " 'The following is the details for the pair ONG / USD which operates on the BNB Chain Mainnet. This asset is named \"Ontology Gas\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.5%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0xcf95796f3016801a1da5c518fc7a59c51dcef793',\n",
       " 'The following is the details for the pair CAKE / USD which operates on the BNB Chain Mainnet. This asset is named \"PancakeSwap\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.2%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0xb6064ed41d4f67e353768aa239ca86f4f73665a1',\n",
       " 'The following is the details for the pair AUD / USD which operates on the BNB Chain Mainnet. This asset is named \"Australian Dollar\". and falls under the \"Forex\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.2%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0x498f912b09b5df618c77fcc9e8da503304df92bf',\n",
       " 'The following is the details for the pair FIL / USD which operates on the BNB Chain Mainnet. This asset is named \"Filecoin\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.3%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0xe5dbfd9003bff9df5feb2f4f445ca00fb121fb83',\n",
       " 'The following is the details for the pair CAKE / BNB which operates on the BNB Chain Mainnet. This asset is named \"PancakeSwap\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.5%. 16 / 16 oracles carries and support this asset. You can find its contract at the address \"0xcb23da9ea243f53194cbc2380a6d4d9bc046161f',\n",
       " 'The following is the details for the pair DODO / USD which operates on the BNB Chain Mainnet. This asset is named \"Dodo\". and falls under the \"Crypto\" asset class. It has a tier status of \"Verified\". The deviation threshold for this asset is set at 0.5%. 16 / 16 oracles carries and support this asset. You can find its contract at the address \"0x87701b15c08687341c2a847ca44ecfbc8d7873e1',\n",
       " 'The following is the details for the pair FTT / USD which operates on the BNB Chain Mainnet. This asset is named \"FTX Token\". and falls under the \"Crypto\" asset class. It has a tier status of \"Monitored\". The deviation threshold for this asset is set at 0.5%. 15 / 15 oracles carries and support this asset. You can find its contract at the address \"0x38e05754eb00171cbe72ba1ee792933d6e8d2891']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chain.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_urls_by_base_url(urls:List, base_url:str):\n",
    "    \"\"\"\n",
    "    Filters a list of URLs and returns only those that include the base_url.\n",
    "\n",
    "    :param urls: List of URLs to filter.\n",
    "    :param base_url: Base URL to filter by.\n",
    "    :return: List of URLs that include the base_url.\n",
    "    \"\"\"\n",
    "    return [url for url in urls if base_url in url]\n",
    "\n",
    "def normalize_url(url:str):\n",
    "    \"\"\"\n",
    "    Normalize a URL by ensuring it ends with '/'.\n",
    "\n",
    "    :param url: URL to normalize.\n",
    "    :return: Normalized URL.\n",
    "    \"\"\"\n",
    "    return url if url.endswith('/') else url + '/'\n",
    "\n",
    "def fetch_url_request(url:str):\n",
    "    \"\"\"\n",
    "    Fetches the content of a URL using requests library and returns the response.\n",
    "    In case of any exception during fetching, logs the error and returns None.\n",
    "\n",
    "    :param url: URL to fetch.\n",
    "    :return: Response object on successful fetch, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = SESSION.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response\n",
    "    except RequestException as e:\n",
    "        logger.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_url_selenium(url:str):\n",
    "    \"\"\"\n",
    "    Fetches the content of a URL using Selenium and returns the source HTML of the page.\n",
    "    In case of any exception during fetching, logs the error and returns None.\n",
    "\n",
    "    :param url: URL to fetch.\n",
    "    :return: HTML source as a string on successful fetch, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(7)\n",
    "        time.sleep(7)\n",
    "        return driver.page_source\n",
    "    \n",
    "    except RequestException as e:\n",
    "        logger.error(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_url(response:requests.Response, visited:Set, base_url:str):\n",
    "    \"\"\"\n",
    "    Process a URL response. Extract all absolute URLs from the response that \n",
    "    haven't been visited yet and belong to the same base_url.\n",
    "\n",
    "    :param response: Response object from a URL fetch.\n",
    "    :param visited: Set of URLs already visited.\n",
    "    :param base_url: Base URL to filter by.\n",
    "    :return: Set of new URLs to visit.\n",
    "    \"\"\"\n",
    "    urls = set()\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href is not None and '#' not in href:\n",
    "                absolute_url = normalize_url(urljoin(response.url, href))\n",
    "                if absolute_url not in visited and base_url in absolute_url:\n",
    "                    visited.add(absolute_url)\n",
    "                    urls.add(absolute_url)\n",
    "    return urls\n",
    "\n",
    "def get_all_suburls(url:str, visited:Optional[Set]=None):\n",
    "    \"\"\"\n",
    "    Get all sub-URLs of a given URL that belong to the same domain.\n",
    "\n",
    "    :param url: Base URL to start the search.\n",
    "    :param visited: Set of URLs already visited.\n",
    "    :return: Set of all sub-URLs.\n",
    "    \"\"\"\n",
    "    if visited is None:\n",
    "        visited = set()\n",
    "\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url\n",
    "\n",
    "    base_url = url.split(\"//\")[1].split(\"/\")[0]\n",
    "    urls = set()\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        future_responses = [executor.submit(fetch_url_request, url)]\n",
    "        while future_responses:\n",
    "            for future in concurrent.futures.as_completed(future_responses):\n",
    "                future_responses.remove(future)\n",
    "                response = future.result()\n",
    "                new_urls = process_url(response, visited, base_url)\n",
    "                urls.update(new_urls)\n",
    "                if len(future_responses) < MAX_THREADS:\n",
    "                    for new_url in new_urls:\n",
    "                        future_responses.append(executor.submit(fetch_url_request, new_url))\n",
    "\n",
    "    urls = filter_urls_by_base_url(urls, base_url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_there_video(soup:BeautifulSoup) -> List[str]:\n",
    "    \"\"\"Check if there is a video in the soup\n",
    "    params:\n",
    "        soup: BeautifulSoup object\n",
    "    returns:\n",
    "        video_links: List of video links\n",
    "    \"\"\"\n",
    "    iframes = soup.find_all('iframe')\n",
    "    video_links = []\n",
    "    for iframe in iframes:\n",
    "        src = iframe.get('src')\n",
    "        if 'youtube' in src:\n",
    "            video_links.append(src)\n",
    "\n",
    "    return video_links\n",
    "\n",
    "def get_youtube_docs(video_tags:List[str], chain_description) -> List[Document]:\n",
    "    \"\"\"Get youtube docs from the video tags\n",
    "    params:\n",
    "        video_tags: List of video tags\n",
    "    returns:\n",
    "        u_tube_docs: List of youtube docs\n",
    "    \"\"\"\n",
    "    if video_tags:\n",
    "        u_tube_docs = []\n",
    "        for v_tag in video_tags:\n",
    "            try:\n",
    "                u_tube = json.loads(v_tag.script.string)[\"items\"][0][\"url\"]\n",
    "                u_tube_id = YoutubeLoader.extract_video_id(u_tube)\n",
    "                u_tube_doc = YoutubeLoader(u_tube_id, add_video_info=True).load()[0]\n",
    "\n",
    "                # Get description\n",
    "                description = chain_description.predict(context=u_tube_doc.page_content[:1500])\n",
    "\n",
    "                # Make sure its Chainlink video\n",
    "                assert u_tube_doc.metadata[\"author\"].lower() == \"chainlink\"\n",
    "                metadata = {\n",
    "                    \"source\":u_tube, \n",
    "                    \"source_type\":\"video\", \n",
    "                    \"title\":u_tube_doc.metadata[\"title\"],\n",
    "                    \"description\":description}\n",
    "\n",
    "                # Update the metadata\n",
    "                u_tube_doc.metadata = metadata\n",
    "\n",
    "                # Append to the list\n",
    "                u_tube_docs.append(u_tube_doc)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                u_tube_doc = []\n",
    "    else:\n",
    "        u_tube_docs = []\n",
    "    return u_tube_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix_text(markdown):\n",
    "    # Split the content at the first title\n",
    "    parts = re.split(r'^(#\\s.+)$', markdown, maxsplit=1, flags=re.MULTILINE)\n",
    "\n",
    "    # If a split occurred, then take the content from the first title onward\n",
    "    new_text = parts[-2] + parts[-1] if len(parts) > 1 else markdown\n",
    "\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def extract_first_n_paragraphs(content, num_para=2):\n",
    "\n",
    "    # Split by two newline characters to denote paragraphs\n",
    "    paragraphs = content.split('\\n\\n')\n",
    "    \n",
    "    # Return the first num_para paragraphs or whatever is available\n",
    "    return '\\n\\n'.join(paragraphs[:num_para])\n",
    "\n",
    "\n",
    "def scrap_url(url: str,  chain_description:LLMChain, driver: webdriver.Chrome=driver) -> Document:\n",
    "    \"\"\"Process a URL and return a list of words\n",
    "    param url: URL to process\n",
    "    param driver: Selenium driver\n",
    "    return: Document object\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(2)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Get the page source\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Get the Markdown content\n",
    "    # Remove images, videos, SVGs, and other media elements; also nav\n",
    "    for media_tag in soup.find_all(['img', 'video', 'svg', 'audio', 'source', 'track', 'picture', 'nav']):\n",
    "        media_tag.decompose()\n",
    "\n",
    "    # Remove the footer (assuming it's in a <footer> tag or has a class/id like 'footer')\n",
    "    for footer_tag in soup.find_all(['footer', {'class': 'footer'}, {'id': 'footer'}]):\n",
    "        footer_tag.decompose()\n",
    "\n",
    "    # Remove sections with class=\"section-page-alert\"\n",
    "    for page_alert in soup.find_all('div', class_='section-page-alert'):\n",
    "        page_alert.decompose()\n",
    "\n",
    "    # Remove sections with class=\"cta-subscribe\"\n",
    "    for cta_subscribe in soup.find_all(class_='cta-subscribe'):\n",
    "        cta_subscribe.decompose()\n",
    "        \n",
    "\n",
    "    html_content = str(soup)\n",
    "    h = html2text.HTML2Text()\n",
    "    markdown_content = h.handle(html_content)\n",
    "\n",
    "    # Remove the prefix\n",
    "    markdown_content = remove_prefix_text(markdown_content)\n",
    "\n",
    "    # Get the title\n",
    "    titles = re.findall(r'^#\\s(.+)$', markdown_content, re.MULTILINE)\n",
    "    title = titles[0].strip() \n",
    "\n",
    "    # Get description\n",
    "    para = extract_first_n_paragraphs(markdown_content, num_para=2)\n",
    "    description = chain_description.predict(context=para)\n",
    "    \n",
    "    # Put the markdown content into a Document object\n",
    "    doc = Document(page_content=markdown_content, metadata={\n",
    "        \"source\": url, \n",
    "        \"title\": title, \n",
    "        \"description\": description, \n",
    "        \"source_type\": \"main\"})\n",
    "\n",
    "    # Get YouTube docs\n",
    "    video_tags = soup.find_all('a', href=True, class_=\"techtalk-video-lightbox\")\n",
    "    u_tube_docs = get_youtube_docs(video_tags, chain_description)    \n",
    "\n",
    "    return doc, u_tube_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_chain():\n",
    "    system_template = \"\"\"\n",
    "    Please summarize the context below in one sentence (no more than 15 words). This will be used as the description of the article in the search results.\n",
    "\n",
    "    Response should be NO MORE THAN 15 words.\n",
    "    \"\"\"\n",
    "\n",
    "    human_template = \"\"\"{context}\"\"\"\n",
    "\n",
    "    PROMPT = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            SystemMessagePromptTemplate.from_template(system_template),\n",
    "            HumanMessagePromptTemplate.from_template(human_template),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0.)\n",
    "    chain = LLMChain(llm=llm, prompt=PROMPT)\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap_chain_link() -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Scrap all the urls from https://chain.link/ and save the main docs and you tube docs to disk\n",
    "    return: Tuple[List[Dict], List[Dict]]\n",
    "    \"\"\"\n",
    "\n",
    "    raw_urls = get_all_suburls(\"https://chain.link/\")\n",
    "    \n",
    "    # Only keep urls that start with https://chain.link\n",
    "    raw_urls = [url for url in raw_urls if url.startswith(\"https://chain.link\")]\n",
    "\n",
    "    # add chain.link/faqs if not in raw_urls\n",
    "    if \"https://chain.link/faqs\" not in raw_urls:\n",
    "        raw_urls.append(\"https://chain.link/faqs\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    raw_urls = list(set(raw_urls))\n",
    "\n",
    "    # Process urls\n",
    "    all_main_docs = []\n",
    "    all_you_tube_docs = []\n",
    "\n",
    "    # Get description chain\n",
    "    chain_description = get_description_chain()\n",
    "\n",
    "    for url in tqdm(raw_urls, total=len(raw_urls)):\n",
    "        logger.info(f\"Processing {url}\")\n",
    "        main_doc, you_tube_docs = scrap_url(url, chain_description)\n",
    "        all_main_docs.append(main_doc)\n",
    "        all_you_tube_docs.extend(you_tube_docs)\n",
    "        logger.info(f\"Processed {url}\")\n",
    "\n",
    "    # Save to disk as pickle\n",
    "    with open(f\"chain_link_main_docs_{datetime.now().strftime('%Y-%m-%d')}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_main_docs, f)\n",
    "\n",
    "    with open(f\"chain_link_you_tube_docs_{datetime.now().strftime('%Y-%m-%d')}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(all_you_tube_docs, f)\n",
    "\n",
    "    logger.info(\"Done\")\n",
    "\n",
    "    return all_main_docs, all_you_tube_docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error fetching mailto:custom@chain.link/: No connection adapters were found for 'mailto:custom@chain.link/'\n",
      "Error fetching mailto:support@chain.link/: No connection adapters were found for 'mailto:support@chain.link/'\n",
      "Error fetching https://blog.chain.link/build-deploy-and-sell-your-own-dynamic-nft/: 403 Client Error: Forbidden for url: https://blog.chain.link/build-deploy-and-sell-your-own-dynamic-nft/\n",
      "Error fetching https://blog.chain.link/smart-contract-use-cases/: 403 Client Error: Forbidden for url: https://blog.chain.link/smart-contract-use-cases/\n",
      "Error fetching https://blog.chain.link/: 403 Client Error: Forbidden for url: https://blog.chain.link/\n",
      "Error fetching https://blog.chain.link/apis-smart-contracts-and-how-to-connect-them/: 403 Client Error: Forbidden for url: https://blog.chain.link/apis-smart-contracts-and-how-to-connect-them/\n",
      "Error fetching https://blog.chain.link/chainlink-chinese-communities/: 403 Client Error: Forbidden for url: https://blog.chain.link/chainlink-chinese-communities/\n",
      "Error fetching mailto:press@chain.link/: No connection adapters were found for 'mailto:press@chain.link/'\n",
      "Error fetching https://blog.chain.link/what-is-chainlink/: 403 Client Error: Forbidden for url: https://blog.chain.link/what-is-chainlink/\n",
      "Error fetching https://blog.chain.link/fetch-current-crypto-price-data-solidity/: 403 Client Error: Forbidden for url: https://blog.chain.link/fetch-current-crypto-price-data-solidity/\n",
      "Error fetching https://blog.chain.link/spring-2023-hackathon-winners/: 403 Client Error: Forbidden for url: https://blog.chain.link/spring-2023-hackathon-winners/\n",
      "Error fetching https://blog.chain.link/increasing-cost-efficiency-of-decentralized-oracle-networks/: 403 Client Error: Forbidden for url: https://blog.chain.link/increasing-cost-efficiency-of-decentralized-oracle-networks/\n",
      "Error fetching https://blog.chain.link/chainlink-build-program/: 403 Client Error: Forbidden for url: https://blog.chain.link/chainlink-build-program/\n",
      "Error fetching https://blog.chain.link/verify-stablecoin-collateral-with-chainlink-proof-of-reserve/: 403 Client Error: Forbidden for url: https://blog.chain.link/verify-stablecoin-collateral-with-chainlink-proof-of-reserve/\n",
      "Error fetching https://blog.chain.link/build-a-marine-insurance-smart-contract-with-chainlink/: 403 Client Error: Forbidden for url: https://blog.chain.link/build-a-marine-insurance-smart-contract-with-chainlink/\n",
      "Error fetching https://blog.chain.link/defi-security-best-practices/: 403 Client Error: Forbidden for url: https://blog.chain.link/defi-security-best-practices/\n",
      "Error fetching mailto:security@chain.link/: No connection adapters were found for 'mailto:security@chain.link/'\n",
      "Error fetching https://blog.chain.link/blockchain-technology-real-estate/: 403 Client Error: Forbidden for url: https://blog.chain.link/blockchain-technology-real-estate/\n",
      "Error fetching https://blog.chain.link/introducing-the-cross-chain-interoperability-protocol-ccip/: 403 Client Error: Forbidden for url: https://blog.chain.link/introducing-the-cross-chain-interoperability-protocol-ccip/\n",
      "Error fetching https://blog.chain.link/explicit-staking-in-chainlink-2-0/: 403 Client Error: Forbidden for url: https://blog.chain.link/explicit-staking-in-chainlink-2-0/\n",
      "Error fetching https://blog.chain.link/what-is-cryptographic-truth/: 403 Client Error: Forbidden for url: https://blog.chain.link/what-is-cryptographic-truth/\n",
      "Error fetching https://blog.chain.link/build-and-use-external-adapters/: 403 Client Error: Forbidden for url: https://blog.chain.link/build-and-use-external-adapters/\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Freserves%2Fgbpt-por%2Fbbd4-0be8-6b4c-be4b%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Freserves%2Fgbpt-por%2Fbbd4-0be8-6b4c-be4b%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Freserves%2Fcachegold-por-usd%2Fad7f-4188-bc30-9e7a%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Freserves%2Fcachegold-por-usd%2Fad7f-4188-bc30-9e7a%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Ffiat%2Ftry-usd%2F2281-ec7d-7d3e-02fa%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Ffiat%2Ftry-usd%2F2281-ec7d-7d3e-02fa%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Findexes%2Fmcap-usd%2F1b70-cf4c-58c9-86d4%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Findexes%2Fmcap-usd%2F1b70-cf4c-58c9-86d4%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Findexes%2Fcspx-usd%2F2157-b37c-e7f1-2f70%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Fethereum%2Fmainnet%2Findexes%2Fcspx-usd%2F2157-b37c-e7f1-2f70%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Findexes%2Fnft-blue-chip-usd%2F4919-b6ae-41d5-3d5c%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Findexes%2Fnft-blue-chip-usd%2F4919-b6ae-41d5-3d5c%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fcrypto-usd%2Flink-usd%2F5f8e-3609-2121-8e66%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fcrypto-usd%2Flink-usd%2F5f8e-3609-2121-8e66%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fcrypto-usd%2Frdnt-usd%2F4707-fbfd-93fe-160a%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fcrypto-usd%2Frdnt-usd%2F4707-fbfd-93fe-160a%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fcrypto-usd%2Fsteth-usd%2F0205-1681-b7e3-9dfc%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fcrypto-usd%2Fsteth-usd%2F0205-1681-b7e3-9dfc%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fstablecoins%2Fusdt-usd%2F21d6-0116-8392-5b59%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fstablecoins%2Fusdt-usd%2F21d6-0116-8392-5b59%2F/'\n",
      "Error fetching tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fstablecoins%2Ffrax-usd%2Fc0ff-d895-fa23-a9e0%2F/: No connection adapters were found for 'tg://msg_url?url=https%3A%2F%2Fdata.chain.link%2Farbitrum%2Fmainnet%2Fstablecoins%2Ffrax-usd%2Fc0ff-d895-fa23-a9e0%2F/'\n",
      "  1%|          | 1/159 [00:04<12:45,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/159 [00:08<10:50,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[<a aria-haspopup=\"dialog\" aria-label=\"open lightbox\" class=\"techtalk-video-lightbox w-inline-block w-lightbox\" href=\"#\"><div class=\"btn btn-primary\">Watch now</div><script class=\"w-json\" type=\"application/json\">{\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"url\": \"https://www.youtube.com/watch?v=K3YmflNXbEc\",\n",
      "      \"html\": \"<iframe class=\\\"embedly-embed\\\" src=\\\"//cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FK3YmflNXbEc%3Ffeature%3Doembed&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DK3YmflNXbEc&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FK3YmflNXbEc%2Fhqdefault.jpg&key=96f1f04c5f4143bcb0f2e68c87d65feb&type=text%2Fhtml&schema=youtube\\\" width=\\\"854\\\" height=\\\"480\\\" scrolling=\\\"no\\\" title=\\\"YouTube embed\\\" frameborder=\\\"0\\\" allow=\\\"autoplay; fullscreen\\\" allowfullscreen=\\\"true\\\"></iframe>\",\n",
      "      \"height\": 480,\n",
      "      \"width\": 854,\n",
      "      \"thumbnailUrl\": \"https://i.ytimg.com/vi/K3YmflNXbEc/hqdefault.jpg\",\n",
      "      \"type\": \"video\"\n",
      "    }\n",
      "  ],\n",
      "  \"group\": \"\"\n",
      "}</script></a>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4/159 [00:18<11:43,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/159 [00:21<10:50,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 6/159 [00:27<11:27,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7/159 [00:31<10:59,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 8/159 [00:35<11:02,  4.39s/it]"
     ]
    }
   ],
   "source": [
    "all_main_docs, all_you_tube_docs = scrap_chain_link()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all docuemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/marshath/play/chainlink/chainlink-assistant/data/chain_link_main_docs_2023-08-18.pkl', 'rb') as f:\n",
    "    chain_link_docs = pickle.load(f)\n",
    "\n",
    "with open('/home/marshath/play/chainlink/chainlink-assistant/data/chain_link_you_tube_docs_2023-08-18.pkl', 'rb') as f:\n",
    "    chain_link_youtube_docs = pickle.load(f)\n",
    "\n",
    "with open('/home/marshath/play/chainlink/chainlink-assistant/data/datadocs_2023-08-18.pkl', 'rb') as f:\n",
    "    data_docs = pickle.load(f)\n",
    "\n",
    "with open('/home/marshath/play/chainlink/chainlink-assistant/data/blog_2023-08-18.pkl', 'rb') as f:\n",
    "    blog_docs = pickle.load(f)\n",
    "\n",
    "with open('/home/marshath/play/chainlink/chainlink-assistant/data/techdocs_2023-08-18.pkl', 'rb') as f:\n",
    "    tech_docs = pickle.load(f)\n",
    "\n",
    "docs = chain_link_docs + chain_link_youtube_docs + data_docs + blog_docs + tech_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1509"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/documents.pkl', 'wb') as f:\n",
    "    pickle.dump(docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
