{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import faiss\n",
    "import pickle\n",
    "import tiktoken\n",
    "from pydantic import BaseModel\n",
    "from typing import Any, Dict, List\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.text_splitter import TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# Formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# stream handler\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomeSplitter:\n",
    "    def __init__(self, chunk_threshold=6000, chunk_size=6000, chunk_overlap=50):\n",
    "        self.chunk_threshold = chunk_threshold\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.splitter = TokenTextSplitter(\n",
    "            chunk_size=chunk_size, \n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "\n",
    "    def token_counter(self, document):\n",
    "        tokens = self.enc.encode(document.page_content)\n",
    "        return len(tokens)\n",
    "\n",
    "    def split(self, documents):\n",
    "        chunked_documents = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            try:\n",
    "                if self.token_counter(doc) > self.chunk_threshold:\n",
    "                    chunks = self.splitter.split_documents([doc])\n",
    "                    chunks = [\n",
    "                        Document(\n",
    "                            page_content=chunk.page_content,\n",
    "                            metadata={\n",
    "                                \"source\": f\"{chunk.metadata['source']} chunk {i}\"\n",
    "                            },\n",
    "                        )\n",
    "                        for i, chunk in enumerate(chunks)\n",
    "                    ]\n",
    "                    chunked_documents.extend(chunks)\n",
    "                else:\n",
    "                    chunked_documents.append(doc)\n",
    "            except Exception as e:\n",
    "                chunked_documents.append(doc)\n",
    "                print(f\"Error on document {i}\")\n",
    "                print(e)\n",
    "                print(doc.metadata[\"source\"])\n",
    "\n",
    "        return chunked_documents\n",
    "\n",
    "\n",
    "class CustomRetriever(BaseRetriever, BaseModel):\n",
    "    full_docs: List[Document]\n",
    "    base_retriever_all: BaseRetriever = None\n",
    "    base_retriever_data: BaseRetriever = None\n",
    "    k_initial: int = 10\n",
    "    k_final: int = 4\n",
    "\n",
    "    logger: Any = None\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    @classmethod\n",
    "    def from_documents(\n",
    "        cls,\n",
    "        full_docs: List[Document],\n",
    "        vectorstore_all: FAISS,\n",
    "        vectorstore_data: FAISS,\n",
    "        search_kwargs: Dict[str, Any] = {},\n",
    "        k_initial: int = 10,\n",
    "        k_final: int = 4,\n",
    "        logger: Any = None,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        # splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=50)\n",
    "        # split_docs = splitter.split_documents(full_docs)\n",
    "        # vector_store = FAISS.from_documents(split_docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "        return cls(\n",
    "            full_docs=full_docs,\n",
    "            base_retriever_all=vectorstore_all.as_retriever(search_kwargs={\"k\": k_initial}),\n",
    "            base_retriever_data=vectorstore_data.as_retriever(search_kwargs={\"k\": k_initial}),\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "    def get_relevant_documents(self, query: str, workflow:int=1) -> List[Document]:\n",
    "        self.logger.info(f\"Worflow: {workflow}\")\n",
    "\n",
    "        if workflow == 2:\n",
    "            results = self.base_retriever_data.get_relevant_documents(query=query)\n",
    "            self.logger.info(f\"Retrieved {len(results)} documents\")\n",
    "            return results[:self.k_final]\n",
    "\n",
    "        else:\n",
    "            results =  self.base_retriever_all.get_relevant_documents(query=query)\n",
    "            self.logger.info(f\"Retrieved {len(results)} documents\")\n",
    "            if workflow == 1:\n",
    "                doc_ids = [doc.metadata[\"source\"] for doc in results]\n",
    "\n",
    "                # make it a set but keep the order\n",
    "                doc_ids = list(dict.fromkeys(doc_ids))[:self.k_final]\n",
    "\n",
    "                # log to the logger\n",
    "                self.logger.info(f\"Retrieved {len(doc_ids)} unique documents\")\n",
    "\n",
    "                # get upto 4 documents\n",
    "                full_retrieved_docs = [d for d in self.full_docs if d.metadata[\"source\"] in doc_ids]\n",
    "\n",
    "                return self.prepare_source(full_retrieved_docs)\n",
    "\n",
    "            full_retrieved_docs = results[:self.k_final]\n",
    "            return self.prepare_source(full_retrieved_docs)\n",
    "        \n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def prepare_source(self, documents: List[Document]) -> List[Document]:\n",
    "        \n",
    "        for doc in documents:\n",
    "            source = doc.metadata[\"source\"]\n",
    "            if \"chunk\" in source:\n",
    "                source = source.split(\"chunk\")[0].strip()\n",
    "                doc.metadata[\"source\"] = source\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/blog_2023-08-18.pkl', 'rb') as f:\n",
    "    blog_docs = pickle.load(f)\n",
    "\n",
    "with open('/Users/arshath/play/chainlink-assistant/data/chain_link_main_docs_2023-08-18.pkl', 'rb') as f:\n",
    "    chainlink_docs = pickle.load(f)\n",
    "\n",
    "with open('/Users/arshath/play/chainlink-assistant/data/chain_link_you_tube_docs_2023-08-18.pkl', 'rb') as f:\n",
    "    chainlink_youtube_docs = pickle.load(f)\n",
    "\n",
    "with open('/Users/arshath/play/chainlink-assistant/data/stackoverflow_documents.pkl', 'rb') as f:\n",
    "    stackoverflow_docs = pickle.load(f)\n",
    "\n",
    "with open('/Users/arshath/play/chainlink-assistant/data/techdocs_2023-08-18.pkl', 'rb') as f:\n",
    "    tech_docs = pickle.load(f)\n",
    "\n",
    "with open('/Users/arshath/play/chainlink-assistant/data/education_2023-08-14.pkl', 'rb') as f:\n",
    "    education_docs = pickle.load(f)\n",
    "    \n",
    "with open('/Users/arshath/play/chainlink-assistant/data/datadocs_2023-08-18.pkl', 'rb') as f:\n",
    "    data_docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = blog_docs + chainlink_docs + chainlink_youtube_docs + stackoverflow_docs + tech_docs + education_docs\n",
    "\n",
    "with open('/Users/arshath/play/chainlink-assistant/data/documents.pkl', 'wb') as f:\n",
    "    pickle.dump(all_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-apfNELnY4pAbHrx6LItJCss8 on tokens per min. Limit: 1000000 / min. Current: 797714 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "WARNING:langchain.embeddings.openai:Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-embedding-ada-002 in organization org-apfNELnY4pAbHrx6LItJCss8 on tokens per min. Limit: 1000000 / min. Current: 824326 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split documents into chunks for 16k model\n",
    "full_doc_splitter = CustomeSplitter()\n",
    "chunked_full_documents = full_doc_splitter.split(all_docs)\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=50)\n",
    "split_docs = splitter.split_documents(all_docs)\n",
    "\n",
    "# Create vectorstore for all documents\n",
    "vectorstore_all = FAISS.from_documents(split_docs, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Split documents into chunks using datadocs\n",
    "split_docs_data = splitter.split_documents(data_docs)\n",
    "\n",
    "# Create vectorstore for datadocs\n",
    "vectorstore_data = FAISS.from_documents(split_docs_data, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore_all\n",
    "faiss.write_index(vectorstore_all.index, \"docs_all.index\")\n",
    "vectorstore_all.index = None\n",
    "with open(\"faiss_store_all.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore_all, f)\n",
    "\n",
    "# Save vectorstore_data\n",
    "faiss.write_index(vectorstore_data.index, \"docs_data.index\")\n",
    "vectorstore_data.index = None\n",
    "with open(\"faiss_store_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainlink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
